<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 6.2.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/logo_1.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/logo_1.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/logo_1.png">
  <link rel="mask-icon" href="/images/logo_1.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">
  <link rel="stylesheet" href="/lib/pace/pace-theme-pace-theme-center-circle.min.css">
  <script src="/lib/pace/pace.min.js"></script>

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"example.com","root":"/","scheme":"Mist","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":true,"show_result":true,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":true,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"buttons","active":"valine","storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},"path":"search.xml"};
  </script>

  <meta name="description" content="0. 我的思考 两个可供突破的大方向：  是否能推动深度学习的训练数据规模减小？ 怎么使得训练模型的结果更加精确。  资源:  kaggle机器学习竞赛">
<meta property="og:type" content="article">
<meta property="og:title" content="机器学习">
<meta property="og:url" content="https://example.com/2022/09/12/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/index.html">
<meta property="og:site_name" content="Trluper">
<meta property="og:description" content="0. 我的思考 两个可供突破的大方向：  是否能推动深度学习的训练数据规模减小？ 怎么使得训练模型的结果更加精确。  资源:  kaggle机器学习竞赛">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://example.com/2022/09/12/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/relationship.png">
<meta property="og:image" content="https://example.com/2022/09/12/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/knowladge.png">
<meta property="og:image" content="https://example.com/2022/09/12/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/CNN.png">
<meta property="og:image" content="https://example.com/2022/09/12/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/pixels.png">
<meta property="og:image" content="https://example.com/2022/09/12/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/optimizer.png">
<meta property="og:image" content="https://example.com/2022/09/12/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/car.png">
<meta property="og:image" content="https://example.com/2022/09/12/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/MLprocedrue.png">
<meta property="og:image" content="https://example.com/2022/09/12/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/pro.png">
<meta property="og:image" content="https://example.com/2022/09/12/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/overAndlack.png">
<meta property="og:image" content="https://example.com/2022/09/12/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/EarlyStop.png">
<meta property="og:image" content="https://example.com/2022/09/12/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/random.png">
<meta property="og:image" content="https://example.com/2022/09/12/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/binaryclassfier.png">
<meta property="og:image" content="https://example.com/2022/09/12/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/logistic.png">
<meta property="og:image" content="https://example.com/2022/09/12/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/DL.png">
<meta property="article:published_time" content="2022-09-12T12:47:43.000Z">
<meta property="article:modified_time" content="2023-02-24T14:04:32.736Z">
<meta property="article:author" content="trluper">
<meta property="article:tag" content="深度学习算法">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://example.com/2022/09/12/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/relationship.png">

<link rel="canonical" href="https://example.com/2022/09/12/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'zh-CN'
  };
</script>

  <title>机器学习 | Trluper</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">Trluper</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>关于</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
        <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off"
           placeholder="搜索..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div id="search-result">
  <div id="no-result">
    <i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>
  </div>
</div>

    </div>
  </div>

</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>
  <div class="reading-progress-bar"></div>
  <a role="button" class="book-mark-link book-mark-link-fixed"></a>

  <a href="https://github.com/trluper" class="github-corner" title="Follow me on GitHub" aria-label="Follow me on GitHub" rel="noopener" target="_blank"><svg width="80" height="80" viewBox="0 0 250 250" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg></a>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://example.com/2022/09/12/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="trluper">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Trluper">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          机器学习
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2022-09-12 20:47:43" itemprop="dateCreated datePublished" datetime="2022-09-12T20:47:43+08:00">2022-09-12</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2023-02-24 22:04:32" itemprop="dateModified" datetime="2023-02-24T22:04:32+08:00">2023-02-24</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95/" itemprop="url" rel="index"><span itemprop="name">深度学习算法</span></a>
                </span>
            </span>

          
            <span class="post-meta-item" title="阅读次数" id="busuanzi_container_page_pv" style="display: none;">
              <span class="post-meta-item-icon">
                <i class="fa fa-eye"></i>
              </span>
              <span class="post-meta-item-text">阅读次数：</span>
              <span id="busuanzi_value_page_pv"></span>
            </span><br>
            <span class="post-meta-item" title="本文字数">
              <span class="post-meta-item-icon">
                <i class="far fa-file-word"></i>
              </span>
                <span class="post-meta-item-text">本文字数：</span>
              <span>13k</span>
            </span>
            <span class="post-meta-item" title="阅读时长">
              <span class="post-meta-item-icon">
                <i class="far fa-clock"></i>
              </span>
                <span class="post-meta-item-text">阅读时长 &asymp;</span>
              <span>12 分钟</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <h4 id="我的思考">0. 我的思考</h4>
<p>两个可供突破的大方向：</p>
<ul>
<li>是否能推动深度学习的训练数据规模减小？</li>
<li>怎么使得训练模型的结果更加精确。</li>
</ul>
<p>资源:</p>
<ul>
<li>kaggle机器学习竞赛</li>
</ul>
<span id="more"></span>
<h4 id="机器学习和深度学习关系概述">1. 机器学习和深度学习关系概述</h4>
<p>人工智能、机器学习和深度学习三者的关系如下所示： <img src="/2022/09/12/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/relationship.png" width="400"> <img src="/2022/09/12/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/knowladge.png" width="400"></p>
<ul>
<li><p><strong>机器学习</strong>：机器学习系统是训练出来的，而不是明确用代码编写出来的。人们通过输入数据和从这些数据中预期得到的答案，系统输出相应的规则。</p></li>
<li><p><strong>深度学习</strong>：<strong>深度学习里面的&quot;深度&quot;不是指通过学习获得更深层次的理解，而是指通过一系列连续的表示层来获得更有效果的数据特征。一个深度学习模型有多少层就被称为模型的深度是多少</strong>。一般来说，现代深度学习通常包含数十个甚至上百个连续的表示层。<strong>这些层也被称为神经网络</strong> <img src="/2022/09/12/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/CNN.png" width="500"></p></li>
</ul>
<h5 id="深度学习的指标">1.2 深度学习的指标</h5>
<p>在上面我们已经知道，深度学习是<strong>通过各层对输入数据不断提取特征</strong>，提取方式的具体操作就是<strong>每层都是对输入数据进行权重计算</strong>，其本质就是一串数字，</p>
<p>如下图所示，<span class="math inline">\(X_i\)</span>是从图片拉伸出来的像素，它是不会变的，无法对其进行操作。而<span class="math inline">\(W\)</span>矩阵就是参数矩阵，<span class="math inline">\(b\)</span>是神经元偏置。<strong>因此对于深度学习，对于参数矩阵的选择是一项艰巨的任务，参数的好坏决定了输出结果的优劣</strong>。但是我们不能糊里糊涂的对参数进行更改，这样可能适得其反，<strong>必须借助某一个评估指标来更改，这就是接下来要介绍的损失函数</strong> <img src="/2022/09/12/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/pixels.png" width="500"></p>
<h5 id="损失函数">1.2.1 损失函数</h5>
<p><strong>损失函数</strong>：损失函数的任务，就是通过一定数学式子衡量出该深度学习的实际输出与预期标准值之间的距离。</p>
<p><strong>通过损失函数计算的损失值作为反馈信号来对参数矩阵中的权值进行微调，来降低对应的损失值</strong>。这种调节右优化器来完成，它实现了所谓的<strong>反向传播</strong>算法。 <img src="/2022/09/12/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/optimizer.png" width="300"></p>
<blockquote>
<p>一般来说，一开始神经网络的权值是随机赋值的，那么第一次的输出也就通常与预期值相差甚远，但随着权重值的不断微调，损失在也在降低，这就是为什么说学习模型是训练出来的。</p>
</blockquote>
<h6 id="softmax分类器">1.2.2 softmax分类器</h6>
<p>上面我们经过各层神经网络最终得到一个输入的得分值，但我们想一想如果结果给与我们一个概率不是更好，那么把得分值转化为概率就是<code>softmax</code>的作用。</p>
<p><code>softmax</code>分类器将各个得分值<strong>进行放大然后归一化后再计算概率</strong></p>
<h5 id="卷积神经网络">1.3 卷积神经网络</h5>
<p>神经网络本质上就是将我们的输入信息转换为特征矩阵。<strong>卷积神经网络就是对同一个区域可以提取多个特征值，传统的神经网络一般只提取单个特征值，这样卷积更加全面稳实。</strong></p>
<p>卷积神经网络右输入层、隐含层、输出层组成：</p>
<ul>
<li><p>输入层：卷积神经网络的输入层可以处理多维数据，常见地，一维卷积神经网络的输入层接收一维或二维数组，其中一维数组通常为时间或频谱采样；二维数组可能包含多个通道；二维卷积神经网络的输入层接收二维或三维数组；三维卷积神经网络的输入层接收四维数组。由于使用<strong>梯度下降算法</strong>进行学习，卷积神经网络的输入特征需要进行标准化处理。具体地，在将学习数据输入卷积神经网络前，<strong>需在通道或时间/频率维对输入数据进行归一化</strong>，若输入数据为像素，也可将分布于的原始像素值归一化至<span class="math inline">\([0,1]\)</span>区间</p></li>
<li><p><strong>隐含层</strong>：隐含层包含卷积层、池化层和全连接层3类常见构筑。卷积层中的卷积核包含权重系数，而池化层不包含权重系数</p>
<ul>
<li><p><strong>卷积核</strong>：卷积层的功能是对输入数据进行特征提取，其内部包含多个<strong>卷积核</strong>，组成卷积核的每个元素都对应一个权重系数和一个偏差量（bias vector）。卷积核就是图像处理时，给定输入图像，输入图像中一个小区域中像素加权平均后成为输出图像中的每个对应像素，其中权值由一个函数定义，这个函数称为卷积核。</p></li>
<li><p><strong>激励函数</strong>：卷积层中包含激励函数以协助表达复杂特征，卷积神经网络通常使用线性整流函数<code>Rectified Linear Unit, ReLU</code>,它的作用就是对于上一层经过卷积后的数据经过<code>relu</code>的<code>max(0,x)</code>筛选</p></li>
<li><strong>池化层</strong>：在卷积层进行特征提取后，为避免特征量太多计算缓慢，对其进行特征值压缩，输出的特征图会被传递至池化层进行特征选择和信息过滤。池化层包含预设定的池化函数，其功能是将特征图中单个点的结果替换为其相邻区域的特征图统计量。</li>
<li><p><strong>全连接层</strong>：卷积神经网络中的卷积层和池化层能够对输入数据进行特征提取，<strong>全连接层的作用则是对提取的特征进行非线性组合以得到输出，即全连接层本身不被期望具有特征提取能力</strong>，而是试图利用现有的高阶特征完成学习目标 <img src="/2022/09/12/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/car.png" width="500"></p></li>
</ul></li>
</ul>
<h4 id="机器学习">2. 机器学习</h4>
<p>当我们用机器学习来解决实际任务时，会面对多种多样的数据形式，比如声音、图像、文本等．不同数据的特征构造方式差异很大．对于图像这类数据，我们可以很自然地将其表示为一个连续的向量． 将图像数据表示为向量的方法有很多种，比如直接将一幅图像的所有像素值（灰度值或RGB 值）组成一个连续向量．而对于文本数据，因为其一般由离散符号组成，并且每个符号在计算机内部都表示为无意义的编码，所以通常很难找到合适的表示方式．因此，在实际任务中使用机器学习模型一般会包含以下几个步骤</p>
<ul>
<li><strong>（1）数据预处理</strong>：对数据的原始形式进行初步的数据清理（比如去掉一些有缺失特征的样本，或去掉一些冗余的数据特征等）和加工（对数值特征进行缩放和归一化等），并构建成可用于训练机器学习模型的数据集．</li>
<li><strong>（2）特征提取</strong>：从数据的原始特征中提取一些对特定机器学习任务有用的高质量特征．比如在图像分类中提取边缘、尺度不变特征变换（Scale InvariantFeature Transform，SIFT）特征，在文本分类中去除停用词等．</li>
<li><strong>（3）特征转换</strong>：对特征进行进一步的加工，比如降维和升维． 很多特征转换方法也都是机器学习方法．降维包括特征抽取（Feature Extraction）和特征选择（Feature Selection）两种途径．常用的特征转换方法有主成分分析（Principal Components Analysis，PCA）、 线性判别分析（Linear Discriminant Analysis，LDA）等．</li>
<li><strong>（4）预测</strong>：机器学习的核心部分，学习一个函数并进行预测 <img src="/2022/09/12/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/MLprocedrue.png" width="500"></li>
</ul>
<p>上述流程中，每步特征处理以及预测一般都是分开进行的．<strong>传统的机器学习模型主要关注最后一步，即构建预测函数．但是实际操作过程中，不同预测模型的性能相差不多</strong>，<strong><em>而前三步中的特征处理对最终系统的准确性有着十分关键的作用．特征处理一般都需要人工干预完成，利用人类的经验来选取好的特征，并最终提高机器学习系统的性能．因此，很多的机器学习问题变成了特征工程（Feature Engineering）问题．开发一个机器学习系统的主要工作量都消耗在了预处理、特征提取以及特征转换上</em></strong>． <img src="/2022/09/12/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/pro.png" width="500"></p>
<h5 id="机器学习类型">2.1 机器学习类型</h5>
<p>常见的机器学习有监督学习、无监督学习和强化学习。</p>
<ul>
<li>监督学习：训练集有标签，学习准则为期望风险最小化，进行最大似然估计</li>
<li>无监督学习：训练集没有标签，学习准则为最大似然估计和最小重构错误</li>
<li>强化学习：智能体和环境交互的规矩t和累积奖励G，学习准则中会使用策略评估和改进。</li>
</ul>
<p>总的来说学习的分类有，有监督主要是回归、分类问题；无监督学习问题主要为聚类、降维和密度估计</p>
<h5 id="机器学习要素">2.2 机器学习要素</h5>
<p>机器学习的主要四要素是数据、模型、学习准则和优化算法。</p>
<h6 id="模型">2.2.1 模型</h6>
<ul>
<li>模型：以回归问题为例，输入空间<span class="math inline">\(X\)</span>和输出空间<span class="math inline">\(Y\)</span>构成了一个样本空间．对于样本空间中的样本<span class="math inline">\((x, y) ∈ X × Y\)</span>，假定x和y 之间的关系可以通过一个未知的真实映射函数<span class="math inline">\(y =g(x)\)</span>或真实条件概率分布<span class="math inline">\(P_r(y|x)\)</span>来描述．机器学习的目标是找到一个模型来近 似真实映射函数<span class="math inline">\(g(x)\)</span>或真实条件概率分布<span class="math inline">\(p_r(y|x)\)</span>．</li>
</ul>
<h6 id="学习准则">2.2.2 学习准则</h6>
<ul>
<li>学习准则：这里使用了损失函数，损失函数是一个非负实数函数，用来量化模型预测和真实标签之间的差异．下面介绍几种常用的损失函数：
<ul>
<li><strong>平方损失函数</strong>：平方损失函数（Quadratic Loss Function）经常用在预测标签<span class="math inline">\(y\)</span>为实数值的任务中，定义为 <span class="math display">\[L(y,f(x;θ))={1 \over 2}(y-f(x;θ))^2\]</span></li>
<li><strong>交叉熵损失函数</strong>：交叉熵损失函数（Cross-Entropy Loss Function）一般用于分类问题．假设样本的标签 <span class="math inline">\(y ∈ {1, ⋯ , C}\)</span> 为离散的类别，模型<span class="math inline">\(f(x; θ) ∈ [0, 1]^C\)</span>的输出为类别标签的条件概率分布。如下：比如对于三分类问题，一个样本的标签向量为 <span class="math inline">\(y = [0, 0, 1]^T\)</span>，模型预测的标签分布为 <span class="math inline">\(f(x; θ) = [0.3, 0.3, 0.4]^T\)</span>，则它们的交叉熵为 <span class="math display">\[ L(y,f(x;θ))=−(0 × log(0.3) + 0 ×log(0.3) + 1 × log(0.4)) = − log(0.4)\]</span></li>
<li><strong>Hinge损失函数</strong>：对于二分类问题，假设<span class="math inline">\(y∈{−1, +1}，f(x; θ)∈ℝ\)</span>，Hinge损失函数（Hinge Loss Function）为： <span class="math display">\[L(y,f(x;θ))=max(0,1-yf(x;θ))\]</span></li>
</ul></li>
</ul>
<h6 id="风险最小化准则">2.2.3 风险最小化准则</h6>
<p>一个好的模型 <span class="math inline">\(f(x;θ)\)</span>应当有一个比较小的期望错误，但由于不知道真实的数据分布和映射函数，实际上无法计算其期望风险 ℛ(θ)．给定一个训练集 <span class="math inline">\(D ={(x^{(n)}, y^{(n)})}_{n=1}^N\)</span>，我们可以计算的是<strong>经验风险（Empirical Risk）</strong>，即在训练集上的<strong>平均损失</strong>: <span class="math display">\[R_D^{emp}(θ)={1\over N} \sum_{n=1}^NL(y^{(n)},f(x^{(n)};θ))\]</span></p>
<p>因此，一个切实可行的学习准则是找到一组参数<span class="math inline">\(θ^*\)</span> 使得经验风险最小，即: <span class="math display">\[θ^*=argminR_D^{emp}(θ)\]</span> 这就是经验风险最小化（Empirical Risk Minimization，ERM）准则</p>
<p><strong>过拟合</strong>：</p>
<p>根据大数定理可知，当训练集大小 |𝒟| 趋向于无穷大时，经验风险就趋于期望风险。然而通常情况下，我们无法获取无限的训练样本，并且训练样本往往是真实数据的一个很小的子集或者包含一定的噪声数据，不能很好地反映全部数据的真实分布．<strong>经验风险最小化原则很容易导致模型在训练集上错误率很低，但是在未知数据上错误率很高．这就是所谓的过拟合（Overfitting）</strong></p>
<p><strong>过拟合问题往往是由于训练数据少和噪声以及模型能力强等原因造成的</strong>．为了解决过拟合问题，一般在经验风险最小化的基础上再引入<strong>参数的正则化（Regularization）来限制模型能力</strong>，使其不要过度地最小化经验风险。这种准则就是<strong>结构风险最小化（Structure Risk Minimization，SRM）准则</strong>： <span class="math display">\[θ^*=argminR_D^{struct}(θ)=argminR_D^{emp}(θ)+{1\over 2}λ||θ||^2\]</span> 其中<span class="math inline">\(‖θ‖\)</span>是<span class="math inline">\(ℓ2\)</span> 范数的正则化项，用来减少参数空间，避免过拟合；<span class="math inline">\(λ\)</span>用来控制正则化的强度．正则化项也可使用其他函数，比如$ ℓ1<span class="math inline">\(范数．\)</span>ℓ1 $范数的引入通常会使得参数有一定稀疏性，因此在很多算法中也经常使用． 从贝叶斯学习的角度来讲，正则化是引入了参数的先验分布，使其不完全依赖训练数据 <img src="/2022/09/12/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/overAndlack.png" width="400"></p>
<p><strong><em>注</em></strong>：所有损害优化的方法都可称为正则化</p>
<h6 id="最优化">2.2.4 最优化</h6>
<p>在确定了训练集<span class="math inline">\(X\)</span>、假设空间<span class="math inline">\(ℱ\)</span>以及学习准则后，如何找到最优的模型<span class="math inline">\(f(x, θ^∗)\)</span> 就成了一个最优化（Optimization）问题．机器学习的训练过程其实就是最优化问题的求解过程。</p>
<p>参数与超参数 在机器学习中，优化又可以分为<strong>参数优化和超参数优化</strong>．模型<span class="math inline">\(f(x; θ)\)</span>中的<span class="math inline">\(θ\)</span>称为模型的参数，可以通过优化算法进行学习．除了可学习的参数<span class="math inline">\(θ\)</span>之外，<strong>还有一类参数是用来定义模型结构或优化策略的，这类参数叫作超参数(Hyper-Parameter）</strong></p>
<p>常见的超参数包括：<strong>聚类算法中的类别个数、梯度下降法中的步长、正则化项的系数、神经网络的层数、支持向量机中的核函数等</strong>．<em><u>超参数的选取一般都是组合优化问题，很难通过优化算法来自动学习．因此，超参数优化是机器学习的一个经验性很强的技术，通常是按照人的经验设定，或者通过搜索的方法对一组超参数组合进行不断试错调整</u>．</em></p>
<ul>
<li><strong>优化算法：</strong>
<ul>
<li><p><strong>梯度下降法</strong>：为了充分利用凸优化中一些高效、成熟的优化方法，比如共轭梯度、拟牛顿法等，很多机器学习方法都倾向于选择合适的模型和损失函数，以构造一个凸函数作为优化目标．但也有很多模型（比如神经网络）的优化目标是非凸的，只能退而求其次找到局部最优解。在机器学习中，<strong>最简单、常用的优化算法就是梯度下降法</strong>，即首先初始化参数<span class="math inline">\(θ_0\)</span>，然后按下面的迭代公式来计算训练集<span class="math inline">\(D\)</span>上风险函数的最小值: <span class="math display">\[θ_{t+1}=θ_t-α{δR_D(θ)\over δθ}=θ_t-α{1\over N}\sum{_{n=1}^N}{δL(y^{(n)},f(x^{(n)};θ)\over δθ}\]</span> 其中<span class="math inline">\(θ_t\)</span>为第<span class="math inline">\(t\)</span>次迭代时的参数值，<span class="math inline">\(α\)</span>为搜索步长．在机器学习中，<span class="math inline">\(α\)</span>一般称为学习率（Learning Rate）</p></li>
<li><p><strong>提前停止</strong>：针对梯度下降的优化算法，<strong>除了加正则化项之外，还可以通过提前停止来防止过拟合</strong>．在梯度下降训练的过程中，由于过拟合的原因，在训练样本上收敛的参数，并不一定在测试集上最优．因此，除了训练集和测试集之外，有时也会使用一个验证集（Validation Set）来进行模型选择，测试模型在验证集上是否最优. 在每次迭代时，把新得到的模型 <span class="math inline">\(f(x; θ)\)</span> 在验证集上进行测试，并计算错误率．如果在验证集上的错误率不再下降，就停止迭代．这种策略叫提前停止（EarlyStop）．如果没有验证集，可以在训练集上划分出一个小比例的子集作为验证集 <img src="/2022/09/12/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/EarlyStop.png" width="400"></p></li>
<li><p><strong>随机梯度下降法</strong>：上面的<strong>梯度下降法</strong>的目标函数是整个训练集上的风险函数，批量梯度下降法在每次迭代时需要计算每个样本上损失函数的梯度并求和．当训练集中的样本数量<span class="math inline">\(N\)</span>很大时，空间复杂度比较高，每次迭代的计算开销也很大。<strong>批量梯度下降法相当于是从真实数据分布中采集<span class="math inline">\(N\)</span>个样本</strong>，并由它们计算出来的经验风险的梯度来近似期望风险的梯度．为了减少每次迭代的计算复杂度，我们也可以在每次迭代时只采集一个样本，计算这个样本损失函数的梯度并更新参数，即<strong>随机梯度下降法（Stochastic Gradient Descent，SGD</strong>）．当经过足够次数的迭代时，随机梯度下降 作增量梯度下降法．也可以收敛到局部最优解[Nemirovski et al., 2009] <img src="/2022/09/12/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/random.png" width="400"></p></li>
</ul>
<p>批量梯度下降和随机梯度下降之间的区别在于，每次迭代的优化目标是对所有样本的平均损失函数还是对单个样 本的损失函数．由于随机梯度下降实现简单，收敛速度也非常快，因此使用非常广泛．随机梯度下降相当于在批 量梯度下降的梯度上引入了随机噪声．在非凸优化问题中，随机梯度下降更容易逃离局部最优点．</p>
<ul>
<li><strong>小批量梯度下降法</strong>：随机梯度下降法的一个缺点是无法充分利用计算机的并行计算能力．小批量梯度下降法（Mini-Batch Gradient Descent）是批量梯度下降和随机梯度下降的折中．每次迭代时，我们随机选取一小部分训练样本来计算梯度并更新参数，这样既可以兼顾随机梯度下降法的优点，也可以提高训练效率： <span class="math display">\[θ_{t+1}←θ_t-α{1\over k}\sum_{(x,y)∈δ_t}{δL(y,f(x;θ))\over δθ}\]</span></li>
</ul></li>
</ul>
<h5 id="线性回归">2.3 线性回归</h5>
<p>本节通过一个简单的模型（线性回归）来具体了解机器学习的一般过程，以及不同学习准则（经验风险最小化、结构风险最小化、最大似然估计、最大后验估计）之间的关系．线性回归（Linear Regression）是机器学习和统计学中最基础和最广泛应用的模型，是一种对自变量和因变量之间关系进行建模的回归分析．自变量数量为1时称为简单回归，自变量数量大于1时称为多元回归。如下： <span class="math display">\[f(x;w,b)=w^Tx+b\]</span> 其中权重向量<span class="math inline">\(w∈ ℝ^D\)</span> 和偏置<span class="math inline">\(b∈ ℝ\)</span>都是可学习的参数，<span class="math inline">\(f(x;w,b)=w^Tx+b\)</span>即为线性模型</p>
<h6 id="参数学习">2.3.1 参数学习</h6>
<p>给定一组包含 𝑁 个训练样本的训练集 <span class="math inline">\(D = {(x^{(n)}, y{(n)})}_{n=1}^N=1\)</span>，我们希望能够学习一个最优的线性回归的模型参数<span class="math inline">\(w\)</span>．我们介绍四种不同的参数估计方法：经验风险最小化、结构风险最小化、最大似然估计、最大后验估计．</p>
<h5 id="经验风险最小化">2.4 经验风险最小化</h5>
<p>由于线性回归的标签𝑦和模型输出都为连续的实数值，因此平方损失函数非常合适衡量真实标签和预测标签之间的差异．根据经验风险最小化准则，训练集<span class="math inline">\(w\)</span>上的经验风险定义 <span class="math display">\[R(w)=\sum_{n=1}^NL(y^{(n)},f(x^{(n)};w)\\
={1\over 2}\sum_{n=1}^N(y^{(n)}-w^Tx^{(n)})^2 \\
={1\over 2}||y-X^Tw||^2
\]</span></p>
<p>风险函数<span class="math inline">\(ℛ(w)\)</span>是关于𝒘的凸函数，其对<span class="math inline">\(w\)</span>的偏导数为: <span class="math display">\[
{δR(w)\over δw}={1\over 2}{δ||y-X^Tw||^2\over δw}\\
=-X(y-X^Tw)
\]</span> 令其等于0，则可得<span class="math inline">\(w^*=(XX^T)^{(-1)}Xy\)</span> 上述求解线性回归参数的方法为<strong>最小二乘法</strong>。在最小二乘法，<span class="math inline">\(XX^T∈R^{(D+1)×(D+1)}\)</span>必须存在逆矩阵。</p>
<h5 id="结构风险最小化">2.5 结构风险最小化</h5>
<p>最小二乘法的基本要求是各个特征之间相互独立，保证<span class="math inline">\(XX^T\)</span>可逆，且即使可逆，如果特征值之间有较大的多重共线性，也会使得<span class="math inline">\(XX^T\)</span>在的逆在数值上无法准确计算。<strong>因此此时给<span class="math inline">\(XX^T\)</span>的对角线加上一个常数<span class="math inline">\(λ\)</span>，使其满秩</strong> <span class="math display">\[R(w)={1\over 2}||y-X^Tw||^2+{1\over 2}λ||w||\]</span></p>
<h5 id="最大似然估计">2.6 最大似然估计</h5>
<p>一类是样本的特征向量<span class="math inline">\(x\)</span>和标签<span class="math inline">\(y\)</span> 之间存在未知的函数关系 <span class="math inline">\(y = ℎ(x)\)</span>，另一类是条件概率$ p(y|x)$ 服从某个未知分布．最小二乘法是属于第一类，直接建模 <span class="math inline">\(x\)</span> 和标签 <span class="math inline">\(y\)</span> 之间的函数关系．此外，线性回归还可以从建模条件概率<span class="math inline">\(p(y|x)\)</span>的角度来进行参数估计。</p>
<p>假设标签<span class="math inline">\(y\)</span>为一个随机变量，并由函数<span class="math inline">\(f(x; w) = w^Tx\)</span> 加上一个随机噪声<span class="math inline">\(ε\)</span>决定，即<span class="math inline">\(f(x; w) = w^Tx+ε\)</span> 其中<span class="math inline">\(ε\)</span>服从均值为0，方差为<span class="math inline">\(δ^2\)</span>的高斯分布，则<span class="math inline">\(y\)</span>服从均值为<span class="math inline">\(w^Tx\)</span>,方差为<span class="math inline">\(δ^2\)</span>的高斯分布： <span class="math display">\[
p(y|x;w,δ)={1\over \sqrt{2\piδ}}exp(-{(y-w^Tx)^2\over2δ^2})
\]</span></p>
<p>接下来计算<span class="math inline">\(w\)</span>在训练集上的似然函数，后对似然函数取对数方便计算，令其导等于0，求出解得: <span class="math display">\[w^{ML}=(XX^T)^{-1}Xy\]</span></p>
<h5 id="最大后验估计">2.7 最大后验估计</h5>
<p><strong>最大似然估计的一个缺点是当训练数据比较少时会发生过拟合，估计的参数可能不准确．为了避免过拟合，我们可以给参数加上一些先验知识</strong>．略，详看书</p>
<h5 id="偏差-方差分解">2.8 偏差-方差分解</h5>
<p>为了避免过拟合，我们经常会在模型的拟合能力和复杂度之间进行权衡．拟合能力强的模型一般复杂度会比较高，容易导致过拟合．相反，如果限制模型的复杂度，降低其拟合能力，又可能会导致欠拟合．因此，如何在模型的拟合能力和复杂度之间取得一个较好的平衡，对一个机器学习算法来讲十分重要．<strong>偏差-方差分解（Bias-Variance Decomposition）为我们提供了一个很好的分析和指导工具．</strong></p>
<h5 id="机器学习算法类型">2.9 机器学习算法类型</h5>
<p>机器学习算法可以按照不同的标准来进行分类．比如按函数<span class="math inline">\(f(y; θ)\)</span>的不同，机器学习算法可以分为<strong>线性模型和非线性模型</strong>；按照学习准则的不同，机器学习算法也可以分为<strong>统计方法和非统计方法</strong>．</p>
<p>但一般来说，我们会<strong>按照训练样本提供的信息以及反馈方式的不同</strong>，将机器学习算法分为以下几类：</p>
<ul>
<li><strong>监督学习</strong>：如果机器学习的目标是建模样本的特征<span class="math inline">\(x\)</span>和标签<span class="math inline">\(y\)</span>之间的关系：<span class="math inline">\(y =f(x; θ)\)</span>或<span class="math inline">\(p(y|x; θ)\)</span>，并且训练集中每个样本都有标签，那么这类机器学习称为监督学习（Supervised Learning）.根据标签类型的不同，监督学习又可以分为<strong>回归问题、分类问题和结构化学习问题</strong>．
<ul>
<li>回归（Regression）问题中的标签<span class="math inline">\(y\)</span>是连续值（实数或连续整数），f(x; θ)$的输出也是连续值．</li>
<li><strong>分类（Classification）问题</strong>中的标签<span class="math inline">\(y\)</span>是离散的类别（符号）．在分类问题中，学习到的模型也称为分类器（Classifier）。分类问题根据其类别数量又可分为二分类（Binary Classification）和多分类（Multi-class Classification）问题．</li>
<li>结构化学习（Structured Learning）问题是一种特殊的分类问题．在结构化学习中，标签𝒚通常是结构化的对象，比如序列、树或图等．由于结构化学习的输出空间比较大，因此我们一般定义一个联合特征空间，将<span class="math inline">\(x, y\)</span>映射为该空间中的联合特征向量<span class="math inline">\(\phi(x, y)\)</span>，预测模型可以写为: <span class="math display">\[y=arg max f(\phi(x,y);θ)\]</span> 计算<span class="math inline">\(argmax\)</span>s得过程也称为解码Decoding过程</li>
</ul></li>
<li><p><strong>无监督学习</strong>：无监督学习（Unsupervised Learning，UL）是指从不包含目标标签的训练样本中自动学习到一些有价值的信息．典型的无监督学习问题有聚类、密度估计、特征学习、降维等．</p></li>
<li><p><strong>强化学习</strong>：强化学习（Reinforcement Learning，RL）是一类通过交互来学习的机器学习算法．在强化学习中，智能体根据环境的状态做出一个动作，并得到即时或延时的奖励．智能体在和环境的交互中不断学习并调整策略，以取得最大化的期望总回报</p></li>
</ul>
<h4 id="线性模型">3. 线性模型</h4>
<p>线性模型（Linear Model）是机器学习中应用最广泛的模型，指通过样本特征的线性组合来进行预测的模型。本节我们主要介绍四种不同线性分类模型：<strong>Logistic回归、Softmax回归、感知器和支持向量机</strong>，这些模型的区别主要在于使用了不同的损失函数</p>
<h5 id="线性判别函数和决策边界">3.1 线性判别函数和决策边界</h5>
<p>一个线性分类模型（Linear Classification Model）或线性分类器（Linear Classifier），是由一个（或多个）线性的判别函数<span class="math inline">\(f(x; w) =w^T + b\)</span> 和非线性的决策函数<span class="math inline">\(g(⋅)\)</span>组成．我们首先考虑二分类的情况，然后再扩展到多分类的情况．</p>
<h6 id="二分类">3.1.1 二分类</h6>
<p>二分类问题是最简单得，其类别标签只有两个值，通常设为<span class="math inline">\({+1,-1}\)</span>。因此只需要一个线性判别函数<span class="math inline">\(f(x; w) =w^T + b\)</span>。在特征空间<span class="math inline">\(R^D\)</span>中所有满足<span class="math inline">\(f(x;w)=0\)</span>得点组成一个<strong>分割超平面</strong>，也称为决策边界或决策平面，它将特征空间一分为二，两个区域各对应一个类边。特征空间中样本点到决策平面得距离为： <span class="math display">\[
\gamma={f(x;w)\over||w||}
\]</span> <img src="/2022/09/12/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/binaryclassfier.png" width="300"></p>
<h6 id="多分类">3.1.2 多分类</h6>
<p>多分类（Multi-class Classification）问题是指分类的类别数<span class="math inline">\(C\)</span>大于 2．多分类一般需要多个线性判别函数，但设计这些判别函数有很多种方式。一个多分类问题常用以下三种：</p>
<ul>
<li><strong>一对其余方式</strong>：把多分类问题转换为 C个“一对其余”的二分类问题．这种方式共需要<span class="math inline">\(C\)</span>个判别函数，其中第c个判别函数<span class="math inline">\(f_c\)</span> 是将类别c的样本和不属于类别c的样本分开</li>
<li><strong>“一对一”方式</strong>：把多分类问题转换为 <span class="math inline">\(C(C − 1)/2\)</span>个“一对一”的二分类问题．这种方式共需要<span class="math inline">\(C(C − 1)/2\)</span>个判别函数，其中第<span class="math inline">\((i, j)\)</span>个判别函数是把类别i和类别j的样本分开</li>
<li><strong>“argmax”方式</strong>：这是一种改进的“一对其余”方式，共需要<span class="math inline">\(C\)</span>个判别函数: <span class="math display">\[
f_C(x;w_c)=w^T_cx+b_C
\]</span> 表示对于样本<span class="math inline">\(x\)</span>，如果存在一个类边<span class="math inline">\(c\)</span>，相对于所有其他类边<span class="math inline">\(\tilde{c}(\tilde{c}≠c)\)</span>有<span class="math inline">\(f_c(x;w_c)&gt;f_{\tilde{c}(x,w_{\tilde{c}})}\)</span>。那么<span class="math inline">\(x\)</span>属于类边<span class="math inline">\(c\)</span></li>
</ul>
<p>上述得三种分类判别函数中前两个都有缺陷，那就是会存在一些难以区分得区域，二<strong>argmax</strong>方式很好得解决了这个问题</p>
<h5 id="logistic回归">3.2 Logistic回归</h5>
<p>Logistic 回归（Logistic Regression，LR）是一种常用的处理二分类问题的线性模型．在本节中，我们采用<span class="math inline">\(y ∈ [0, 1]\)</span>以符合Logistic回归的描述习惯。为解决连续线性函数不适合分类问题，引入非线性函数g来预测后验概率<span class="math inline">\(p(y=1|x)\)</span>: <span class="math display">\[
p(y=1|x)=g(f(x;w))
\]</span> 上面<span class="math inline">\(g\)</span>也被称为<strong>激活函数</strong>，它得作业其实就是将通过<span class="math inline">\(f(x;w)\)</span>得出得值进行(0,1)间的压缩，使其成为概率。反函数<span class="math inline">\(g^{-1}\)</span>称为<strong>联系函数</strong>。</p>
<p>因此使用LR作为激活函数时，相应的后验概率为（为简单起见，<span class="math inline">\(w\)</span>和<span class="math inline">\(x\)</span>均为增广矩阵）： <span class="math display">\[
p(y=1|x)={1\over 1+exp(-w^Tx)}；
p(y=0|x)=1-p(y=1|x)
\]</span> <img src="/2022/09/12/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/logistic.png" width="300"></p>
<h6 id="参数学习-1">3.2.1 参数学习</h6>
<p>Logistic 回归采用交叉熵作为损失函数，并使用梯度下降法来对参数进行优化．</p>
<p>给定<span class="math inline">\(N\)</span>个训练样本<span class="math inline">\((x^{(n)},y^{(n)})_{n=1}^N\)</span>，使用LR回归模型对每个样本进行预测，输出其标签为1的后验概率，<strong>记为<span class="math inline">\(\tilde{y}^{(n)}\)</span></strong>。则交叉熵损失函数后，其风险函数为： <span class="math display">\[
R(w)={-1\over N}\sum_{n=1}^N(p_r(y^{(n)}=1|x^{(n)}))log\tilde{y}^{(n)}+p_r(y^{(n)}=0|x^{(n)}))log(1-\tilde{y}^{(n)}))\\
={-1\over N}\sum_{n=1}^N(y^{(n)}log\tilde{y}^{(n)}+(1-y^{(n)})log(1-\tilde{y}^{(n)}))   \qquad式3.2.1
\]</span></p>
<p>然后对其进行求导的<span class="math inline">\({δR(w)\over δw}=-{1\over N}\sum x^{(n)}(y^{(n)}-\tilde{y}^{(n)})\)</span>。然后使用梯度下降法进行迭代更新进行优化： <span class="math display">\[
w_{t+1}=w_t+α{δR(w)\over δw}
\]</span></p>
<p>由式子<span class="math inline">\(3.2.1\)</span>可知，其风险函数是关于参数<span class="math inline">\(w\)</span>连续可导的凸函数，因此除了使用梯度下降，LR回归还可以用高阶的优化方法，如牛顿法。</p>
<h5 id="softmax回归">3.3 softmax回归</h5>
<p>Softmax 回归（Softmax Regression），也称为多项（Multinomial）或多类（Multi-Class）的Logistic回归，<strong>是Logistic回归在多分类问题上的推广</strong>。对于多类问题，类别标签<span class="math inline">\(y∈ {1, 2, ⋯ , C}\)</span>可以有<span class="math inline">\(C\)</span> 个取值．给定一个样本<span class="math inline">\(x\)</span>，Softmax回归预测的属于类别<span class="math inline">\(c\)</span>的条件概率为: <span class="math display">\[
\tilde{y}=p(y=c|x)=softmax(w^T_cx)\\
={exp(w_c^Tx)\over \sum ^C_{\tilde{c}=1}exp(w^T_\tilde{c}x)}
\]</span></p>
<p><strong>其决策函数为：</strong> <span class="math display">\[
\{y=[I(C=c]\}=arg max_{c=1}^C p(y=c|x)
\]</span></p>
<h6 id="参数学习-2">3.3.1 参数学习</h6>
<p>给定<span class="math inline">\(N\)</span>个训练样本<span class="math inline">\({(x^{(n)}, y^{(n)})}_{n=1}^N\)</span>，<strong>Softmax回归使用交叉熵损失函数来学习最优的参数矩阵<span class="math inline">\(W\)</span></strong>．为了方便起见，我们用<span class="math inline">\(C\)</span>维的one-hot向量<span class="math inline">\(y ∈(0, 1)^c\)</span> 来表示类别标签．对于类别<span class="math inline">\(C\)</span>，其向量表示为: <span class="math display">\[
y=[I(1=c),I(2=c),...,I(C=c)]
\]</span></p>
<p>采用交叉熵损失函数时，softmax的风险函数为： <span class="math display">\[
R(w)=-{1\over N}\sum_{n=1}^N\sum_{c=1}^C y_c^{(n)}log\tilde{y}_c^{(n)} \\
=-{1\over N}\sum_{n=1}^N (y^{(n)})^Tlog\tilde{y}^{(n)}
\]</span></p>
<p>利用梯度下降法，对其求导可得梯度： <span class="math display">\[
{δR(W)\over δW}=-{1\over N}\sum_{n=1}^N x^{(n)}(y^{(n)}-\tilde{y}^{(n)})^T
\]</span> 然后进行迭代更新： <span class="math display">\[
w_{t+1}=w_t+α{δR(w)\over δw}
\]</span></p>
<h4 id="深度学习简述">4.深度学习简述</h4>
<p>为了学习一种好的表示，需要构建具有一定“深度”的模型，并通过学习算法来让模型自动学习出好的特征表示（从底层特征，到中层特征，再到高层特征），从而最终提升预测模型的准确率．<strong>所谓“深度”是指原始数据进行非线性特征转换的次数</strong> <img src="/2022/09/12/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/DL.png" width="500"></p>
<p>深度学习是将原始的数据特征通过多步的特征转换得到一种特征表示，并进一步输入到预测函数得到最终结果．和“浅层学习”不同，<strong>深度学习需要解决的关键问题是贡献度分配问题（Credit Assignment Problem，CAP）[Minsky,1961]</strong>，即一个系统中不同的组件（component）或其参数对最终系统输出结果的贡献或影响.从某种意义上讲，深度学习可以看作一种强化学习（Reinforcement Learning，RL），每个内部组件并不能直接得到监督信息，需要通过整个模型的最终监督信息（奖励）得到，并且有一定的延时性</p>
<p><strong>深度学习采用的模型主要是神经网络模型，其主要原因是神经网络模型可以使用误差反向传播算法，从而可以比较好地解决贡献度分配问题</strong>．</p>
<h5 id="表示学习">4.1 表示学习</h5>
<p>为了提高机器学习系统的准确率，我们就需要将输入信息转换为有效的特 征，或者更一般性地称为表示（Representation）。如果有一种算法可以自动地学习出有效的特征，并提高最终机器学习模型的性能，那么这种学习就可以叫作<strong>表示学习</strong>。</p>
<p>表示学习的关键是解决<strong>语义鸿沟（Semantic Gap）问题</strong>．语义鸿沟问题是指输入数据的底层特征和高层语义信息之间的不一致性和差异性．比如给定一些关于“车”的图片，由于图片中每辆车的颜色和形状等属性都不尽相同，因此不同图片在像素级别上的表示（即底层特征）差异性也会非常大．但是我们理解这些图片是建立在比较抽象的高层语义概念上的．如果一个预测模型直接建立在底层特征之上，会导致对预测模型的能力要求过高．如果可以有一个好的表示在某种程度上能够反映出数据的高层语义特征，那么我们就能相对容易地构建后续的机器学习模型</p>
<h6 id="局部和分布式表示">4.1.1 局部和分布式表示</h6>
<p>在机器学习中，我们经常使用两种方式来表示特征：<strong>局部表示（Local Representation）和分布式表示（Distributed Representation）．</strong></p>
<ul>
<li><strong>局部表示</strong>：离散表示，one-Hot向量，单一值表示一个东西。
<ul>
<li><strong>优点：</strong>
<ul>
<li>这种离散的表示方式具有很好的解释性，有利于人工归纳和总结特征，并通过特征组合进行高效的特征工程</li>
<li>通过多种特征组合得到的表示向量通常是稀疏的二值向量，当用于线性模型时计算效率非常高</li>
</ul></li>
<li><strong>缺点</strong>：
<ul>
<li>one-hot向量的维数很高，且不能扩展．如果有一种新的颜色，我们就需要增加一维来表示</li>
<li>不同颜色之间的相似度都为0，即我们无法知道“红色”和“中国红”的相似度要高于“红色”和“黑色”的相似度</li>
</ul></li>
</ul></li>
<li><strong>分布式表示</strong>：压缩、低维的稠密向量，使用多个值表示一个东西，如表示颜色的方法是用RGB值来表示颜色，不同颜色对应到R、G、B三维空间中一个点，这种表示方式叫作分布式表示
<ul>
<li>分布式表示的表示能力要强很多，分布式表示的向量维度一般都比较低．我们只需要用一个三维的稠密向量就可以表示所有颜色．并且，分布式表示也很容易表示新的颜色名．此外，不同颜色之间的相似度也很容易计算</li>
</ul></li>
</ul>
<h5 id="端到端学习">4.2 端到端学习</h5>
<p><strong>端到端学习（End-to-End Learning），也称端到端训练，是指在学习过程中不进行分模块或分阶段训练，直接优化任务的总体目标</strong>．在端到端学习中，一般不需要明确地给出不同模块或阶段的功能，中间过程不需要人为干预．端到端学习的训练数据为“输入-输出”对的形式，无须提供其他额外信息．因此，端到端学习和深度学习一样，都是要解决贡献度分配问题．目前，大部分采用神经网络模型的深度学习也可以看作一种端到端的学习</p>

    </div>

    
    
    
        <div class="reward-container">
  <div></div>
  <button onclick="var qr = document.getElementById('qr'); qr.style.display = (qr.style.display === 'none') ? 'block' : 'none';">
    打赏
  </button>
  <div id="qr" style="display: none;">
      
      <div style="display: inline-block;">
        <img src="/images/wechatpay.png" alt="trluper 微信支付">
        <p>微信支付</p>
      </div>
      
      <div style="display: inline-block;">
        <img src="/images/alipay.png" alt="trluper 支付宝">
        <p>支付宝</p>
      </div>

  </div>
</div>


      <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95/" rel="tag"># 深度学习算法</a>
          </div>

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/2022/09/09/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0/" rel="prev" title="计算机视觉课程笔记">
      <i class="fa fa-chevron-left"></i> 计算机视觉课程笔记
    </a></div>
      <div class="post-nav-item">
    <a href="/2022/09/17/python%E7%8E%AF%E5%A2%83%E9%85%8D%E7%BD%AE/" rel="next" title="python环境配置">
      python环境配置 <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          
    <div class="comments" id="gitalk-container"></div>

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%88%91%E7%9A%84%E6%80%9D%E8%80%83"><span class="nav-number">1.</span> <span class="nav-text">0. 我的思考</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%92%8C%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%85%B3%E7%B3%BB%E6%A6%82%E8%BF%B0"><span class="nav-number">2.</span> <span class="nav-text">1. 机器学习和深度学习关系概述</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%8C%87%E6%A0%87"><span class="nav-number">2.1.</span> <span class="nav-text">1.2 深度学习的指标</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0"><span class="nav-number">2.2.</span> <span class="nav-text">1.2.1 损失函数</span></a><ol class="nav-child"><li class="nav-item nav-level-6"><a class="nav-link" href="#softmax%E5%88%86%E7%B1%BB%E5%99%A8"><span class="nav-number">2.2.1.</span> <span class="nav-text">1.2.2 softmax分类器</span></a></li></ol></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C"><span class="nav-number">2.3.</span> <span class="nav-text">1.3 卷积神经网络</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0"><span class="nav-number">3.</span> <span class="nav-text">2. 机器学习</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%B1%BB%E5%9E%8B"><span class="nav-number">3.1.</span> <span class="nav-text">2.1 机器学习类型</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%A6%81%E7%B4%A0"><span class="nav-number">3.2.</span> <span class="nav-text">2.2 机器学习要素</span></a><ol class="nav-child"><li class="nav-item nav-level-6"><a class="nav-link" href="#%E6%A8%A1%E5%9E%8B"><span class="nav-number">3.2.1.</span> <span class="nav-text">2.2.1 模型</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#%E5%AD%A6%E4%B9%A0%E5%87%86%E5%88%99"><span class="nav-number">3.2.2.</span> <span class="nav-text">2.2.2 学习准则</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#%E9%A3%8E%E9%99%A9%E6%9C%80%E5%B0%8F%E5%8C%96%E5%87%86%E5%88%99"><span class="nav-number">3.2.3.</span> <span class="nav-text">2.2.3 风险最小化准则</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#%E6%9C%80%E4%BC%98%E5%8C%96"><span class="nav-number">3.2.4.</span> <span class="nav-text">2.2.4 最优化</span></a></li></ol></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92"><span class="nav-number">3.3.</span> <span class="nav-text">2.3 线性回归</span></a><ol class="nav-child"><li class="nav-item nav-level-6"><a class="nav-link" href="#%E5%8F%82%E6%95%B0%E5%AD%A6%E4%B9%A0"><span class="nav-number">3.3.1.</span> <span class="nav-text">2.3.1 参数学习</span></a></li></ol></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E7%BB%8F%E9%AA%8C%E9%A3%8E%E9%99%A9%E6%9C%80%E5%B0%8F%E5%8C%96"><span class="nav-number">3.4.</span> <span class="nav-text">2.4 经验风险最小化</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E7%BB%93%E6%9E%84%E9%A3%8E%E9%99%A9%E6%9C%80%E5%B0%8F%E5%8C%96"><span class="nav-number">3.5.</span> <span class="nav-text">2.5 结构风险最小化</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E6%9C%80%E5%A4%A7%E4%BC%BC%E7%84%B6%E4%BC%B0%E8%AE%A1"><span class="nav-number">3.6.</span> <span class="nav-text">2.6 最大似然估计</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E6%9C%80%E5%A4%A7%E5%90%8E%E9%AA%8C%E4%BC%B0%E8%AE%A1"><span class="nav-number">3.7.</span> <span class="nav-text">2.7 最大后验估计</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E5%81%8F%E5%B7%AE-%E6%96%B9%E5%B7%AE%E5%88%86%E8%A7%A3"><span class="nav-number">3.8.</span> <span class="nav-text">2.8 偏差-方差分解</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E7%B1%BB%E5%9E%8B"><span class="nav-number">3.9.</span> <span class="nav-text">2.9 机器学习算法类型</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B"><span class="nav-number">4.</span> <span class="nav-text">3. 线性模型</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#%E7%BA%BF%E6%80%A7%E5%88%A4%E5%88%AB%E5%87%BD%E6%95%B0%E5%92%8C%E5%86%B3%E7%AD%96%E8%BE%B9%E7%95%8C"><span class="nav-number">4.1.</span> <span class="nav-text">3.1 线性判别函数和决策边界</span></a><ol class="nav-child"><li class="nav-item nav-level-6"><a class="nav-link" href="#%E4%BA%8C%E5%88%86%E7%B1%BB"><span class="nav-number">4.1.1.</span> <span class="nav-text">3.1.1 二分类</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#%E5%A4%9A%E5%88%86%E7%B1%BB"><span class="nav-number">4.1.2.</span> <span class="nav-text">3.1.2 多分类</span></a></li></ol></li><li class="nav-item nav-level-5"><a class="nav-link" href="#logistic%E5%9B%9E%E5%BD%92"><span class="nav-number">4.2.</span> <span class="nav-text">3.2 Logistic回归</span></a><ol class="nav-child"><li class="nav-item nav-level-6"><a class="nav-link" href="#%E5%8F%82%E6%95%B0%E5%AD%A6%E4%B9%A0-1"><span class="nav-number">4.2.1.</span> <span class="nav-text">3.2.1 参数学习</span></a></li></ol></li><li class="nav-item nav-level-5"><a class="nav-link" href="#softmax%E5%9B%9E%E5%BD%92"><span class="nav-number">4.3.</span> <span class="nav-text">3.3 softmax回归</span></a><ol class="nav-child"><li class="nav-item nav-level-6"><a class="nav-link" href="#%E5%8F%82%E6%95%B0%E5%AD%A6%E4%B9%A0-2"><span class="nav-number">4.3.1.</span> <span class="nav-text">3.3.1 参数学习</span></a></li></ol></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AE%80%E8%BF%B0"><span class="nav-number">5.</span> <span class="nav-text">4.深度学习简述</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#%E8%A1%A8%E7%A4%BA%E5%AD%A6%E4%B9%A0"><span class="nav-number">5.1.</span> <span class="nav-text">4.1 表示学习</span></a><ol class="nav-child"><li class="nav-item nav-level-6"><a class="nav-link" href="#%E5%B1%80%E9%83%A8%E5%92%8C%E5%88%86%E5%B8%83%E5%BC%8F%E8%A1%A8%E7%A4%BA"><span class="nav-number">5.1.1.</span> <span class="nav-text">4.1.1 局部和分布式表示</span></a></li></ol></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E7%AB%AF%E5%88%B0%E7%AB%AF%E5%AD%A6%E4%B9%A0"><span class="nav-number">5.2.</span> <span class="nav-text">4.2 端到端学习</span></a></li></ol></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="trluper"
      src="/images/avatar.gif">
  <p class="site-author-name" itemprop="name">trluper</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">69</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">35</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">34</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://github.com/trluper" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;trluper" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
  </div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2023</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Trluper</span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-chart-area"></i>
    </span>
    <span title="站点总字数">1.1m</span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-coffee"></i>
    </span>
    <span title="站点阅读时长">16:01</span>
</div>

        
<div class="busuanzi-count">
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
    <span class="post-meta-item" id="busuanzi_container_site_uv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="fa fa-user"></i>
      </span>
      <span class="site-uv" title="总访客量">
        <span id="busuanzi_value_site_uv"></span>
      </span>
    </span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item" id="busuanzi_container_site_pv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="fa fa-eye"></i>
      </span>
      <span class="site-pv" title="总访问量">
        <span id="busuanzi_value_site_pv"></span>
      </span>
    </span>
</div>








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/muse.js"></script>


<script src="/js/next-boot.js"></script>

<script src="/js/bookmark.js"></script>




  




  
<script src="/js/local-search.js"></script>













  

  
      

<script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
          load: ['[tex]/mhchem'],
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
          packages: {'[+]': ['mhchem']},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }
</script>

    

  


<!-- LOCAL: You can save these files to your site and update links -->
  
  <link rel="stylesheet" href="https://unpkg.com/gitalk/dist/gitalk.css">
  <script src="https://unpkg.com/gitalk/dist/gitalk.min.js"></script>
<!-- END LOCAL -->
 
    
      <script type="text/javascript">
      function renderGitalk(){
        var gitalk = new Gitalk({
            owner: '',
            repo: '',
            clientID: '123fe329fdbb06b0f1d1',
            clientSecret: '40add829107b6e69f43274d3545ae71c988de8eb',
            admin: '',
            
            });
        gitalk.render('gitalk-container');
      }
      renderGitalk();
      </script>
    
 


<script async>window.onload=function(){var a=document.createElement('script'),b=document.getElementsByTagName('script')[0];a.type='text/javascript',a.async=!0,a.src='/sw-register.js?v='+Date.now(),b.parentNode.insertBefore(a,b)};</script></body></html>