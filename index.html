<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 6.2.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/logo_1.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/logo_1.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/logo_1.png">
  <link rel="mask-icon" href="/images/logo_1.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"example.com","root":"/","scheme":"Mist","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":true,"show_result":true,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":true,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"buttons","active":"valine","storage":true,"lazyload":false,"nav":null,"activeClass":"valine"},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},"path":"search.xml"};
  </script>

  <meta property="og:type" content="website">
<meta property="og:title" content="Trluper">
<meta property="og:url" content="http://example.com/index.html">
<meta property="og:site_name" content="Trluper">
<meta property="og:locale" content="zh_CN">
<meta property="article:author" content="trluper">
<meta name="twitter:card" content="summary">

<link rel="canonical" href="http://example.com/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : true,
    isPost : false,
    lang   : 'zh-CN'
  };
</script>

  <title>Trluper</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">Trluper</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>关于</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
        <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off"
           placeholder="搜索..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div id="search-result">
  <div id="no-result">
    <i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>
  </div>
</div>

    </div>
  </div>

</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>
  <div class="reading-progress-bar"></div>
  <a role="button" class="book-mark-link book-mark-link-fixed"></a>

  <a href="https://github.com/trluper" class="github-corner" title="Follow me on GitHub" aria-label="Follow me on GitHub" rel="noopener" target="_blank"><svg width="80" height="80" viewBox="0 0 250 250" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg></a>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content index posts-expand">
            
      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/2022/12/16/Linux%E7%B3%BB%E7%BB%9F%E7%BC%96%E7%A8%8B-2/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="trluper">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Trluper">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/12/16/Linux%E7%B3%BB%E7%BB%9F%E7%BC%96%E7%A8%8B-2/" class="post-title-link" itemprop="url">Linux系统编程_2</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2022-12-16 15:20:41" itemprop="dateCreated datePublished" datetime="2022-12-16T15:20:41+08:00">2022-12-16</time>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine：</span>
    
    <a title="valine" href="/2022/12/16/Linux%E7%B3%BB%E7%BB%9F%E7%BC%96%E7%A8%8B-2/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2022/12/16/Linux%E7%B3%BB%E7%BB%9F%E7%BC%96%E7%A8%8B-2/" itemprop="commentCount"></span>
    </a>
  </span>
  
  <br>
            <span class="post-meta-item" title="本文字数">
              <span class="post-meta-item-icon">
                <i class="far fa-file-word"></i>
              </span>
                <span class="post-meta-item-text">本文字数：</span>
              <span>0</span>
            </span>
            <span class="post-meta-item" title="阅读时长">
              <span class="post-meta-item-icon">
                <i class="far fa-clock"></i>
              </span>
                <span class="post-meta-item-text">阅读时长 &asymp;</span>
              <span>1 分钟</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/2022/12/16/Linux%E7%B3%BB%E7%BB%9F%E7%BC%96%E7%A8%8B-1/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="trluper">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Trluper">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/12/16/Linux%E7%B3%BB%E7%BB%9F%E7%BC%96%E7%A8%8B-1/" class="post-title-link" itemprop="url">Linux系统编程_1</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2022-12-16 15:20:24" itemprop="dateCreated datePublished" datetime="2022-12-16T15:20:24+08:00">2022-12-16</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2022-12-17 14:52:58" itemprop="dateModified" datetime="2022-12-17T14:52:58+08:00">2022-12-17</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Linux/" itemprop="url" rel="index"><span itemprop="name">Linux</span></a>
                </span>
                  ，
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Linux/Linux%E7%B3%BB%E7%BB%9F%E7%BC%96%E7%A8%8B/" itemprop="url" rel="index"><span itemprop="name">Linux系统编程</span></a>
                </span>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine：</span>
    
    <a title="valine" href="/2022/12/16/Linux%E7%B3%BB%E7%BB%9F%E7%BC%96%E7%A8%8B-1/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2022/12/16/Linux%E7%B3%BB%E7%BB%9F%E7%BC%96%E7%A8%8B-1/" itemprop="commentCount"></span>
    </a>
  </span>
  
  <br>
            <span class="post-meta-item" title="本文字数">
              <span class="post-meta-item-icon">
                <i class="far fa-file-word"></i>
              </span>
                <span class="post-meta-item-text">本文字数：</span>
              <span>8.5k</span>
            </span>
            <span class="post-meta-item" title="阅读时长">
              <span class="post-meta-item-icon">
                <i class="far fa-clock"></i>
              </span>
                <span class="post-meta-item-text">阅读时长 &asymp;</span>
              <span>8 分钟</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h4 id="基本概念">1.基本概念</h4>
<p>这里我们只对一些概念做简单介绍，让第一次接触的读者有一个大概的映像。后续会进行详细的讲述 ##### 1.1 操作系统的核心-内核 操作系统是指完整的软件包，它包括了用来管理和分配计算机资源的核心层软件，以及附带所有标准软件工具，诸如命令行解释器、图形用户界面、文件操作工具和文件编辑器等。其中最为重要的是其核心层软件——内核：</p>
<p><strong>内核</strong>：内核执行的主要任务是<strong>进程调度、内存管理</strong>、提供系统文件、创建和终止进程、对设备的访问、联网、提供系统调用应用编程接口(API)</p>
<ul>
<li><strong>进程调度</strong>：计算机均配备一个或多个CPU以执行程序指令，<strong>Linux与其他系统如Windows一样属于抢占式多任务操作系统</strong>，即多个任务（进程）可同时驻留在CPU中，每个进程都可以获得对CPU的使用权，那么内核就得规定什么时候哪个进程占有CPU进行处理。</li>
<li><strong>内存管理</strong>：linux当中采用了<strong>虚拟内存管理机制</strong>，这种机制有两个优点：<strong>一</strong>是使得进程与进程之间，进程与内核之间彼此隔离，一个进程无法读取或修改内核或其他进程的内存内容；<strong>二</strong>是只需将进程的一部分保存在内存中，降低了每个进程对内存的需求量，使得RAM能够加载更多的进程。</li>
<li><strong>系统调用</strong>：内核提供了系统调用应用编程接口，这样进程可以利用内核入口点（系统调用）请求内核去执行各种任务，如<code>epoll</code></li>
</ul>
<p><strong><em>注意：</em></strong>必须得理解并行并发，在多核CPU中可以做到并行</p>
<ul>
<li>并行：真正的同一时刻</li>
<li>并发：同一时间周期（我们感觉是同时执行，但是计算机执行周期是毫秒单位级别的，我们感觉不到）</li>
</ul>
<h5 id="内核态和用户态">1.2 内核态和用户态</h5>
<ul>
<li><strong>用户态：</strong>当在用户态运行时，CPU只能访问进程中被标记为用户空间的内存，试图访问内核空间会引发硬件异常。</li>
<li><strong>内核态</strong>：当处于内核态时，CPU既能访问用户内存空间，也能访问内核空间内存。一个用户态要切换到内核态，唯一方法是通过<strong>中断、故障、或者陷入系统调用</strong>这样的异常。处理后返回时，又变为用户模式。</li>
</ul>
<h5 id="shell">1.3 shell</h5>
<p>shell时一种具有特殊用途的的程序，主要用于读取用户输入的命令，并执行相应的程序以响应命令</p>
<h5 id="用户和组">1.4 用户和组</h5>
<ul>
<li><p><strong>用户</strong>：系统的每一个用户都有唯一的登录名和相应的整型用户UID</p></li>
<li><p><strong>组</strong>：为了管理方便，控制对文件和其它资源的访问，将多个用户分组。每个组都有相应的组ID（GID）。</p></li>
</ul>
<p>每个文件都有与之相应的用户ID和组ID，分别定义文件的属主和属组。系统根据所有权来判定用户对文件的访问权。</p>
<h5 id="单根目录层级目录链接及文件">1.5 单根目录层级、目录、链接及文件</h5>
<p>Linux内核维护着一套<strong>单根目录结构</strong>,以放置系统的所有文件，这与windows形成鲜明对比（windows分硬盘区映射）。Linux的目录层级的根未<code>/</code>，其余的所有目录均为其子孙： <img src="/2022/12/16/Linux%E7%B3%BB%E7%BB%9F%E7%BC%96%E7%A8%8B-1/root.jpg"></p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">root@trluper-<span class="keyword">virtual</span>-machine:/home/trluper<span class="meta"># cd /</span></span><br><span class="line">root@trluper-<span class="keyword">virtual</span>-machine:/<span class="meta"># ls</span></span><br><span class="line">bin    dev   lib    libx32      mnt   root  snap      sys  var</span><br><span class="line">boot   etc   lib32  lost+found  opt   run   srv       tmp</span><br><span class="line">cdrom  home  lib64  media       proc  sbin  swapfile  usr</span><br></pre></td></tr></table></figure>
<ul>
<li><code>/</code>：根目录，一般根目录下只存放目录，<strong>在<code>Linux</code>下有且只有一个根目录</strong>。所有的东西都是从这里开始。当你在终端里输入<code>/home</code>，你其实是在告诉电脑，先从/（根目录）开始，再进入到home目录。</li>
<li><code>/bin</code>: <strong>这一目录中存放了供超级用户和一般用户都可以使用的命令，常用的命令<code>ls、tar、mv、cat</code>等</strong></li>
<li><code>/usr/bin</code>: <strong>安装的外部的命令，<code>usr</code>表示的是<code>unix software source</code>，不是user。</strong></li>
<li><code>/boot</code>：放置<code>linux</code>系统启动时用到的一些文件，如<code>Linux</code>的内核文件<code>/boot/vmlinuz</code>，系统引导管理器<code>/boot/grub</code>。</li>
<li><code>/dev</code>：存放<code>linux</code>系统下的设备文件，访问该目录下某个文件，相当于访问某个设备，</li>
<li><code>/etc</code>：<strong>系统配置文件存放的目录，不建议在此目录下存放可执行文件</strong>，重要的配置文件有 <code>/etc/inittab、/etc/fstab、/etc/init.d、/etc/X11、/etc/sysconfig、/etc/xinetd.d。</code></li>
<li><code>/home</code>：<strong>系统默认的用户家目录，新增用户账号时，用户的家目录都存放在此目录下，~表示当前用户的家目录，<code>~edu</code>表示用户<code>edu</code>的家目录。</strong></li>
<li><code>/lib</code>：<strong>内核级别</strong>,系统使用的函数库的目录，包含许多被<code>/bin/</code>和<code>/sbin/</code>中的程序使用的库文件。</li>
<li><code>/usr/lib</code>: <strong>系统级别</strong>，目标库文件，包括动态连接库加上一些通常不是直接调用的可执行文件的存放位置。这个目录功能类似/lib目录</li>
<li><code>/usr/local/lib</code>：<strong>用户级别</strong>，包含许多被<code>/bin/</code>和<code>/sbin/</code>中的程序使用的库文件。</li>
<li><code>/usr/include</code>：<strong>C程序语言编译使用的头文件。<code>linux</code>下开发和编译应用程序所需要的头文件一般都存放在这里，通过头文件来使用某些库函数</strong></li>
<li><code>/lost+fount</code>：系统异常产生错误时，会将一些遗失的片段放置于此目录下。</li>
<li><code>/mnt、/media</code>:光盘默认挂载点，通常光盘挂载<code>/mnt/cdrom</code>下，也不一定，可以选择任意位置进行挂载。</li>
<li><code>/opt</code>：给主机额外安装软件所摆放的目录。</li>
<li><code>/proc</code>：此目录的数据都在内存中，如系统核心，外部设备，网络状态，由于数据都存放于内存中，所以不占用磁盘空间，比较重要的目录有 <code>/proc/cpuinfo、/proc/interrupts、/proc/dma、/proc/ioports、/proc/net/*</code> 等。</li>
<li><code>/root</code>：系统管理员root的家目录。</li>
<li><code>/sbin、/usr/sbin、/usr/local/sbin</code>：<strong>放置系统管理员使用的可执行命令</strong>，如<code>fdisk、shutdown、mount</code>等。<strong>与<code>/bin</code>不同的是，这几个目录是给系统管理员root使用的命令，</strong>一般用户只能&quot;查看&quot;而不能设置和使用。</li>
<li><code>/tmp</code>：<strong>一般用户或正在执行的程序临时存放文件的目录，任何人都可以访问，重要数据不可放置在此目录下。</strong></li>
<li><code>/srv</code>：服务启动之后需要访问的数据目录，如<code>www</code>服务需要访问的网页数据存放在<code>/srv/www</code>内。</li>
<li><code>/usr</code>：<strong>应用程序存放目录，<code>/usr/bin</code>存放应用程序，<code>/usr/share</code>存放共享数据，<code>/usr/lib</code>存放不能直接运行的，却是许多程序运行所必需的一些函数库文件。<code>/usr/local</code>:存放软件升级包。<code>/usr/share/doc:</code>系统说明文件存放目录。<code>/usr/share/man:</code>程序说明文件存放目录。</strong></li>
<li><code>/var</code>：放置系统执行过程中经常变化的文件，如随时更改的日志文件 <code>/var/log，/var/log/message</code>所有的登录文件存放目录，<code>/var/spool/mail</code>邮件存放的目录，<code>/var/run</code>程序或服务启动后，其PID存放在该目录</li>
</ul>
<p>Linux世界里，一切皆文件。在Linux中，文件可分为以下几种：<strong>普通文件（普通文件、可执行文件、压缩文件）、目录文件、设备文件、链接文件和套接字</strong> <img src="/2022/12/16/Linux%E7%B3%BB%E7%BB%9F%E7%BC%96%E7%A8%8B-1/linuxFile.png" width="400"></p>
<p><strong>文件符号标识：</strong></p>
<ul>
<li>普通文件:<code>-</code></li>
<li>目录：<code>d</code></li>
<li>字符设备：<code>c</code></li>
<li>管道：<code>p</code></li>
<li>符号链接：<code>l</code></li>
<li>套接字：<code>s</code></li>
<li>符号连结：<code>l</code></li>
<li>一般文件:<code>f</code></li>
</ul>
<h5 id="文件io模型">1.6 文件IO模型</h5>
<p>unix系统I/0模型具有通用性，即对任何文件采用<code>open()、read()、write()、close()</code>等这些系统调用的程序能够处理任何类型的文件。</p>
<p><strong><em>附</em></strong>：<code>stdio</code>函数库的I<code>fopen()、fclose()、scanf()、print()、fputs()、fgets()</code>等，这些函数位于系统调用<code>open()、read()、write()、close()</code>之上</p>
<h5 id="进程">1.7 进程</h5>
<p>进程是正在执行的程序的实例，当我们运行一个程序时就创建了一个进程。在执行过程中，内核会将程序代码载入虚拟内存，未程序变量分配空间<strong>(Linux中分配4G的虚拟内存映射)</strong>。</p>
<ul>
<li><p>因此在一个系统可同时运行多个进程，而每个进程都好像独占地使用硬件。</p></li>
<li><p>对于单核CPU来说进程是并发运行，则是说一个进程的指令和另一个进程的指令是交错执行（有重叠）的。传统的系统在一个时刻只能执行一个程序，但多核处理器能同时执行多个程序，即并行。</p></li>
<li><p>并发的实现是通过进程间切换来实现的（上下文切换），上下文切换，即保存当前进程的上下文、恢复新进程的上下文，然后将控制权传递到新进程。</p></li>
<li><p>一个进程实际上可以由多个称为线程的执行单元组成。每个线程都运行在进程的上下文中，共享同样的代码、全局数据和堆。但它们有自己的栈，用来装载本地变量和函数调用链接信息。</p></li>
</ul>
<p><strong><em>特殊进程介绍</em></strong>： - <strong>init进程</strong>：init进程是Linux当中所有进程之父，当我们启动Linux系统时，它就已经在运行，后续的所有进程不是有init亲自创建<code>fork()</code>，就是尤其子孙创建的。init进程的<strong>ID号为1</strong>，以超级用户<code>root</code>权限运行，谁都无法杀死init进程，其任务是创建并监控系统运行过程中的一系列进程。</p>
<ul>
<li><strong>守护进程</strong>：守护进程也称为<strong>精灵进程</strong>，守护进程在后台运行，没有控制终端供其读取或写入数据的能力，只有在系统主动关闭是他才会消失，否则一直健在。有着它特殊的作用。</li>
</ul>
<h5 id="静态库和共享库">1.8 静态库和共享库</h5>
<ul>
<li><p><strong>静态链接</strong>：静态库时一种目标库，如果一个程序引用了静态库的函数，那么链接器在解析了引用之后，会从库中抽取所需目标模块，将其复制到最重可执行目标文件中，这就是静态链接。<strong>静态链接需要更多内存和静态库版本依赖严重</strong></p></li>
<li><p><strong>动态链接</strong>：共享库是一目标模块，在运行或加载时，可以加载到任意内存地址，并和一个内存中的程序链接起来，实施<strong>运行时链接。这个过程称为动态链接</strong></p></li>
</ul>
<h5 id="内存映射">1.9 内存映射</h5>
<p>调用系统函数<code>mmap()</code>的进程，会在指定进程的虚拟地址空间中创建一个新的内存映射，映射有两种：</p>
<ul>
<li><p><strong>文件映射：</strong>将文件的<strong>部分区域</strong>映射入调用进程的虚拟内存。映射一旦完成，对文件映射内容的访问则转化为对相应内存区域的字节操作（<strong>即将数据放入内存缓存中，避免磁盘反复I/O影响速率，这样就能加快执行效率</strong>）。注意映射不是一次性映射，而是按需自动从文件加载</p></li>
<li><p><strong>匿名映射</strong>：没有文件与之对应映射时，映射页面的内容会被初始化为0</p></li>
</ul>
<h5 id="进程间的通信和同步">1.10 进程间的通信和同步</h5>
<p>Linux上有许多进程在运行，有些时独立的，但有一些是相互合作的，Linux提供了丰富的进程间通信（IPC)机制：</p>
<ul>
<li>信号（signal),用来表示事件的发生</li>
<li>管道（shell当中的<code>|</code>操作）和FIFO，用于进程间的数据传递。</li>
<li>套接字，提供了一台主机或是联网的不同主机所运行进程之间的数据传递（服务端----客户端）</li>
<li>消息队列，用于进程间交换信息</li>
<li>信号量，用来同步进程动作</li>
<li>共享内存，运行两个及以上的进程共享一块内存，当某一进程改变了共享内存的内容，其他进程会知道。</li>
</ul>
<h5 id="信号">1.11 信号</h5>
<p>尽管信号被认为是IPC的一种，但是它运用于其他更为广泛，往往会将信号作为<strong>‘软件中断’</strong>，进程收到信号，就意味着某一事件或异常发生（如定时任务，<code>ctrl+c</code>中断）</p>
<p>信号的类型有很多，以<code>SIGXXX</code>标识，</p>
<p><strong><em>接下来就正式进入Linux系统编程的学习</em></strong></p>
<h4 id="系统调用">2. 系统调用</h4>
<p><strong>系统调用</strong>是操作系统提供给用户程序调用的一组“特殊”接口。用户程序可以通过这组“特殊”接口来获得操作系统内核提供的服务</p>
<p>相应的操作系统也有不同的运行级别，<strong>用户态和内核态</strong>。</p>
<ul>
<li><strong>运行在内核态的进程可以毫无限制的访问各种资源，</strong></li>
<li>** 而在用户态下的用户进程的各种操作都有着限制，比如不能随意的访问内存、不能开闭中断以及切换运行的特权级别。**</li>
</ul>
<p>操作系统一般是通过软件中断从用户态切换到内核态。</p>
<h5 id="库函数和系统调用区别">2.1 库函数和系统调用区别</h5>
<p>Linux 下对文件操作有两种方式：<strong>系统调用（<code>system call</code>）和库函数调用（<code>Library functions</code>）。</strong></p>
<ul>
<li><strong>系统调用</strong>：系统调用是需要时间的，程序中频繁的使用系统调用会降低程序的运行效率。当运行内核代码时，CPU工作在内核态，在系统调用发生前需要保存用户态的栈和内存环境，然后转入内核态工作。系统调用结束后，又要切换回用户态。这种环境的切换会消耗掉许多时间 。</li>
</ul>
<p><img src="/2022/12/16/Linux%E7%B3%BB%E7%BB%9F%E7%BC%96%E7%A8%8B-1/libraryfunctions.png" width="100"></p>
<h5 id="文件描述符">2.2 文件描述符</h5>
<p>对文件进行相应的操作<code>open()、close()、write() 、read()</code>等）。打开现存文件或新建文件时，<strong>系统（内核）会返回一个文件描述符，文件描述符用来指定已打开的文件</strong></p>
<p>程序运行起来后（每个进程）都有一张文件描述符的表，标准输入、标准输出、标准错误输出设备文件被打开，对应的文件描述符 <code>0、1、2</code>记录在表中。程序运行起来后这三个文件描述符是默认打开的。</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">define</span> STDIN_FILENO 0  <span class="comment">//标准输入的文件描述符</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> STDOUT_FILENO 1 <span class="comment">//标准输出的文件描述符</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> STDERR_FILENO 2 <span class="comment">//标准错误的文件描述符</span></span></span><br></pre></td></tr></table></figure>
<p><strong>Linux 中一个进程默认最多只能打开的文件是1024个。</strong></p>
<h5 id="相关文件函数系统调用版">2.3 相关文件函数（系统调用版）</h5>
<h6 id="open函数">2.3.1 open函数</h6>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span><span class="string">&lt;sys/types.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span><span class="string">&lt;sys/stat.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span><span class="string">&lt;fcntl.h&gt;</span></span></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">open</span><span class="params">(<span class="type">const</span> <span class="type">char</span>* pathname,<span class="type">int</span> flags)</span></span>;</span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">open</span><span class="params">(<span class="type">const</span> <span class="type">char</span>* pathname,<span class="type">int</span> flags,<span class="type">mode_t</span> mode)</span></span>;</span><br><span class="line">功能：</span><br><span class="line">	打开文件，如果文件不存在则可以选择创建，并返回一个文件描述符</span><br><span class="line">参数：</span><br><span class="line">	pathname:文件路径及文件名</span><br><span class="line">	flags:打开文件的行为标志，必选项有O_RDONLY,O_WRONLY,O_RDWR</span><br><span class="line">	mode:该参数只在文件不存时创建文件时有效，指新建文件时指定的文件权限</span><br></pre></td></tr></table></figure>
<p><strong>flages详细说明：</strong></p>
<ul>
<li><p>必选项 <img src="/2022/12/16/Linux%E7%B3%BB%E7%BB%9F%E7%BC%96%E7%A8%8B-1/openflags.png" width="600"></p></li>
<li><p>按位或（可选）： <img src="/2022/12/16/Linux%E7%B3%BB%E7%BB%9F%E7%BC%96%E7%A8%8B-1/weihuo.png" width="600"></p></li>
</ul>
<h6 id="close函数">2.3.2 close函数</h6>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span><span class="string">&lt;unistd.h&gt;</span></span></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">close</span><span class="params">(<span class="type">int</span> fd)</span></span>;</span><br><span class="line"></span><br><span class="line">功能：</span><br><span class="line">	关闭已打开的文件</span><br><span class="line">参数：</span><br><span class="line">	fd:文件描述符，<span class="built_in">open</span>()返回值</span><br><span class="line">返回值：</span><br><span class="line">	成功：<span class="number">0</span></span><br><span class="line">	失败：<span class="number">-1</span>，并设置errno</span><br></pre></td></tr></table></figure>
<p>需要说明的是，当一个进程终止时，内核对该进程所有尚未关闭的文件描述符调用<code>close</code>关闭，所以即使用户程序不调用<code>close</code>，在终止时内核也会自动关闭它打开的所有文件。</p>
<h6 id="write函数">2.3.3 write函数</h6>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span><span class="string">&lt;unistd.h&gt;</span></span></span><br><span class="line"><span class="function"><span class="type">sszie_t</span> <span class="title">write</span><span class="params">(<span class="type">int</span> fd,<span class="type">const</span> <span class="type">void</span>* buf,<span class="type">size_t</span> count)</span></span>;</span><br><span class="line"></span><br><span class="line">功能：</span><br><span class="line">	把指定数目的数据写到文件fd</span><br><span class="line">参数：</span><br><span class="line">	fd：文件描述符</span><br><span class="line">	buf：数据首地址</span><br><span class="line">	count：写入数据长度</span><br><span class="line">返回值：</span><br><span class="line">	成功：实写入数据的字节数</span><br><span class="line">	失败：<span class="number">-1</span></span><br></pre></td></tr></table></figure>
<h6 id="read函数">2.3.4 read函数</h6>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span><span class="string">&lt;unistd.h&gt;</span></span></span><br><span class="line"><span class="function"><span class="type">ssize_t</span> <span class="title">read</span><span class="params">(<span class="type">int</span> fd,<span class="type">void</span>* buf,<span class="type">size_t</span> count)</span></span>;</span><br><span class="line"></span><br><span class="line">功能：</span><br><span class="line">	把指定数目的数据读到内存（缓冲区）</span><br><span class="line">参数：</span><br><span class="line">	fd：文件描述符</span><br><span class="line">	buf：内存首地址</span><br><span class="line">	count：读取的字节数</span><br><span class="line">返回值：</span><br><span class="line">	成功：实际读取到的字节数</span><br><span class="line">	失败：<span class="number">-1</span></span><br></pre></td></tr></table></figure>
<h6 id="lseek函数文件偏移量">2.3.5 lseek()函数:文件偏移量</h6>
<p><img src="/2022/12/16/Linux%E7%B3%BB%E7%BB%9F%E7%BC%96%E7%A8%8B-1/lseek.png" width="600"></p>
<p>所有打开的文件都有一个当前文件偏移量(current file offset)，以下简称为<code>cfo</code>。<code>cfo</code> 通常是一个非负整数，用于表明文件开始处到文件当前位置的字节数。</p>
<p><strong><em>示例：</em></strong> <figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span><span class="string">&lt;sys/types.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span><span class="string">&lt;unistd.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span><span class="string">&lt;fcntl.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> SIZE 100</span></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">(<span class="type">void</span>)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">	<span class="type">int</span> fd =<span class="number">-1</span>;</span><br><span class="line">	<span class="type">int</span> ret =<span class="number">-1</span>;</span><br><span class="line">	<span class="type">const</span> <span class="type">char</span> *str=<span class="string">&quot;trluper&quot;</span></span><br><span class="line">	<span class="type">char</span> buf[SIZE];</span><br><span class="line">	fd = <span class="built_in">open</span>(<span class="string">&quot;txt&quot;</span>,O_WRONLY|O_CREAT,<span class="number">0644</span>);</span><br><span class="line">	<span class="comment">//写入</span></span><br><span class="line">	<span class="keyword">if</span>(<span class="number">-1</span>==fd)</span><br><span class="line">	&#123;</span><br><span class="line">		<span class="built_in">perror</span>(<span class="string">&quot;open&quot;</span>);</span><br><span class="line">		<span class="keyword">return</span> <span class="number">1</span></span><br><span class="line">	&#125;</span><br><span class="line">	<span class="built_in">printf</span>(<span class="string">&quot;fd=%d\n&quot;</span>,fd);</span><br><span class="line">	ret= <span class="built_in">write</span>(fd,str,<span class="built_in">strlen</span>(str));</span><br><span class="line">	<span class="keyword">if</span>(<span class="number">-1</span> == ret)</span><br><span class="line">	&#123;</span><br><span class="line">		<span class="built_in">perror</span>(<span class="string">&quot;write&quot;</span>);</span><br><span class="line">		<span class="keyword">return</span> <span class="number">1</span>;</span><br><span class="line">	&#125;</span><br><span class="line">	<span class="built_in">printf</span>(<span class="string">&quot;write len:%d\n&quot;</span>,ret);</span><br><span class="line">	<span class="comment">//设置偏移量</span></span><br><span class="line">	ret=<span class="built_in">lseek</span>(fd,<span class="number">7</span>,SEEK_SET);</span><br><span class="line">	<span class="keyword">if</span>(<span class="number">-1</span> == ret)</span><br><span class="line">	&#123;</span><br><span class="line">		<span class="built_in">perror</span>(<span class="string">&quot;lseek&quot;</span>);</span><br><span class="line">		<span class="keyword">return</span> <span class="number">1</span>;</span><br><span class="line">	&#125;</span><br><span class="line">	<span class="built_in">write</span>(fd,<span class="string">&quot; github&quot;</span>,<span class="number">6</span>);</span><br><span class="line">	<span class="comment">//将文件位置指针指向文件开头，读取</span></span><br><span class="line">	<span class="built_in">lseek</span>(fd,<span class="number">0</span>,SEEK_SET);</span><br><span class="line">	<span class="built_in">memset</span>(buf,<span class="number">0</span>,SIZE);</span><br><span class="line">	ret= <span class="built_in">read</span>(fd,buf,SIZE)</span><br><span class="line">	<span class="built_in">printf</span>(<span class="string">&quot;read ret:%d buf:%s\n&quot;</span>,ret,buf);</span><br><span class="line">	<span class="built_in">close</span>(fd);</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<h5 id="阻塞和非阻塞">2.4 阻塞和非阻塞</h5>
<ul>
<li><p><strong>读常规文件是不会阻塞的，不管读多少字节，<code>read</code>一定会在有限的时间内返回。</strong></p></li>
<li><p><strong>从终端设备或网络读则不一定，如果从终端输入的数据没有换行符，调用<code>read</code>读终端设备就会阻塞</strong>，如果网络上没有接收到数据包，调用<code>read</code>从网络读就会阻塞，至于会阻塞多长时间也是不确定的，如果一直没有数据到达就一直阻塞在那里。</p></li>
<li><p><strong>同样，写常规文件是不会阻塞的，而向终端设备或网络写则不一定</strong></p></li>
</ul>
<h5 id="stat函数">2.5 stat()函数</h5>
<p>stat函数在之后的编程中会经常用到，可以了解一下 <img src="/2022/12/16/Linux%E7%B3%BB%E7%BB%9F%E7%BC%96%E7%A8%8B-1/stat.png" width="600"> <code>struct stat</code>结构体说明：buf传出，获取指定文件信息 <img src="/2022/12/16/Linux%E7%B3%BB%E7%BB%9F%E7%BC%96%E7%A8%8B-1/statbuf.png" width="600"> 使用实例1：判断文件类型（先明白如何判断，用man 2 stat查看st_mode的使用）</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="type">int</span> <span class="title">show_file_type</span><span class="params">(<span class="keyword">struct</span> stat* s)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">	<span class="keyword">switch</span>(s-&gt;st_mode&amp;S_IFMT)</span><br><span class="line">	&#123;</span><br><span class="line">		<span class="keyword">case</span> S_IFREG:</span><br><span class="line">			<span class="built_in">print</span>(<span class="string">&quot;该文件是普通文件\n&quot;</span>);</span><br><span class="line">			<span class="keyword">break</span>;</span><br><span class="line">		<span class="keyword">case</span> S_IFDIR:</span><br><span class="line">			<span class="built_in">print</span>(<span class="string">&quot;该文件是目录\n&quot;</span>);</span><br><span class="line">			<span class="keyword">break</span>;</span><br><span class="line">		<span class="keyword">case</span> S_IFCHR:</span><br><span class="line">			<span class="built_in">print</span>(<span class="string">&quot;该文件是字符设备\n&quot;</span>);</span><br><span class="line">			<span class="keyword">break</span>;</span><br><span class="line">		<span class="keyword">case</span> S_IFBLK:</span><br><span class="line">			<span class="built_in">print</span>(<span class="string">&quot;该文件是块设备\n&quot;</span>);</span><br><span class="line">			<span class="keyword">break</span>;</span><br><span class="line">		<span class="keyword">case</span> S_IFSOCK:</span><br><span class="line">			<span class="built_in">print</span>(<span class="string">&quot;该文件是套接字\n&quot;</span>);</span><br><span class="line">			<span class="keyword">break</span>;</span><br><span class="line">		<span class="keyword">case</span> S_IFFIFO:</span><br><span class="line">			<span class="built_in">print</span>(<span class="string">&quot;该文件是管道\n&quot;</span>);</span><br><span class="line">			<span class="keyword">break</span>;</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>文件类型和权限解释： <img src="/2022/12/16/Linux%E7%B3%BB%E7%BB%9F%E7%BC%96%E7%A8%8B-1/filetype.png" width="600"></p>
<h5 id="文件描述符的复制">2.6 文件描述符的复制</h5>
<p>dup() 和 dup2() 是两个非常有用的系统调用，都是用来复制一个文件的描述符，使新的文件描述符也标识旧的文件描述符所标识的文件。</p>
<h6 id="dup函数">2.6.1 dup()函数</h6>
<p><img src="/2022/12/16/Linux%E7%B3%BB%E7%BB%9F%E7%BC%96%E7%A8%8B-1/dup.png" width="600"></p>
<h6 id="dup2函数">2.6.2 dup2()函数</h6>
<p>dup2函数：应用于重定向 <img src="/2022/12/16/Linux%E7%B3%BB%E7%BB%9F%E7%BC%96%E7%A8%8B-1/dup2.png" width="600"></p>
<h5 id="fcnlt函数改变打开文件的性质">2.7 fcnlt函数：改变打开文件的性质</h5>
<p><strong><code>fcntl</code>函数有5种功能：</strong></p>
<ul>
<li><ol type="1">
<li>复制一个现有的描述符（cmd=F_DUPFD）</li>
</ol></li>
<li><ol start="2" type="1">
<li>获得／设置文件描述符标记(cmd=F_GETFD或F_SETFD)</li>
</ol></li>
<li><ol start="3" type="1">
<li>获得／设置文件状态标记(cmd=F_GETFL或F_SETFL)</li>
</ol></li>
<li><ol start="4" type="1">
<li>获得／设置异步I/O所有权(cmd=F_GETOWN或F_SETOWN)</li>
</ol></li>
<li><ol start="5" type="1">
<li>获得／设置记录锁(cmd=F_GETLK, F_SETLK或F_SETLKW) <img src="/2022/12/16/Linux%E7%B3%BB%E7%BB%9F%E7%BC%96%E7%A8%8B-1/fcntl.png" width="600"></li>
</ol></li>
</ul>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span><span class="string">&lt;unistd.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span><span class="string">&lt;fcnlt.h&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">	<span class="comment">//等价于dup()</span></span><br><span class="line">	<span class="type">int</span> new_fd=<span class="built_in">fcnlt</span>(fd,F_DUPFD,<span class="number">0</span>);</span><br><span class="line">	<span class="comment">//获取文件状态标记</span></span><br><span class="line">	<span class="type">int</span> flag=<span class="built_in">fcnlt</span>(fd,F_GETFD,<span class="number">0</span>)</span><br><span class="line">	<span class="keyword">switch</span>(flag&amp;O_ACCMODE)</span><br><span class="line">	&#123;</span><br><span class="line">		<span class="keyword">case</span> O_RDONLY:</span><br><span class="line">			<span class="built_in">printf</span>(<span class="string">&quot;read only\n&quot;</span>);</span><br><span class="line">			<span class="keyword">break</span>;</span><br><span class="line">	&#125;</span><br><span class="line">	<span class="keyword">if</span>(flag&amp;O_APPEND)&#123;</span><br><span class="line">		<span class="built_in">printf</span>(<span class="string">&quot;append\n&quot;</span>);</span><br><span class="line">	&#125;</span><br><span class="line">	flag|=O_APPEND;	<span class="comment">//追加flag</span></span><br><span class="line">	<span class="built_in">fcnlt</span>(new_fd,F_SETFT,flag);	<span class="comment">//设置状态标记</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h5 id="目录的相关函数">2.8 目录的相关函数</h5>
<h6 id="getcwd函数">2.8.1 getcwd函数</h6>
<p>getcwd函数：获取当前进程的工作目录 <img src="/2022/12/16/Linux%E7%B3%BB%E7%BB%9F%E7%BC%96%E7%A8%8B-1/getcwd.png" width="600"></p>
<h6 id="chdir函数">2.8.2 chdir函数</h6>
<p><code>chdir</code>函数：修改当前进程的路径 <img src="/2022/12/16/Linux%E7%B3%BB%E7%BB%9F%E7%BC%96%E7%A8%8B-1/chdir.png" width="500"></p>
<h6 id="opendir函数">2.8.3 opendir函数</h6>
<p><code>opendir</code>函数：打开一个目录，相当于cd命令 <img src="/2022/12/16/Linux%E7%B3%BB%E7%BB%9F%E7%BC%96%E7%A8%8B-1/opendir.png" width="500"></p>
<h6 id="closedir函数">2.8.4 closedir函数</h6>
<p><img src="/2022/12/16/Linux%E7%B3%BB%E7%BB%9F%E7%BC%96%E7%A8%8B-1/closedir.png" width="500"></p>
<h6 id="readdir函数">2.8.5 readdir函数</h6>
<p><code>readdir</code>函数:读取目录 <img src="/2022/12/16/Linux%E7%B3%BB%E7%BB%9F%E7%BC%96%E7%A8%8B-1/readdir.png" width="500"></p>
<p><strong>相关结构体说明：</strong> <img src="/2022/12/16/Linux%E7%B3%BB%E7%BB%9F%E7%BC%96%E7%A8%8B-1/dirent.png" width="500"></p>
<p>d_type文件类型说明： <img src="/2022/12/16/Linux%E7%B3%BB%E7%BB%9F%E7%BC%96%E7%A8%8B-1/d_type.png" width="500"></p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/2022/12/16/github%E8%AE%BF%E9%97%AE%E9%97%AE%E9%A2%98/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="trluper">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Trluper">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/12/16/github%E8%AE%BF%E9%97%AE%E9%97%AE%E9%A2%98/" class="post-title-link" itemprop="url">github访问问题</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>
              

              <time title="创建时间：2022-12-16 14:29:48 / 修改时间：15:04:50" itemprop="dateCreated datePublished" datetime="2022-12-16T14:29:48+08:00">2022-12-16</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/github/" itemprop="url" rel="index"><span itemprop="name">github</span></a>
                </span>
            </span>

          
  
  <br>
            <span class="post-meta-item" title="本文字数">
              <span class="post-meta-item-icon">
                <i class="far fa-file-word"></i>
              </span>
                <span class="post-meta-item-text">本文字数：</span>
              <span>545</span>
            </span>
            <span class="post-meta-item" title="阅读时长">
              <span class="post-meta-item-icon">
                <i class="far fa-clock"></i>
              </span>
                <span class="post-meta-item-text">阅读时长 &asymp;</span>
              <span>1 分钟</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h4 id="github无法访问">1. github无法访问</h4>
<p>由于github是外国网站，常常会有些时候无法登进去，这是由于<code>github</code>的IP地址时常会发生改变，我们在浏览器输入 GitHub 的网址时，会向 DNS 服务器发送一个请求，获取到 GitHub 网站所在的服务器 IP 地址，从而进行访问。如果DNS未即使更新地址的话给你一个过期ip，你自然无法访问到。为解决这个问题，可以通过修改host。</p>
<h5 id="查询可通的ip地址">1.1 查询可通的ip地址</h5>
<p>首先通过<a target="_blank" rel="noopener" href="https://tool.chinaz.com/dns/?type=1&amp;host=github.com&amp;ip=">站长</a>,查询<code>github.com</code>可达的ip节点： <img src="/2022/12/16/github%E8%AE%BF%E9%97%AE%E9%97%AE%E9%A2%98/ip.jpg"></p>
<h5 id="本地修改host">1.2 本地修改host</h5>
<p>先找到域名解析文件 hosts 文件的位置，不同操作系统，hosts 文件的存储位置也不同：</p>
<ul>
<li>Windows 系统：<code>C:\Windows\System32\drivers\etc\hosts</code></li>
<li>Linux 系统：<code>/etc/hosts</code></li>
<li>Mac（苹果电脑）系统：<code>/etc/hosts</code></li>
<li>Android（安卓）系统：<code>/system/etc/hosts</code></li>
<li>iPhone（iOS）系统：<code>/etc/hosts</code></li>
</ul>
<p>进入host文件，在末尾增加该地址<code>20.205.243.166 www.github.com</code> <img src="/2022/12/16/github%E8%AE%BF%E9%97%AE%E9%97%AE%E9%A2%98/host.jpg"></p>
<p>最后在终端窗口执行：<code>ipconfig/flushdns</code>即可。</p>
<p><strong><em>附：</em></strong>你也可以用<a target="_blank" rel="noopener" href="https://github.com/521xueweihan/GitHub520">github520定时更新的host</a></p>
<p>将其复制到host保存,在执行<code>ipconfig/flushdns</code>即可</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/2022/12/16/blog%E5%9F%9F%E5%90%8D%E6%8C%82%E9%9D%A0/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="trluper">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Trluper">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/12/16/blog%E5%9F%9F%E5%90%8D%E6%8C%82%E9%9D%A0/" class="post-title-link" itemprop="url">blog域名挂靠</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>
              

              <time title="创建时间：2022-12-16 13:37:53 / 修改时间：14:20:00" itemprop="dateCreated datePublished" datetime="2022-12-16T13:37:53+08:00">2022-12-16</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/blog/" itemprop="url" rel="index"><span itemprop="name">blog</span></a>
                </span>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine：</span>
    
    <a title="valine" href="/2022/12/16/blog%E5%9F%9F%E5%90%8D%E6%8C%82%E9%9D%A0/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2022/12/16/blog%E5%9F%9F%E5%90%8D%E6%8C%82%E9%9D%A0/" itemprop="commentCount"></span>
    </a>
  </span>
  
  <br>
            <span class="post-meta-item" title="本文字数">
              <span class="post-meta-item-icon">
                <i class="far fa-file-word"></i>
              </span>
                <span class="post-meta-item-text">本文字数：</span>
              <span>505</span>
            </span>
            <span class="post-meta-item" title="阅读时长">
              <span class="post-meta-item-icon">
                <i class="far fa-clock"></i>
              </span>
                <span class="post-meta-item-text">阅读时长 &asymp;</span>
              <span>1 分钟</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h4 id="github上的博客域名挂靠">1. github上的博客域名挂靠</h4>
<p>由于github是外国网站，存在者有时候节点ip无法访问到github，导致博客也登不上去，为了解决这个方法，可以使用域名进行挂靠。</p>
<h5 id="域名设置">1.1 域名设置</h5>
<p>域名购买可以买腾讯云、阿里喝华为的，这里以腾讯云为例子，点击<a target="_blank" rel="noopener" href="https://cloud.tencent.com/act/pro/domain_sales?fromSource=gwzcw.6927043.6927043.6927043&amp;utm_medium=cpc&amp;utm_id=gwzcw.6927043.6927043.6927043&amp;bd_vid=11616374416468829912">域名购买</a>。购买完成后，进入<a target="_blank" rel="noopener" href="https://console.cloud.tencent.com/domain">域名控制平台</a>-&gt;我的域名，进行设置： <img src="/2022/12/16/blog%E5%9F%9F%E5%90%8D%E6%8C%82%E9%9D%A0/yuming.jpg"></p>
<p>点击<strong>解析</strong>,进入如下页面： <img src="/2022/12/16/blog%E5%9F%9F%E5%90%8D%E6%8C%82%E9%9D%A0/jiexi.jpg"> <strong>然后增加<code>CNAME</code>记录类型，其记录值为你的博客地址，如我的为<code>trluper.github.io</code></strong></p>
<h5 id="github进行settings">1.2 github进行settings</h5>
<p>完成上面步骤后，进入你github的博客仓库，点击<code>settings</code>后选择<code>Pages</code>,设置<code>custom domain</code>的值为你购买的域名，如我的为<code>trluper.cn</code>。 <img src="/2022/12/16/blog%E5%9F%9F%E5%90%8D%E6%8C%82%E9%9D%A0/github.jpg"></p>
<p>之后大功告成，可以测试一下是否通过域名能够访问你的博客。</p>
<h5 id="解决每次git-depoly后域名消失">1.3 解决每次git depoly后域名消失</h5>
<p>git deploy之后发现个问题，访问自己的域名返回404，查看github的Custom domain发现已经被还原为空值。解决方案为： - 在你的本地blog文件中<code>source</code>目录下，增加一文件<code>CNAME</code>,文件内只写上你自己的域名url <img src="/2022/12/16/blog%E5%9F%9F%E5%90%8D%E6%8C%82%E9%9D%A0/xiaoshi.jpg"></p>
<p>然后进行<code>hexo clean\hexo g\ hexo s</code>即可</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/2022/12/16/vim/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="trluper">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Trluper">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/12/16/vim/" class="post-title-link" itemprop="url">vim</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>
              

              <time title="创建时间：2022-12-16 11:50:28 / 修改时间：13:57:09" itemprop="dateCreated datePublished" datetime="2022-12-16T11:50:28+08:00">2022-12-16</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/vim/" itemprop="url" rel="index"><span itemprop="name">vim</span></a>
                </span>
                  ，
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/vim/Linux/" itemprop="url" rel="index"><span itemprop="name">Linux</span></a>
                </span>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine：</span>
    
    <a title="valine" href="/2022/12/16/vim/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2022/12/16/vim/" itemprop="commentCount"></span>
    </a>
  </span>
  
  <br>
            <span class="post-meta-item" title="本文字数">
              <span class="post-meta-item-icon">
                <i class="far fa-file-word"></i>
              </span>
                <span class="post-meta-item-text">本文字数：</span>
              <span>787</span>
            </span>
            <span class="post-meta-item" title="阅读时长">
              <span class="post-meta-item-icon">
                <i class="far fa-clock"></i>
              </span>
                <span class="post-meta-item-text">阅读时长 &asymp;</span>
              <span>1 分钟</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h5 id="vim的三种模式">6.1 vim的三种模式</h5>
<p>vi有三种基本工作模式: <strong>命令模式、文本输入模式(编辑模式)、末行模式</strong></p>
<ul>
<li><p><strong>命令模式：</strong> <strong>任何时候,不管用户处于何种模式,只要按一下<code>ESC</code>键,即可使<code>vi</code>进入命令模式</strong>。我们在<code>shell</code>环境(提示符为<code>$</code>)下输入启动<code>vim</code>命令，进入编辑器时，也是处于该模式下。在命令模式下，用户可以输入各种合法的<code>vi</code>命令，用于管理自己的文档。<strong>此时从键盘上输入的任何字符都被当做编辑命令来解释，若输入的字符是合法的<code>vi</code>命令，则<code>vi</code>在接受用户命令之后完成相应的动作</strong>。但需注意的是，所输入的命令并不在屏幕上显示出来。若输入的字符不是<code>vi</code>的合法命令，<code>vi</code>会响铃报警。</p></li>
<li><p>文本输入模式： 在命令模式下输入插入命令<code>i（I）、附加命令a（A） 、打开命令o（O）、替换命s（S）</code>都可以进入文本输入模式，此时vi窗口的最后一行会显示“插入”,可进行文本编辑.</p></li>
<li><p>末行模式： 末行模式下，用户可以对文件进行一些附加处理。尽管命令模式下的命令可以完成很多功能，但要执行一些如字符串查找、替换、显示行号等操作还是必须要进入末行模式的。<strong>在命令模式下，输入冒号<code>:</code>即可进入末行模式。此时vi窗口的状态行会显示出冒号，等待用户输入命令。用户输入完成后，按回车执行，之后<code>vi</code>编辑器又自动返回到命令模式下</strong> <img src="/2022/12/16/vim/vi-vim.gif"></p></li>
</ul>
<h5 id="命令模式下的命令操作">6.2 命令模式下的命令操作</h5>
<h6 id="切换到编辑模式">6.2.1 切换到编辑模式</h6>
<p><img src="/2022/12/16/vim/vimbianji.png"></p>
<h6 id="光标移动">6.2.2 光标移动</h6>
<p><img src="/2022/12/16/vim/mousemove.png"></p>
<h6 id="删除">6.2.3 删除</h6>
<p><img src="/2022/12/16/vim/delete.png"> <strong>常用：<code>D</code>、<code>dG</code>、<code>nx</code>(n为数字)、<code>ndd</code></strong></p>
<h6 id="查找">6.2.4 查找</h6>
<p><img src="/2022/12/16/vim/find.png"></p>
<h6 id="可视模式">6.2.5 可视模式</h6>
<p><img src="/2022/12/16/vim/copydelete.png"></p>
<h6 id="粘贴和复制">6.2.6 粘贴和复制</h6>
<ul>
<li><code>p</code>:粘贴在光标后面</li>
<li><code>P</code>:粘贴在光标前面</li>
<li><code>y</code>:拷贝行</li>
<li><code>Y</code>:拷贝行</li>
</ul>
<h5 id="末行模式下的命令操作">6.3 末行模式下的命令操作</h5>
<h6 id="保存退出">6.3.1 保存退出</h6>
<p><img src="/2022/12/16/vim/saveexit.png"></p>
<h6 id="替换">6.3.2 替换</h6>
<p><img src="/2022/12/16/vim/tihuan.png"> 可以从上图总结：<code>s(</code>代表本行，<code>g</code>为所有(global)都替换，<code>%</code>指本文件所有行，<code>c</code>每次替换都要求确认</p>
<h6 id="分屏">6.3.3 分屏</h6>
<p><img src="/2022/12/16/vim/fenping.png"></p>
<h6 id="其他用法扩展">6.3.4 其他用法扩展</h6>
<p><img src="/2022/12/16/vim/others.png"></p>
<h5 id="vim的配置文件所在">6.4 vim的配置文件所在</h5>
<p>局部配置文件（推荐）：<code>~/.vimrc</code> 全局配置文件:<code>/etc/vim/vimrc</code></p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/2022/10/29/Effective-C/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="trluper">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Trluper">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/10/29/Effective-C/" class="post-title-link" itemprop="url">Effective C++</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>
              

              <time title="创建时间：2022-10-29 18:26:41 / 修改时间：21:01:48" itemprop="dateCreated datePublished" datetime="2022-10-29T18:26:41+08:00">2022-10-29</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Effective-C/" itemprop="url" rel="index"><span itemprop="name">Effective C++</span></a>
                </span>
                  ，
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Effective-C/C/" itemprop="url" rel="index"><span itemprop="name">C++</span></a>
                </span>
                  ，
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Effective-C/C/%E9%9D%A2%E8%AF%95/" itemprop="url" rel="index"><span itemprop="name">面试</span></a>
                </span>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine：</span>
    
    <a title="valine" href="/2022/10/29/Effective-C/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2022/10/29/Effective-C/" itemprop="commentCount"></span>
    </a>
  </span>
  
  <br>
            <span class="post-meta-item" title="本文字数">
              <span class="post-meta-item-icon">
                <i class="far fa-file-word"></i>
              </span>
                <span class="post-meta-item-text">本文字数：</span>
              <span>1.1k</span>
            </span>
            <span class="post-meta-item" title="阅读时长">
              <span class="post-meta-item-icon">
                <i class="far fa-clock"></i>
              </span>
                <span class="post-meta-item-text">阅读时长 &asymp;</span>
              <span>1 分钟</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p><strong>在本笔记中主要对《Effective C++》一书中的重要条款做学习笔记，提取当中主要的知识点和面试考点，读者在进行<code>C plus plus</code>岗位面试前可细读该篇文章的（重要）部分。</strong></p>
<h5 id="条款1视c为一个语言联邦">1 条款1：视C++为一个语言联邦</h5>
<p>在C++中我们总会有一种错觉，那就是人为C++主要就是面向对象的编程，但是这是不全面的，作为从C中延伸的语言，它保留了C的特性，也集成了许多新的功能，你可理解成:</p>
<ul>
<li>C：以C为基础。</li>
<li>面向对象的C++：添加面向对象特性。</li>
<li>模板C++：泛型编程概念，使用模板。</li>
<li>STL：使用STL的容器、迭代器、算法、及函数对象。</li>
</ul>
<p>四者的集合</p>
<h5 id="条款2尽量用编译器替换预处理器">2 条款2：尽量用编译器替换预处理器</h5>
<p>在C/C++中，我们明白一个程序流程为<code>预处理--&gt;编译--&gt;汇编--&gt;链接</code>，在预处理阶段中，我们对文件内的进行<strong>头文件展开、宏替换和去注释</strong>等预处理操作。那么也就是说<strong>像宏定义相关的定义从未被编译器看见</strong> <figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">define</span> ASPECT_RATIO 1.645</span></span><br></pre></td></tr></table></figure> 上面的记号名称<code>ASPECT_RATIO</code>也许在编译器开始处理源码之前它就被预处理器移走了，于是该记号就没有进入<strong>记号表（在二进制文件即生成的<code>.o</code>文件中）</strong>。那么当你运用此常量但获得一个编译错误信息时，这个错误信息也许只会提到<code>1.653</code>而不是<code>ASPECT_ARTIO</code>，而它又被定义在一个非你所写的头文件，那么你对<code>1.635</code>来自何处毫无概念，你要耗费大量时间寻找它。</p>
<p>用<code>const</code>替换<code>#define</code>: <figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">const</span> <span class="type">double</span> AspectRatio=<span class="number">1.635</span>;</span><br></pre></td></tr></table></figure> 作为一个常量，它肯定能被编译器看到并放到符号表内。</p>
<p>基于上述的一个讲解，因此对于条款2有一下准则：</p>
<ul>
<li>对于单纯常量，尽量以<code>const</code>对象或<code>enums</code>枚举来代替#define。</li>
<li>若用<code>define</code>的可能会导致程序出出现多份目标码，而常量不会出现这种情况</li>
<li>取一<code>enum</code>的地址就不合法，这种行为和<code>define</code>类似因此可以用此来代替<code>define</code> ，如果你不想让别人获得一个<code>pointer</code>或<code>reference</code>指向你的某个整数常量，<code>enum</code>可以帮助你实现这个约束。</li>
<li>对于函数宏，用<code>inline</code>函数代替<code>#define</code>（<code>define</code>是死板的替换，容易产生传递计算式类似累加多次的问题）</li>
</ul>
<h5 id="条款3尽可能使用const重要">3. 条款3：尽可能使用const(重要)</h5>
<p><code>const</code>是C++中最为常用、也是最为强大的一个关键字，也是面试过程中极有可能问到的。<strong><code>const</code>可以修饰变量、指针、引用、函数、static</strong>。在说其详细应用时，我们必须先去了解<code>const</code>规则：</p>
<ul>
<li><code>const</code>修饰指针时分为底层<code>const</code>和顶层<code>const</code>。</li>
<li>非<code>const</code>变量可以赋值给<code>const</code>变量，反之不行。</li>
<li><code>const</code>常量必须被初始化</li>
<li>后续不能对<code>const</code>常量做改变</li>
</ul>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/2022/10/29/pytorch%E5%B8%B8%E7%94%A8%E5%87%BD%E6%95%B0/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="trluper">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Trluper">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/10/29/pytorch%E5%B8%B8%E7%94%A8%E5%87%BD%E6%95%B0/" class="post-title-link" itemprop="url">pytorch常用函数</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2022-10-29 15:50:21" itemprop="dateCreated datePublished" datetime="2022-10-29T15:50:21+08:00">2022-10-29</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2022-11-16 16:25:37" itemprop="dateModified" datetime="2022-11-16T16:25:37+08:00">2022-11-16</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95/" itemprop="url" rel="index"><span itemprop="name">深度学习算法</span></a>
                </span>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine：</span>
    
    <a title="valine" href="/2022/10/29/pytorch%E5%B8%B8%E7%94%A8%E5%87%BD%E6%95%B0/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2022/10/29/pytorch%E5%B8%B8%E7%94%A8%E5%87%BD%E6%95%B0/" itemprop="commentCount"></span>
    </a>
  </span>
  
  <br>
            <span class="post-meta-item" title="本文字数">
              <span class="post-meta-item-icon">
                <i class="far fa-file-word"></i>
              </span>
                <span class="post-meta-item-text">本文字数：</span>
              <span>14k</span>
            </span>
            <span class="post-meta-item" title="阅读时长">
              <span class="post-meta-item-icon">
                <i class="far fa-clock"></i>
              </span>
                <span class="post-meta-item-text">阅读时长 &asymp;</span>
              <span>13 分钟</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h4 id="torch">1. torch</h4>
<p>包 torch 包含了多维张量的数据结构以及基于其上的多种数学操作。另外，它也提供了多种工具，其中一些可以更有效地对张量和任意类型进行序列化。</p>
<p>它有 CUDA 的对应实现，可以在 NVIDIA GPU 上进行张量运算(计算能力&gt;=2.0)。</p>
<h5 id="tensor张量">1.1 Tensor张量</h5>
<h6 id="torch.numel">1.1.1 torch.numel</h6>
<p><code>torch.numel(input)-&gt;int</code>，返回 input 张量中的元素个数。</p>
<ul>
<li>参数: input (Tensor)为输入张量对象 <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>a = torch.randn(<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.numel(a)</span><br><span class="line"><span class="number">120</span></span><br></pre></td></tr></table></figure></li>
</ul>
<h5 id="创建操作">1.2 创建操作</h5>
<h6 id="torch.eye">1.2.1 torch.eye</h6>
<p><code>torch.eye(n, m=None, out=None)</code>,返回一个 2 维张量，对角线位置全 1，其它位置全 0</p>
<ul>
<li><code>n (int )</code> – 行数</li>
<li><code>m (int, optional)</code> – 列数.如果为 None,则默认为 n</li>
<li><code>out (Tensor, optinal)</code> - Output tensor</li>
</ul>
<p>返回值: 对角线位置全 1，其它位置全 0 的 2 维张量 返回值类型: Tensor <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.eye(<span class="number">3</span>)</span><br><span class="line"><span class="number">1</span> <span class="number">0</span> <span class="number">0</span></span><br><span class="line"><span class="number">0</span> <span class="number">1</span> <span class="number">0</span></span><br><span class="line"><span class="number">0</span> <span class="number">0</span> <span class="number">1</span></span><br><span class="line">[torch.FloatTensor of size 3x3]</span><br></pre></td></tr></table></figure></p>
<h6 id="from_numpy">1.2.2 from_numpy</h6>
<p><code>torch.from_numpy(ndarray) → Tensor</code>。将 numpy.ndarray 转换为 pytorch 的 Tensor。返回的张量 tensor 和 numpy的 ndarray 共享同一内存空间。修改一个会导致另外一个也被修改。返回的张量不能改变大小。 <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>a = numpy.array([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>t = torch.from_numpy(a)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>t</span><br><span class="line">torch.LongTensor([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>t[<span class="number">0</span>] = -<span class="number">1</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>a</span><br><span class="line">array([-<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>])</span><br></pre></td></tr></table></figure></p>
<h6 id="torch.linspace">1.2.3 torch.linspace</h6>
<p><code>torch.linspace(start, end, steps=100, out=None) → Tensor</code>返回一个 1 维张量，包含在区间 <code>start</code> 和 <code>end</code> 上均匀间隔的 step 个点。 输出 1 维张量的长度为</p>
<ul>
<li><code>start (float)</code> – 序列的起始点</li>
<li><code>end (float)</code> – 序列的最终值</li>
<li><code>steps (int)</code> – 在 start 和 end 间生成的样本数</li>
<li><code>out (Tensor, optional)</code> – 结果张量 <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.linspace(<span class="number">3</span>, <span class="number">10</span>, steps=<span class="number">5</span>)</span><br><span class="line"><span class="number">3.0000</span></span><br><span class="line"><span class="number">4.7500</span></span><br><span class="line"><span class="number">6.5000</span></span><br><span class="line"><span class="number">8.2500</span></span><br><span class="line"><span class="number">10.0000</span></span><br><span class="line">[torch.FloatTensor of size <span class="number">5</span>]</span><br></pre></td></tr></table></figure></li>
</ul>
<h6 id="torch.ones">1.2.4 torch.ones</h6>
<p><code>torch.ones(*sizes, out=None) → Tensor</code>.返回一个全为 1 的张量，形状由可变参数 sizes 定义。 参数:</p>
<ul>
<li><code>sizes (int...)</code> – 整数序列，定义了输出形状</li>
<li><code>out (Tensor, optional)</code> – 结果张量 <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.ones(<span class="number">2</span>, <span class="number">3</span>)</span><br><span class="line"><span class="number">1</span> <span class="number">1</span> <span class="number">1</span></span><br><span class="line"><span class="number">1</span> <span class="number">1</span> <span class="number">1</span></span><br><span class="line">[torch.FloatTensor of size 2x3]</span><br></pre></td></tr></table></figure></li>
</ul>
<h6 id="torch.rand">1.2.5 torch.rand</h6>
<p><code>torch.rand(*sizes, out=None) → Tensor</code>返回一个张量，包含了从区间[0,1)的均匀分布中抽取的一组随机数，形状由可变参数sizes 定义。 参数:</p>
<ul>
<li><code>sizes (int...)</code> – 整数序列，定义了输出形状</li>
<li><code>out (Tensor, optinal)</code> - 结果张量 <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.rand(<span class="number">4</span>)</span><br><span class="line"><span class="number">0.9193</span></span><br><span class="line"><span class="number">0.3347</span></span><br><span class="line"><span class="number">0.3232</span></span><br><span class="line"><span class="number">0.7715</span></span><br><span class="line">[torch.FloatTensor of size <span class="number">4</span>]</span><br></pre></td></tr></table></figure></li>
</ul>
<h6 id="torch.randn">1.2.6 torch.randn</h6>
<p><code>torch.randn(*sizes, out=None) → Tensor</code>.返回一个张量，包含了从标准正态分布(均值为 0，方差为 1，即高斯白噪声)中抽取一组随机数，形状由可变参数 sizes 定义。 参数:</p>
<ul>
<li><code>sizes (int...)</code> – 整数序列，定义了输出形状</li>
<li><code>out (Tensor, optinal)</code> - 结果张量 <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">torch.randn(<span class="number">2</span>, <span class="number">3</span>)</span><br><span class="line"><span class="number">1.4339</span> <span class="number">0.3351</span> -<span class="number">1.0999</span></span><br><span class="line"><span class="number">1.5458</span> -<span class="number">0.9643</span> -<span class="number">0.3558</span></span><br><span class="line">[torch.FloatTensor of size 2x3]</span><br></pre></td></tr></table></figure></li>
</ul>
<h6 id="torch.randperm">1.2.7 torch.randperm</h6>
<p><code>torch.randperm(n, out=None) → LongTensor</code>.给定参数 n，返回一个从 0 到 n-1 的随机整数排列。 参数:</p>
<ul>
<li><code>n (int)</code> – 上边界(不包含) <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.randperm(<span class="number">4</span>)</span><br><span class="line"><span class="number">2</span></span><br><span class="line"><span class="number">1</span></span><br><span class="line"><span class="number">3</span></span><br><span class="line"><span class="number">0</span></span><br><span class="line">[torch.LongTensor of size <span class="number">4</span>]</span><br></pre></td></tr></table></figure></li>
</ul>
<h6 id="torch.arange">1.2.8 torch.arange</h6>
<p><code>torch.arange(start, end, step=1, out=None) → Tensor</code>。返回一个 1 维张量，长度为 <code>floor((end−start)/step)</code>。包含从<code>start</code>到<code>end</code>，以 <code>step</code> 为步长的一组序列值(默认步长为 1)。 参数:</p>
<ul>
<li><code>start (float)</code> – 序列的起始点</li>
<li><code>end (float)</code> – 序列的终止点</li>
<li><code>step (float)</code> – 相邻点的间隔大小 <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.arange(<span class="number">1</span>, <span class="number">4</span>)</span><br><span class="line"><span class="number">1</span></span><br><span class="line"><span class="number">2</span></span><br><span class="line"><span class="number">3</span></span><br><span class="line">[torch.FloatTensor of size <span class="number">3</span>]</span><br></pre></td></tr></table></figure></li>
</ul>
<h6 id="torch.range">1.2.9 torch.range</h6>
<p><code>torch.range(start, end, step=1, out=None) → Tensor</code>。返回一个 1 维张量，有 <code>floor((end−start)/step)+1</code>个元素。包含在半开区间<code>[start, end）</code>从 start开始，以 step 为步长的一组值。 step 是两个值之间的间隔，即 <code>xi+1=xi+step</code></p>
<p><strong><em>警告：建议使用函数 torch.arange()</em></strong></p>
<ul>
<li><code>start (float)</code> – 序列的起始点</li>
<li><code>end (float)</code> – 序列的最终值</li>
<li><code>step (int)</code> – 相邻点的间隔大小 <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt;torch.<span class="built_in">range</span>(<span class="number">1</span>, <span class="number">4</span>)</span><br><span class="line"><span class="number">1</span></span><br><span class="line"><span class="number">2</span></span><br><span class="line"><span class="number">3</span></span><br><span class="line"><span class="number">4</span></span><br><span class="line">[torch.FloatTensor of size <span class="number">4</span>]</span><br></pre></td></tr></table></figure></li>
</ul>
<h6 id="torch.tensor">1.2.10 torch.tensor</h6>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">torch.tensor(data, </span><br><span class="line">             dtype=<span class="literal">None</span>, </span><br><span class="line">             device=<span class="literal">None</span>, </span><br><span class="line">             requires_grad=<span class="literal">False</span>, </span><br><span class="line">             pin_memory=<span class="literal">False</span>) -&gt; Tensor</span><br></pre></td></tr></table></figure>
<ul>
<li><code>data</code>：数据，可以是list，也可以是numpy</li>
<li><code>dtype</code>：数据类型，默认和data一致</li>
<li><code>device</code>：tensor所在的设备</li>
<li><code>requires_grad</code>：是否需要梯度，默认False，在搭建神经网络时需要将求导的参数设为True</li>
<li><code>pin_memory</code>：是否存于锁页内存，默认False</li>
</ul>
<h6 id="torch.zeros">1.2.11 torch.zeros</h6>
<p>返回一个全为标量 0 的张量，形状由可变参数 sizes 定义。 <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">torch.zeros(*size, </span><br><span class="line">            out=<span class="literal">None</span>, </span><br><span class="line">            dtype=<span class="literal">None</span>, </span><br><span class="line">            layout=torch.strided, </span><br><span class="line">            device=<span class="literal">None</span>, </span><br><span class="line">            requires_grad=<span class="literal">False</span>) -&gt; Tensor</span><br></pre></td></tr></table></figure> - <code>size</code>：张量的形状，如（3，3） - <code>layout</code> ：这个是内存中的布局形式,有strided和sparse_coo等 - <code>out</code>：表示输出张量，就是再把这个张量赋值给别的一个张量，但是这两个张量时一样的，指的同一个内存地址 - <code>device</code>：所在的设备，gpu/cpu - <code>requires_grad</code>：是否需要梯度</p>
<h5 id="tensor的常用操作">1.3 Tensor的常用操作</h5>
<p>对于tensor数据我们经常会进行切片、索引、降维、升维、连接等操作。这里介绍一下我们经常用到的。</p>
<h6 id="torch.cat">1.3.1 torch.cat</h6>
<p><code>torch.cat(inputs, dimension=0) → Tensor</code>。在<strong>给定维度</strong>上对输入的张量序列 seq 进行连接操作。<code>torch.cat()</code>可以看做 <code>torch.split()</code>和 <code>torch.chunk()</code>的反操作。</p>
<ul>
<li><code>inputs (sequence of Tensors)</code> – 可以是任意相同 Tensor 类型的 python 序列</li>
<li><code>dimension (int, optional)</code> – 沿着此维连接张量序列。 <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#沿着0维合并</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>x = torch.randn(<span class="number">2</span>, <span class="number">3</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>x</span><br><span class="line"><span class="number">0.5983</span> -<span class="number">0.0341</span> <span class="number">2.4918</span></span><br><span class="line"><span class="number">1.5981</span> -<span class="number">0.5265</span> -<span class="number">0.8735</span></span><br><span class="line">[torch.FloatTensor of size 2x3]</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.cat((x, x, x), <span class="number">0</span>)</span><br><span class="line"><span class="number">0.5983</span> -<span class="number">0.0341</span> <span class="number">2.4918</span></span><br><span class="line"><span class="number">1.5981</span> -<span class="number">0.5265</span> -<span class="number">0.8735</span></span><br><span class="line"><span class="number">0.5983</span> -<span class="number">0.0341</span> <span class="number">2.4918</span></span><br><span class="line"><span class="number">1.5981</span> -<span class="number">0.5265</span> -<span class="number">0.8735</span></span><br><span class="line"><span class="number">0.5983</span> -<span class="number">0.0341</span> <span class="number">2.4918</span></span><br><span class="line"><span class="number">1.5981</span> -<span class="number">0.5265</span> -<span class="number">0.8735</span></span><br><span class="line">[torch.FloatTensor of size 6x3]</span><br></pre></td></tr></table></figure></li>
</ul>
<h6 id="torch.chunk">1.3.2 torch.chunk</h6>
<p><code>torch.chunk(tensor, chunks, dim=0)</code>。在给定维度(轴)上将输入张量进行分块儿。</p>
<ul>
<li><code>tensor (Tensor)</code> – 待分块的输入张量</li>
<li><code>chunks (int)</code> – 分块的个数</li>
<li><code>dim (int)</code> – 沿着此维度进行分块</li>
</ul>
<h6 id="torch.split">1.3.3 torch.split</h6>
<p><code>torch.split(tensor, split_size, dim=0)</code>。将输入张量分割成相等形状的 chunks（如果可分）。 如果沿指定维的张量形状大小不能被<code>split_size</code>整分， 则最后一个分块会小于其它分块。</p>
<ul>
<li><code>tensor (Tensor)</code> – 待分割张量</li>
<li><code>split_size (int)</code> – 单个分块的形状大小</li>
<li><code>dim (int)</code> – 沿着此维进行分割</li>
</ul>
<h6 id="torch.squeeze">1.3.4 torch.squeeze</h6>
<p><code>torch.squeeze(input, dim=None, out=None)</code>。当未给定<code>dim</code>时，将输入张量形状中维度为1 去除并返回。 如果输入是形如<code>(A×1×B×1×C×1×D)</code>，那么输出形状就为：<code>(A×B×C×D)</code></p>
<p>当给定 dim 时 ， 那 么 挤 压 操 作 只 在 给 定 维 度 上 。 例 如 ， 输 入 形 状为: <code>(A×1×B)</code>, <code>squeeze(input, 0)</code> 将会保持张量不变，只有用 <code>squeeze(input, 1)</code>，形状会变成<code>(A×B)</code>。</p>
<p><strong><em>注意： 返回张量与输入张量共享内存，所以改变其中一个的内容会改变另一个。</em></strong> 参数:</p>
<ul>
<li><code>input (Tensor)</code> – 输入张量</li>
<li><code>dim (int, optional)</code> – 如果给定，则 input 只会在给定维度挤压</li>
<li><code>out (Tensor, optional)</code> – 输出张量 <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>x = torch.zeros(<span class="number">2</span>,<span class="number">1</span>,<span class="number">2</span>,<span class="number">1</span>,<span class="number">2</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>x.size()</span><br><span class="line">(<span class="number">2L</span>, <span class="number">1L</span>, <span class="number">2L</span>, <span class="number">1L</span>, <span class="number">2L</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>y = torch.squeeze(x)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>y.size()</span><br><span class="line">(<span class="number">2L</span>, <span class="number">2L</span>, <span class="number">2L</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>y = torch.squeeze(x, <span class="number">0</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>y.size()</span><br><span class="line">(<span class="number">2L</span>, <span class="number">1L</span>, <span class="number">2L</span>, <span class="number">1L</span>, <span class="number">2L</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>y = torch.squeeze(x, <span class="number">1</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>y.size()</span><br><span class="line">(<span class="number">2L</span>, <span class="number">2L</span>, <span class="number">1L</span>, <span class="number">2L</span>)</span><br></pre></td></tr></table></figure></li>
</ul>
<h6 id="torch.stacksource">1.3.5 torch.stack[source]</h6>
<p><code>torch.stack(sequence, dim=0)</code>。沿着一个新维度对输入张量序列进行连接。 序列中所有的张量都应该为相同形状。</p>
<p>参数:</p>
<ul>
<li><code>sqequence (Sequence)</code> – 待连接的张量序列</li>
<li><code>dim (int)</code> – 插入的维度。必须介于 0 与 待连接的张量序列数之间。</li>
</ul>
<h6 id="torch.transpose">1.3.6 torch.transpose</h6>
<p><code>torch.transpose(input, dim0, dim1, out=None) → Tensor</code>。交换维度 dim0 和 dim1。 输出张量与输入张量共享内存，所以改变其中一个会导致另外一个也被修改。</p>
<ul>
<li><code>input (Tensor)</code> – 输入张量</li>
<li><code>dim0 (int)</code> – 转置的第一维</li>
<li><code>dim1 (int)</code> – 转置的第二维 <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>x = torch.randn(<span class="number">2</span>, <span class="number">3</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>x</span><br><span class="line"><span class="number">0.5983</span> -<span class="number">0.0341</span> <span class="number">2.4918</span></span><br><span class="line"><span class="number">1.5981</span> -<span class="number">0.5265</span> -<span class="number">0.8735</span></span><br><span class="line">[torch.FloatTensor of size 2x3]</span><br></pre></td></tr></table></figure></li>
</ul>
<h6 id="torch.unsqueeze">1.3.7 torch.unsqueeze</h6>
<p><code>torch.unsqueeze(input, dim, out=None)</code>。返回一个新的张量，对输入的制定位置插入维度1 注意： 返回张量与输入张量共享内存，所以改变其中一个的内容会改变另一个。如果 <code>dim</code> 为负，则将会被转化 <code>dim+input.dim()+1</code> 参数:</p>
<ul>
<li><code>tensor (Tensor)</code> – 输入张量</li>
<li><code>dim (int)</code> – 插入维度的索引</li>
<li><code>out (Tensor, optional)</code> – 结果张量</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>x = torch.Tensor([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.unsqueeze(x, <span class="number">0</span>)</span><br><span class="line"><span class="number">1</span> <span class="number">2</span> <span class="number">3</span> <span class="number">4</span></span><br><span class="line">[torch.FloatTensor of size 1x4]</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.unsqueeze(x, <span class="number">1</span>)</span><br><span class="line"><span class="number">1</span></span><br><span class="line"><span class="number">2</span></span><br><span class="line"><span class="number">3</span></span><br><span class="line"><span class="number">4</span></span><br><span class="line">[torch.FloatTensor of size 4x1]</span><br></pre></td></tr></table></figure>
<h4 id="nn模块">2. nn模块</h4>
<p>首先应该知道在nn模块当中有两个重要的，分别是<code>torch.nn.Module</code>和<code>nn.functional</code>。<code>torch.nn.Module</code>是所有网络的基类。你的模型也应该继承这个类。 ##### 2.1 一般函数 ###### 2.1.1 nn.ModuleList() 将<code>submodules</code>保存在一个<code>list</code>中。ModuleList 可以像一般的 Python list 一样被索引。而且 ModuleList 中包含的 modules 已经被正确的注册，对所有的 module method 可见。它与<code>Sequential()</code>相似，只是为了集成一些操作便于管理。 <figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> out_channel in vgg_out_channels:</span><br><span class="line">    #M为最大池化操作</span><br><span class="line">    <span class="keyword">if</span> out_channel == <span class="string">&#x27;M&#x27;</span>:</span><br><span class="line">        layers += [nn.<span class="built_in">MaxPool2d</span>(kernel_size=<span class="number">2</span>, stride=<span class="number">2</span>)]</span><br><span class="line">    #其他为卷积，输出为指定值，刚开始为<span class="number">3</span>通道</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        conv2d = nn.<span class="built_in">Conv2d</span>(in_channel, out_channel, <span class="number">3</span>, <span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">        layers += [conv2d, nn.<span class="built_in">ReLU</span>(inplace=True)]</span><br><span class="line">        in_channel = out_channel</span><br><span class="line">self.vgg = nn.<span class="built_in">ModuleList</span>(layers)</span><br><span class="line"></span><br><span class="line">使用：</span><br><span class="line">feats = self.vgg[idx](feats)</span><br></pre></td></tr></table></figure></p>
<h6 id="nn.sequential">2.1.2 nn.Sequential()</h6>
<p><code>torch.nn.Sequential</code>是一个<code>Sequential</code>容器，模块将按照构造函数中传递的顺序添加到模块中。另外，也可以传入一个有序模块。具体理解如下： <figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">self.covn1=nn.<span class="built_in">Sequential</span>(nn.<span class="built_in">Conv2d</span>(<span class="number">96</span>, <span class="number">96</span>, <span class="number">3</span>, <span class="number">1</span>, <span class="number">1</span>), nn.<span class="built_in">BatchNorm2d</span>(<span class="number">96</span>), nn.<span class="built_in">ReLU</span>(inplace=True),</span><br><span class="line">              nn.<span class="built_in">Conv2d</span>(<span class="number">96</span>, <span class="number">32</span>, <span class="number">3</span>, <span class="number">1</span>, <span class="number">1</span>), nn.<span class="built_in">BatchNorm2d</span>(<span class="number">32</span>), nn.<span class="built_in">ReLU</span>(inplace=True))</span><br><span class="line"></span><br><span class="line">使用：</span><br><span class="line">features=self.<span class="built_in">covn1</span>(features)</span><br></pre></td></tr></table></figure></p>
<h6 id="torch.nn.parameter">2.1.3 torch.nn.Parameter()</h6>
<p><code>torch.nn.Parameter</code>继承torch.Tensor，其作用将一个不可训练的类型为Tensor的参数转化为可训练的类型为<code>parameter</code>的参数，并将这个参数绑定到<code>module</code>里面，成为<code>module</code>中可训练的参数。 <figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.nn.<span class="built_in">Parameter</span>(Tensor data, <span class="type">bool</span> requires_grad)</span><br></pre></td></tr></table></figure> 其中：data为传入Tensor类型参数，requires_grad默认值为True，表示可训练，False表示不可训练。</p>
<h5 id="卷积函数和层次常用">2.2 卷积函数和层次常用</h5>
<h6 id="conv1d">2.2.1 Conv1d</h6>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">torch.nn.<span class="built_in">Conv1d</span>(in_channels, out_channels, kernel_size, stride=<span class="number">1</span>, padding=<span class="number">0</span>, dilation=<span class="number">1</span>, </span><br><span class="line">groups=<span class="number">1</span>, bias=True)</span><br></pre></td></tr></table></figure>
<h6 id="conv2d">2.2.2 Conv2d</h6>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">torch.nn.<span class="built_in">Conv2d</span>(in_channels, </span><br><span class="line">                out_channels, </span><br><span class="line">                kernel_size, </span><br><span class="line">                stride=<span class="number">1</span>, </span><br><span class="line">                padding=<span class="number">0</span>, </span><br><span class="line">                dilation=<span class="number">1</span>, </span><br><span class="line">                groups=<span class="number">1</span>, </span><br><span class="line">                bias=True, </span><br><span class="line">                padding_mode=<span class="string">&#x27;zeros&#x27;</span>)</span><br></pre></td></tr></table></figure>
<ul>
<li><code>in_channels(int)</code> – 输入信号的通道</li>
<li><code>out_channels(int)</code> – 卷积产生的通道</li>
<li><code>kerner_size(int or tuple)</code> - 卷积核的尺寸</li>
<li><code>stride(int or tuple, optional)</code> - 卷积步长</li>
<li><code>padding(int or tuple, optional)</code> - 输入的每一条边补充 0 的层数</li>
<li><code>dilation(int or tuple, optional)</code> – <strong>定义了卷积核处理数据时各值的间距。换句话说，相比原来的标准卷积，扩张卷积多了一个超参数称之为dilation rate（扩张率），指的是kernel各点之间的间隔数量，正常的卷积核的dilation为1。</strong></li>
<li><code>groups(int, optional)</code> – 从输入通道到输出通道的阻塞连接数</li>
<li><code>bias(bool, optional)</code> - 如果 bias=True，添加偏置</li>
</ul>
<p><img src="/2022/10/29/pytorch%E5%B8%B8%E7%94%A8%E5%87%BD%E6%95%B0/dialation.gif" width="400"> 上图是一个扩张率为2，尺寸为 3×3 的空洞卷积，感受野与5×5的卷积核相同，而且仅需要9个参数。在相同的计算条件下，空洞卷积提供了更大的感受野，获得更为丰富的上下文信息。</p>
<h6 id="conv3d">2.2.3 Conv3d</h6>
<p>三维卷积层, 输入的尺度是<code>(N, C_in,D,H,W)</code>，输出尺度<code>（N,C_out,D_out,H_out,W_out）</code> <figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">torch.nn.<span class="built_in">Conv3d</span>(in_channels, out_channels, kernel_size, stride=<span class="number">1</span>, padding=<span class="number">0</span>, dilation=<span class="number">1</span>, </span><br><span class="line">groups=<span class="number">1</span>, bias=True)</span><br></pre></td></tr></table></figure></p>
<h6 id="convtranspose1d">2.2.4 ConvTranspose1d</h6>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">torch.nn.<span class="built_in">ConvTranspose1d</span>(in_channels, out_channels, kernel_size, stride=<span class="number">1</span>, padding=<span class="number">0</span>, </span><br><span class="line">output_padding=<span class="number">0</span>, groups=<span class="number">1</span>, bias=True)</span><br></pre></td></tr></table></figure>
<p>1 维的解卷积操作（transposed convolution operator，注意改视作操作可视作解卷积操作，但并不是真正的解卷积操作） <strong>该模块可以看作是 Conv1d 相对于其输入的梯度</strong>，有时（但不正确地）被称为解卷积操作</p>
<p><strong><em>注意：由于内核的大小，输入的最后的一些列的数据可能会丢失。因为输入和输出是不是完全的互相关。因此，用户可以进行适当的填充（padding 操作）。</em></strong></p>
<h6 id="convtranspose2d">2.2.5 ConvTranspose2d</h6>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">torch.nn.<span class="built_in">ConvTranspose2d</span>(in_channels, out_channels, kernel_size, stride=<span class="number">1</span>, padding=<span class="number">0</span>, </span><br><span class="line">output_padding=<span class="number">0</span>, groups=<span class="number">1</span>, bias=True)</span><br></pre></td></tr></table></figure>
<p>2 维的转置卷积操作（transposed convolution operator，注意改视作操作可视作解卷积操作，但并不是真正的解卷积操作） 该模块可以看作是 Conv2d 相对于其输入的梯度，有时（但不正确地）被称为解卷积操作。</p>
<h6 id="nn.linear">2.2.6 nn.Linear</h6>
<p>对输入数据做线性变换：$y=Ax+b#，也是我们MLP中使用的函数 <figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.nn.<span class="built_in">Linear</span>(in_features, out_features, bias=True)</span><br></pre></td></tr></table></figure> - <code>in_features</code> - 每个输入样本的大小 - <code>out_features</code> - 每个输出样本的大小 - <code>bias</code> - 若设置为 False，这层不会学习偏置。默认值：True</p>
<h6 id="dropout">2.2.7 Dropout</h6>
<p>随机以概率p将输入张量中部分元素设置为 0。对于每次前向调用，被置 0 的元素都是随机的 <figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.nn.<span class="built_in">Dropout</span>(p=<span class="number">0.5</span>, inplace=False)</span><br></pre></td></tr></table></figure> - <code>p</code> - 将元素置 0 的概率。默认值：0.5 - <code>in-place</code> - 若设置为 True，会在原地执行操作。默认值：False</p>
<h5 id="池化函数">2.3 池化函数</h5>
<h6 id="maxpool2d">2.3.1 MaxPool2d</h6>
<p>对于输入信号的输入通道，提供 2 维最大池化（max pooling）操作。如果输入的大小是<code>(N,C,H,W)</code>，那么输出的大小是<code>(N,C,H_out,W_out)</code>。即只会改变矩阵大小，不会改变通道数量。 <figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">nn.<span class="built_in">MaxPool2d</span>(kernel_size, stride=None, padding=<span class="number">0</span>, dilation=<span class="number">1</span>, return_indices=False, ceil_mode=False)</span><br></pre></td></tr></table></figure> - <code>kernel_size(int or tuple)</code> - max pooling的窗口大小 - <code>stride(int or tuple, optional)</code> - max pooling的窗口移动的步长。默认值是kernel_size - <code>padding(int or tuple, optional)</code> - 输入的每一条边补充0的层数 - <code>dilation(int or tuple, optional)</code> – 每个点之间的间隙，空洞卷积会用到 - <code>return_indices</code> - 如果等于True，会返回输出最大值的序号，对于上采样操作会有帮助 - <code>ceil_mode</code> - 如果等于True，计算输出信号大小的时候，会使用向上取整，代替默认的向下取整的操作</p>
<h6 id="maxunpool2d">2.3.2 MaxUnpool2d</h6>
<p>Maxpool2d的逆过程，不过并不是完全的逆过程，因为在 maxpool2d 的过程中，一些值的已经丢失。 MaxUnpool2d 的输入是 MaxPool2d 的输出，包括最大值的索引，并计算所有 maxpool2d 过程中非最大值被设置为零的部分的反向 <figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.nn.<span class="built_in">MaxUnpool2d</span>(kernel_size, stride=None, padding=<span class="number">0</span>)</span><br></pre></td></tr></table></figure> <strong><em>注意</em></strong>：注意：MaxPool2d 可以将多个输入大小映射到相同的输出大小。因此，反演过程可能会变得模棱两可。 为了适应这一点，可以在调用中将输出大小（output_size）作为额外的参数传入。 <figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; pool = nn.<span class="built_in">MaxPool2d</span>(<span class="number">2</span>, stride=<span class="number">2</span>, return_indices=True)</span><br><span class="line">&gt;&gt;&gt; unpool = nn.<span class="built_in">MaxUnpool2d</span>(<span class="number">2</span>, stride=<span class="number">2</span>)</span><br><span class="line">&gt;&gt;&gt; input = <span class="built_in">Variable</span>(torch.<span class="built_in">Tensor</span>([[[[ <span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>],</span><br><span class="line"> ... [ <span class="number">5</span>, <span class="number">6</span>, <span class="number">7</span>, <span class="number">8</span>],</span><br><span class="line"> ... [ <span class="number">9</span>, <span class="number">10</span>, <span class="number">11</span>, <span class="number">12</span>],</span><br><span class="line"> ... [<span class="number">13</span>, <span class="number">14</span>, <span class="number">15</span>, <span class="number">16</span>]]]]))</span><br><span class="line">&gt;&gt;&gt; output, indices = <span class="built_in">pool</span>(input)</span><br><span class="line">&gt;&gt;&gt; <span class="built_in">unpool</span>(output, indices)</span><br><span class="line"> Variable containing:</span><br><span class="line"> (<span class="number">0</span> ,<span class="number">0</span> ,.,.) =</span><br><span class="line"> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span></span><br><span class="line"></span><br><span class="line"> <span class="number">0</span> <span class="number">6</span> <span class="number">0</span> <span class="number">8</span></span><br><span class="line"> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span></span><br><span class="line"> <span class="number">0</span> <span class="number">14</span> <span class="number">0</span> <span class="number">16</span></span><br><span class="line"> [torch.FloatTensor of size <span class="number">1</span>x1x4x4]</span><br><span class="line">&gt;&gt;&gt; <span class="meta"># specify a different output size than input size</span></span><br><span class="line">&gt;&gt;&gt; <span class="built_in">unpool</span>(output, indices, output_size=torch.<span class="built_in">Size</span>([<span class="number">1</span>, <span class="number">1</span>, <span class="number">5</span>, <span class="number">5</span>]))</span><br><span class="line"> Variable containing:</span><br><span class="line"> (<span class="number">0</span> ,<span class="number">0</span> ,.,.) =</span><br><span class="line"> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span></span><br><span class="line"> <span class="number">6</span> <span class="number">0</span> <span class="number">8</span> <span class="number">0</span> <span class="number">0</span></span><br><span class="line"> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">14</span> <span class="number">0</span></span><br><span class="line"> <span class="number">16</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span></span><br><span class="line"> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span></span><br><span class="line"> [torch.FloatTensor of size <span class="number">1</span>x1x5x5]</span><br><span class="line"><span class="keyword">class</span> torch.nn.<span class="built_in">MaxUnpool3d</span>(kernel_size, stride=None, padding=<span class="number">0</span>)</span><br></pre></td></tr></table></figure></p>
<h6 id="avgpool2d">2.3.3 AvgPool2d</h6>
<p>对信号的输入通道，提供 2 维的平均池化（average pooling ）。输入信号的大小(N,C,H,W)，输出大小(N,C,H_out,W_out)和池化窗口大小(kH,kW)的关系是 <figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">torch.nn.<span class="built_in">AvgPool2d</span>(kernel_size, stride=None, padding=<span class="number">0</span>, ceil_mode=False, </span><br><span class="line">count_include_pad=True)</span><br></pre></td></tr></table></figure></p>
<h6 id="adaptivemaxpool2d">2.3.4 AdaptiveMaxPool2d</h6>
<p>对输入信号，提供 1 维的自适应最大池化操作 对于任何输入大小的输入，可以将输出尺寸指定为<code>H</code>，但是输入和输出特征的数目不会变化 <figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.nn.<span class="built_in">AdaptiveMaxPool2d</span>(output_size, return_indices=False)</span><br></pre></td></tr></table></figure> - <code>output_size</code>: 输出信号的尺寸</p>
<ul>
<li><code>return_indices</code>: 如果设置为 True，会返回输出的索引。对 nn.MaxUnpool1d 有用，默认值是 False</li>
</ul>
<h5 id="归一化函数">2.4 归一化函数</h5>
<h6 id="batchnorm2d">2.4.1 BatchNorm2d</h6>
<p>对于所有的batch中样本的同一个channel的数据元素进行标准化处理，即如果有C个通道，无论batch中有多少个样本，都会在通道维度上进行标准化处理，一共进行C次。</p>
<p><span class="math display">\[

y={{x-mean(x)}\over{\sqrt{var(x)+eps}}}×γ+β

\]</span></p>
<ul>
<li>在训练时，该层计算每次输入的均值与方差，并进行移动平均。移动平均默认的动量值为 0.1。</li>
<li>在验证时，训练求得的均值/方差将用于标准化验证数据。</li>
</ul>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.nn.<span class="built_in">BatchNorm2d</span>(num_features, eps=<span class="number">1e-05</span>, momentum=<span class="number">0.1</span>, affine=True)</span><br></pre></td></tr></table></figure>
<ul>
<li><code>num_features</code>： 来自期望输入的特征数，该期望输入的大小为<code>batch_size × num_features × height × width</code></li>
<li><code>eps</code>： 为保证数值稳定性（分母不能趋近或取 0）,给分母加上的值。默认为 1e-5。</li>
<li><code>momentum</code>： 动态均值和动态方差所使用的动量。默认为 0.1。</li>
<li><code>affine</code>： 一个布尔值，当设为 true，给该层添加可学习的仿射变换参数。</li>
</ul>
<p>momentum的作用：BatchNorm2d里面存储均值（running_mean）和方差（running_var）更新时的参数。 <span class="math display">\[
x_{new}=(1-momentum)*x_{old}+momentum*x_{obser}
\]</span> 其中<span class="math inline">\(x_{old}\)</span>为BatchNorm2d里面的均值<code>（running_mean）</code>和方差<code>（running_var）</code>,<span class="math inline">\(x_{0bser}\)</span>为当前观测值（样本）的均值或方差,<span class="math inline">\(x_{new}\)</span>更新后的均值或方差（最后需要重新存储到BatchNorm2d中），momentum为更新参数</p>
<h5 id="functional模块">2.5 functional模块</h5>
<h6 id="interpolate函数">2.5.1 interpolate()函数</h6>
<p><code>torch.nn.functional.interpolate</code>实现插值和上采样。上采样，在深度学习框架中，可以简单的理解为任何可以让你的图像变成更高分辨率的技术即矩阵升维。 最简单的方式是重采样和插值：将输入图片input image进行rescale到一个想要的尺寸，而且计算每个点的像素点，使用如<strong>双线性插值<code>bilinear</code></strong>等插值方法对其余点进行插值。 <figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.nn.functional.<span class="built_in">interpolate</span>(input, size=None, scale_factor=None, mode=<span class="string">&#x27;nearest&#x27;</span>, align_corners=None)</span><br></pre></td></tr></table></figure> - <code>input (Tensor)</code> – 输入张量</p>
<ul>
<li><p><code>size (int or Tuple[int] or Tuple[int, int] or Tuple[int, int, int])</code> – 输出大小.</p></li>
<li><p><code>scale_factor (float or Tuple[float])</code> – 指定输出为输入的多少倍数。如果输入为tuple，其也要制定为tuple类型</p></li>
<li><p><code>mode (str)</code> – 可使用的上采样算法，有’nearest’, ‘linear’, ‘bilinear’, ‘bicubic’ , ‘trilinear’和’area’. 默认使用’nearest</p></li>
</ul>
<h5 id="nn.module">2.6 nn.Module</h5>
<p>注意下面的这些函数都是<code>Module</code>这个类的成员函数，不同于上面的一般函数为全局函数。下面的<code>modle</code>都指代模型对象</p>
<h6 id="parameter">2.6.1 parameter()</h6>
<p><code>model.parameters()</code>，这个是成员函数,指获取该模型的所有参数，一般我们再把模型参数传递给优化器是会使用到。 <figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">params = model.<span class="built_in">parameters</span>()</span><br><span class="line">optimizer = torch.optim.<span class="built_in">Adam</span>(params, config.TRAIN[<span class="string">&#x27;learning_rate&#x27;</span>])</span><br></pre></td></tr></table></figure></p>
<h6 id="add_modulename-module">2.6.2 add_module(name, module)</h6>
<p>该函数是一个成岩函数。将一个 <code>child module</code> 添加到当前 <code>modle</code>。 被添加的 <code>module</code> 可以通过 <code>name</code> 属性来获取 <figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">add_module</span>(name, <span class="keyword">module</span>)</span><br><span class="line">例子：</span><br><span class="line"><span class="keyword">import</span> torch.<span class="function">nn as nn</span></span><br><span class="line"><span class="function"><span class="keyword">class</span> <span class="title">Model</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line"><span class="function"> def __init__(self):</span></span><br><span class="line"><span class="function"> super(Model, self).__init__()</span></span><br><span class="line"><span class="function"> self.add_module(<span class="string">&quot;conv&quot;</span>, nn.Conv2d(<span class="number">10</span>, <span class="number">20</span>, <span class="number">4</span>))</span></span><br><span class="line"><span class="function"> #self.conv =</span> nn.<span class="built_in">Conv2d</span>(<span class="number">10</span>, <span class="number">20</span>, <span class="number">4</span>) 和上面这个增加 <span class="keyword">module</span> 的方式等价</span><br><span class="line">model = <span class="built_in">Model</span>()</span><br></pre></td></tr></table></figure></p>
<h6 id="children">2.6.3 children()</h6>
<p>该函数返回当前模型子模块的迭代器。 <figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch.<span class="function">nn as nn</span></span><br><span class="line"><span class="function"><span class="keyword">class</span> <span class="title">Model</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line"><span class="function"> def __init__(self):</span></span><br><span class="line"><span class="function"> super(Model, self).__init__()</span></span><br><span class="line"><span class="function"> self.add_module(<span class="string">&quot;conv&quot;</span>, nn.Conv2d(<span class="number">10</span>, <span class="number">20</span>, <span class="number">4</span>))</span></span><br><span class="line"><span class="function"> self.add_module(<span class="string">&quot;conv1&quot;</span>, nn.Conv2d(<span class="number">20</span> ,<span class="number">10</span>, <span class="number">4</span>))</span></span><br><span class="line"><span class="function">model =</span> <span class="built_in">Model</span>()</span><br><span class="line"><span class="keyword">for</span> sub_module in model.<span class="built_in">children</span>():</span><br><span class="line"> <span class="built_in">print</span>(sub_module)</span><br><span class="line"> </span><br><span class="line">输出：</span><br><span class="line"><span class="built_in">Conv2d</span>(<span class="number">10</span>, <span class="number">20</span>, kernel_size=(<span class="number">4</span>, <span class="number">4</span>), stride=(<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line"><span class="built_in">Conv2d</span>(<span class="number">20</span>, <span class="number">10</span>, kernel_size=(<span class="number">4</span>, <span class="number">4</span>), stride=(<span class="number">1</span>, <span class="number">1</span>))</span><br></pre></td></tr></table></figure></p>
<h6 id="cpudevice_idnone和cudadevice_idnone">2.6.4 cpu(device_id=None)和cuda(device_id=None)</h6>
<p><code>cpu(device_id=None)</code>和cuda<code>(device_id=None)</code>将所有的模型参数(parameters)和 buffers 赋值给CPU或者GPU <figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">modle=modle.<span class="built_in">cuda</span>()</span><br><span class="line"><span class="keyword">for</span> i, (images, gts, depths) in <span class="built_in">enumerate</span>(train_loader, start=<span class="number">1</span>):</span><br><span class="line">	optimizer.<span class="built_in">zero_grad</span>()</span><br><span class="line">	images = images.<span class="built_in">cuda</span>()</span><br><span class="line">	gts = gts.<span class="built_in">cuda</span>()</span><br><span class="line">	depths=depths.<span class="built_in">cuda</span>()</span><br></pre></td></tr></table></figure> 上面的式子等价于<code>to()</code>函数： <figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">device = torch.<span class="built_in">device</span>(<span class="string">&quot;cuda&quot;</span>)</span><br><span class="line">images, gts, depths, = images.<span class="built_in">to</span>(device), gts.<span class="built_in">to</span>(device), depths.<span class="built_in">to</span>(device)</span><br></pre></td></tr></table></figure></p>
<h6 id="eval和train模式">2.6.5 eval()和train模式</h6>
<p><code>eval()</code>将模型设置成 evaluation 模式仅仅当模型中有 Dropout 和 BatchNorm 是才会有影响；<code>train()</code>将模型设置为训练模式，会有优化操作，即梯度下降和反向传播算法。</p>
<h6 id="state_dict-dict">2.6.6 state_dict (dict)</h6>
<p><code>state_dict</code>是一个简单的python的字典对象,将每一层与它的对应参数建立映射关系.(如model的每一层的weights及偏置等等)。注意只有那些参数可以训练的layer才会被保存到模型的state_dict中,如卷积层,线性层等等</p>
<ul>
<li><p><code>state_dict</code>是在定义了<code>model</code>或<code>optimizer</code>之后pytorch自动生成的,可以直接调用.常用的保存<code>state_dict</code>的格式是&quot;.pt&quot;或'.pth'的文件,即下面命令的 PATH=&quot;./***.pt&quot; <figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.<span class="built_in">save</span>(model.<span class="built_in">state_dict</span>(), PATH)</span><br></pre></td></tr></table></figure></p></li>
<li><p><code>load_state_dict</code>也是<code>model</code>或<code>optimizer</code>之后pytorch自动具备的函数,可以直接调用 <figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">model = <span class="built_in">TheModelClass</span>(*args, **kwargs)</span><br><span class="line">model.<span class="built_in">load_state_dict</span>(torch.<span class="built_in">load</span>(PATH))</span><br><span class="line">model.<span class="built_in">eval</span>()</span><br></pre></td></tr></table></figure></p></li>
</ul>
<h6 id="modules">2.6.7 modules()</h6>
<p>返回一个包含 当前模型 所有模块的迭代器。 <figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> <span class="keyword">module</span> in model.<span class="built_in">modules</span>():</span><br><span class="line">	<span class="built_in">print</span>(<span class="keyword">module</span>)</span><br><span class="line"> </span><br><span class="line">输出：</span><br><span class="line"><span class="built_in">Model</span> (</span><br><span class="line">	(conv): <span class="built_in">Conv2d</span>(<span class="number">10</span>, <span class="number">20</span>, kernel_size=(<span class="number">4</span>, <span class="number">4</span>), stride=(<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">	(conv1): <span class="built_in">Conv2d</span>(<span class="number">20</span>, <span class="number">10</span>, kernel_size=(<span class="number">4</span>, <span class="number">4</span>), stride=(<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">)</span><br><span class="line"><span class="built_in">Conv2d</span>(<span class="number">10</span>, <span class="number">20</span>, kernel_size=(<span class="number">4</span>, <span class="number">4</span>), stride=(<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line"><span class="built_in">Conv2d</span>(<span class="number">20</span>, <span class="number">10</span>, kernel_size=(<span class="number">4</span>, <span class="number">4</span>), stride=(<span class="number">1</span>, <span class="number">1</span>))</span><br></pre></td></tr></table></figure> 可以看出，<code>modules()</code>返回的 iterator 不止包含子模块，还有父模块。这是和 <code>children()</code>的不同。</p>
<h6 id="named_children">2.6.8 named_children()</h6>
<p>返回包含模型当前子模块的迭代器，yield 模块<strong>名字和模块本身</strong>。 <figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> name, <span class="keyword">module</span> in model.<span class="built_in">named_children</span>():</span><br><span class="line">	<span class="keyword">if</span> name in [<span class="string">&#x27;conv4&#x27;</span>, <span class="string">&#x27;conv5&#x27;</span>]:</span><br><span class="line">	<span class="built_in">print</span>(<span class="keyword">module</span>)</span><br></pre></td></tr></table></figure> 同理有<code>name_modules</code></p>
<h5 id="激活函数">2.7 激活函数</h5>
<h6 id="relu">2.7.1 ReLU</h6>
<p>对输入运用修正线性单元函数<span class="math inline">\({ReLU}(x)= max(0, x)\)</span>， <figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.nn.<span class="built_in">ReLU</span>(inplace=False)</span><br></pre></td></tr></table></figure></p>
<h6 id="sigmoid">2.7.2 Sigmoid</h6>
<p>对每个元素运用 Sigmoid 函数，Sigmoid 定义如下：<span class="math inline">\(f(x)=1/(1+e^{−x})\)</span> <figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.nn.Sigmoid</span><br></pre></td></tr></table></figure></p>
<h6 id="softmax">2.7.3 Softmax</h6>
<p>对 n 维输入张量运用 Softmax 函数，将张量的每个元素缩放到（0,1）区间且和为 1。 <figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.nn.Softmax</span><br></pre></td></tr></table></figure></p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/2022/10/26/Transformer/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="trluper">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Trluper">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/10/26/Transformer/" class="post-title-link" itemprop="url">Transformer</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>
              

              <time title="创建时间：2022-10-26 10:38:28 / 修改时间：21:39:10" itemprop="dateCreated datePublished" datetime="2022-10-26T10:38:28+08:00">2022-10-26</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">深度学习</span></a>
                </span>
                  ，
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/Transformer/" itemprop="url" rel="index"><span itemprop="name">Transformer</span></a>
                </span>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine：</span>
    
    <a title="valine" href="/2022/10/26/Transformer/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2022/10/26/Transformer/" itemprop="commentCount"></span>
    </a>
  </span>
  
  <br>
            <span class="post-meta-item" title="本文字数">
              <span class="post-meta-item-icon">
                <i class="far fa-file-word"></i>
              </span>
                <span class="post-meta-item-text">本文字数：</span>
              <span>2k</span>
            </span>
            <span class="post-meta-item" title="阅读时长">
              <span class="post-meta-item-icon">
                <i class="far fa-clock"></i>
              </span>
                <span class="post-meta-item-text">阅读时长 &asymp;</span>
              <span>2 分钟</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h4 id="transformer是什么">1. Transformer是什么</h4>
<p><code>Transformer</code>是一个利用注意力机制来提高模型训练速度的模型。关于注意力机制可以参看<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/52119092">这篇文章</a>，<strong>trasnformer可以说是完全基于自注意力机制的一个深度学习模型</strong>，因为它适用于并行化计算，和它本身模型的复杂程度导致它在精度和性能上都要高于之前流行的RNN循环神经网络。</p>
<p>对于RNN来说它一般只关注附近的关系关联，而没有从全局寻找关系紧密的相关特征。因此对于一些特定的场景，不能够很好的处理。而Transforme放眼全局,通过计算权重得到相应的向量，得到更为全面的特征</p>
<h4 id="transformer的构成">2. Transformer的构成</h4>
<p>里面主要有两部分组成：<code>Encoder</code>和<code>Decoder</code> <img src="/2022/10/26/Transformer/Constitue.png" width="500"> 当我输入一个文本<code>机器学习</code>的时候，该文本数据会先经过一个叫Encoders的模块，对该文本进行编码，然后将编码后的数据再传入一个叫Decoders的模块进行解码，解码后就得到了翻译后的文本，对应的我们称Encoders为编码器，Decoders为解码器。</p>
<p>这个编码模块里边，有很多小的编码器，一般情况下，<code>Encoders</code>里边有6个小编码器，同样的，<code>Decoders</code>里边有6个小解码器。 <img src="/2022/10/26/Transformer/encorder.png" width="500"></p>
<p>放大一个encoder，发现里边的结构是一个自注意力机制加上一个前馈神经网络。 <img src="/2022/10/26/Transformer/forward.png" width="500"></p>
<h4 id="encoder">3 Encoder</h4>
<h5 id="self-attention的步骤">3.1 self-attention的步骤</h5>
<p>这里我们以文本词向量为例。总结来说Transformer时将输入的特征信息以向量为一组，计算该向量与别的向量之间的紧密程度，即将Q与K进行点积得到权值值，经过softmax后得到所占的比重值，然后整合各向量的V*权重得到该向量的新的特征向量。</p>
<ul>
<li><p><code>transformer</code>首先将词向量乘上三个矩阵，得到三个新的向量，之所以乘上三个矩阵参数而不是直接用原本的词向量是因为这样增加更多的参数，提高模型效果。对于输入<code>X1</code>(机器)，乘上三个矩阵后分别得到<code>Q1,K1,V1</code>，同样的，对于输入<code>X2</code>(学习)，也乘上三个不同的矩阵得到<code>Q2,K2,V2</code>。Q维要去查询的，K为等着被查的，V为实际的特征信息 <img src="/2022/10/26/Transformer/Three.png" width="500"></p></li>
<li><p>计算注意力得分了，这个得分是通过计算Q与各个单词的K向量的点积得到的。我们以X1为例，分别将Q1和K1、K2进行点积运算，假设分别得到得分112和96. <img src="/2022/10/26/Transformer/qk.png" width="500"></p></li>
<li><p>将得分分别除以一个特定数值8（K向量的维度的平方根，这里以K向量的维度是64为例）这能让梯度更加稳定，则得到结果如下： <img src="/2022/10/26/Transformer/score.png" width="500"></p></li>
<li><p>进行softmax运算得到，softmax主要将分数标准化，使他们都是正数并且加起来等于1 <img src="/2022/10/26/Transformer/softmax.png" width="500"></p></li>
<li><p>V向量乘上softmax的结果，这个思想主要是为了保持我们想要关注的单词的值不变，而掩盖掉那些不相关的单词（例如将他们乘上很小的数字） <img src="/2022/10/26/Transformer/cheng.png" width="500"></p></li>
<li><p>将带权重的各个V向量加起来，至此，产生在这个位置上（第一个单词）的self-attention层的输出，其余位置的self-attention输出也是同样的计算方式 <img src="/2022/10/26/Transformer/out.png" width="500"></p></li>
</ul>
<p>将上述的过程总结为一个公式就可以用下图表示： <img src="/2022/10/26/Transformer/comprehension.png" width="500"></p>
<p>上面所举的例子我们可以发现每一个输入向量只是产生了一组Q、K、V，为了进一步提高自注意力的层的性能，产生了<strong>多头注意力机制，它通过不同的head得到多个特征表达式（理解为多个个互不干扰自的注意力机制运算，每一组的Q/K/V都不相同。然后，得到多个个不同的权重矩阵Z，每个权重矩阵被用来将输入向量投射到不同的表示子空间。），然后将所以特征拼接起来，得到更为丰富的输出，然后经过一层全连接实现降维。</strong></p>
<p>如下我们得到了八个矩阵Z，将他们拼接: <img src="/2022/10/26/Transformer/cat.png" width="500"> 后进行全连接后得到我们作为前馈神经网络的输入： <img src="/2022/10/26/Transformer/fc.png" width="500"></p>
<p><strong><em>注意：为了解决梯度消失的问题，在Encoders和Decoder中都是用了残差神经网络的结构，即每一个前馈神经网络的输入不光包含上述self-attention的输出Z，还包含最原始的输入。</em></strong></p>
<h5 id="前馈神经网络">3.2 前馈神经网络</h5>
<p>前馈神经网络的输入是self-attention的输出，即上图的Z,是一个矩阵，矩阵的维度是（序列长度×D词向量），之后前馈神经网络的输出也是同样的维度</p>
<h4 id="decorder">4. Decorder</h4>
<p>decoder中使用的也是同样的结构。也是首先对输出<code>machine learning</code>计算自注意力得分，不同的地方在于，进行过自注意力机制后，将self-attention的输出再与Decoders模块的输出计算一遍交叉注意力机制得分，之后，再进入前馈神经网络模块</p>
<ul>
<li>交叉注意力：指两个模块的矩阵进行注意力计算</li>
<li>自注意力：指在同一模块内进行注意力计算</li>
</ul>
<h4 id="对位置应用认识">5. 对位置应用认识</h4>
<p>上面所举的例子当中只是单纯的由Q、K、V得到权值计算拼接后的东西。但是思考一下下面两个句子： <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">chen Love wu</span><br><span class="line">wu love chen</span><br></pre></td></tr></table></figure> 这两个句子表达的意思事完全不同的，但是相应词的Q、K、V是一样的，经过计算后得到的特征也是一样。这就是所谓的在self-attention中每个词都会考虑整个序列的加权。所以其出现位置并不会对结果产生影响，相当于放哪都无所谓，但这跟实际就有些不符合了，我们希望模型对位置有额外的认识。</p>
<p>因此我们希望能够对位置进行编码，一般对位置编码不进行更新（大佬作品，水论文可以更）</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/2022/10/06/pytorch%E5%9F%BA%E7%A1%80/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="trluper">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Trluper">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/10/06/pytorch%E5%9F%BA%E7%A1%80/" class="post-title-link" itemprop="url">pytorch基础</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2022-10-06 09:33:40" itemprop="dateCreated datePublished" datetime="2022-10-06T09:33:40+08:00">2022-10-06</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2022-10-10 20:08:58" itemprop="dateModified" datetime="2022-10-10T20:08:58+08:00">2022-10-10</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95/" itemprop="url" rel="index"><span itemprop="name">深度学习算法</span></a>
                </span>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine：</span>
    
    <a title="valine" href="/2022/10/06/pytorch%E5%9F%BA%E7%A1%80/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2022/10/06/pytorch%E5%9F%BA%E7%A1%80/" itemprop="commentCount"></span>
    </a>
  </span>
  
  <br>
            <span class="post-meta-item" title="本文字数">
              <span class="post-meta-item-icon">
                <i class="far fa-file-word"></i>
              </span>
                <span class="post-meta-item-text">本文字数：</span>
              <span>36k</span>
            </span>
            <span class="post-meta-item" title="阅读时长">
              <span class="post-meta-item-icon">
                <i class="far fa-clock"></i>
              </span>
                <span class="post-meta-item-text">阅读时长 &asymp;</span>
              <span>33 分钟</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h4 id="pytorch简介">1、pytorch简介</h4>
<p>pytorch是一个基于Python的科学计算包，PyTorch 的设计遵循tensor→variable(autograd)→nn.Module 三个由低到高的抽象层次，分别代表高维数组（张量）、自动求导（变量）和神经网络（层/模块），而且这三个抽象之间联系紧密，可以同时进行修改和操作。它主要有两个用途：</p>
<ul>
<li>类似于Numpy但是能利用GPU加速</li>
<li>一个非常灵活和快速用于深度学习的研究平台</li>
</ul>
<h4 id="基本数据结构tensor">2、基本数据结构：Tensor</h4>
<p>Tensor在pttorch中负责存储基本数据，ptyTorch针对Tensor也提供了丰富的函数和方法，所以pyTorch中的Tensor与Numpy的数组具有极高的相似性。Tensor是一种高级的API。</p>
<p><strong>Tensor即张量，张量是Pytorch的核心概念，pytorch的计算都是基于张量的计算，是PyTorch中的基本操作对象，可以看做是包含单一数据类型元素的多维矩阵</strong>。从使用角度来看，Tensor与NumPy的ndarrays非常类似，相互之间也可以自由转换，只不过Tensor还支持GPU的加速</p>
<table>
<thead>
<tr class="header">
<th>数据类型</th>
<th>CPU Tensor</th>
<th>GPU Tensor</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>32位浮点</td>
<td><code>torch.FloatTensor</code></td>
<td><code>torch.cuda.FloatTensor</code></td>
</tr>
<tr class="even">
<td>64位浮点</td>
<td><code>torch.DoubleTensor</code></td>
<td><code>torch.cuda.DoubleTensor</code></td>
</tr>
<tr class="odd">
<td>16位半精度浮点</td>
<td><code>N/A</code></td>
<td><code>torch.cuda.HalfTensor</code></td>
</tr>
<tr class="even">
<td>8位无符号整型</td>
<td><code>torch.ByteTensor</code></td>
<td><code>torch.cuda.ByteTensor</code></td>
</tr>
<tr class="odd">
<td>8位有符号整型</td>
<td><code>torch.charTensor</code></td>
<td><code>torch.cuda.charTensor</code></td>
</tr>
<tr class="even">
<td>16位有符号整型</td>
<td><code>torch.ShortTensor</code></td>
<td><code>torch.cuda.ShortTensor</code></td>
</tr>
<tr class="odd">
<td>32位有符号整型</td>
<td><code>torch.IntTensor</code></td>
<td><code>torch.cuda.IntTensor</code></td>
</tr>
<tr class="even">
<td>64位有符号整型</td>
<td><code>torch.LongTensor</code></td>
<td><code>torch.cuda.LongTensor</code></td>
</tr>
</tbody>
</table>
<p>pytorch不支持str类型</p>
<h5 id="tensor的创建">2.1 Tensor的创建</h5>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">torch.tensor(data, </span><br><span class="line">             dtype=<span class="literal">None</span>, </span><br><span class="line">             device=<span class="literal">None</span>, </span><br><span class="line">             requires_grad=<span class="literal">False</span>, </span><br><span class="line">             pin_memory=<span class="literal">False</span>) -&gt; Tensor</span><br></pre></td></tr></table></figure>
<ul>
<li><code>data</code>：数据，可以是list，也可以是numpy</li>
<li><code>dtype</code>：数据类型，默认和data一致</li>
<li><code>device</code>：tensor所在的设备</li>
<li><code>requires_grad</code>：是否需要梯度，默认False，在搭建神经网络时需要将求导的参数设为True</li>
<li><code>pin_memory</code>：是否存于锁页内存，默认False</li>
</ul>
<p>还有其他的按数值创建的方法，这里只列举一个： <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">torch.zeros(*size, </span><br><span class="line">            out=<span class="literal">None</span>, </span><br><span class="line">            dtype=<span class="literal">None</span>, </span><br><span class="line">            layout=torch.strided, </span><br><span class="line">            device=<span class="literal">None</span>, </span><br><span class="line">            requires_grad=<span class="literal">False</span>) -&gt; Tensor</span><br></pre></td></tr></table></figure> - <code>size</code>：张量的形状，如（3，3） - <code>layout</code> ：这个是内存中的布局形式,有strided和sparse_coo等 - <code>out</code>：表示输出张量，就是再把这个张量赋值给别的一个张量，但是这两个张量时一样的，指的同一个内存地址 - <code>device</code>：所在的设备，gpu/cpu - <code>requires_grad</code>：是否需要梯度</p>
<p><img src="/2022/10/06/pytorch%E5%9F%BA%E7%A1%80/build.png" width="500"></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#使用特定类型构造函数创建</span></span><br><span class="line">i=torch.FloatTensor(<span class="number">2</span>,<span class="number">3</span>)						<span class="comment">#构造了一个2*3的32位浮点矩阵，初始值为0</span></span><br><span class="line">b=torch.FloatTensor([[<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>],[<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>]])			<span class="comment">#使用列表构造一个2*3的2维张量</span></span><br><span class="line"><span class="comment">#使用tensor函数</span></span><br><span class="line">a=torch.tensor([[<span class="number">2</span>,<span class="number">5</span>,<span class="number">7</span>],[<span class="number">10</span>,<span class="number">2</span>,<span class="number">4</span>]],dtype=<span class="built_in">float</span>)	<span class="comment">#dtype指定类型，如果省略则自动推断</span></span><br><span class="line"><span class="comment">#使用其他函数</span></span><br><span class="line">t=torch.randn(<span class="number">2</span>,<span class="number">2</span>)								<span class="comment">#生一个2*2随机2维张量</span></span><br><span class="line"><span class="comment">#如果张量中只有一个元素, 可以用.item()将值取出, 作为一个python number</span></span><br><span class="line">x = torch.randn(<span class="number">1</span>)</span><br><span class="line"><span class="built_in">print</span>(x)</span><br><span class="line"><span class="built_in">print</span>(x.item())</span><br></pre></td></tr></table></figure>
<h5 id="张量的尺寸">2.2 张量的尺寸</h5>
<p><strong>可以使用<code>shape</code>属性或者<code>size()</code>方法查看张量在每一维的长度.</strong> <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">t=torch.randn(<span class="number">2</span>,<span class="number">2</span>)</span><br><span class="line"><span class="built_in">print</span>(t.shape,t.size())</span><br></pre></td></tr></table></figure> 输出： <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.Size([<span class="number">2</span>, <span class="number">2</span>]) torch.Size([<span class="number">2</span>, <span class="number">2</span>])</span><br></pre></td></tr></table></figure></p>
<p>也能使用可以使用<code>view()</code>方法改变张量的尺寸。如果<code>view()</code>方法改变尺寸失败，可以使用<code>reshape()</code>方法 <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">b=torch.FloatTensor([[<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>],[<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>]])</span><br><span class="line">m=b.view(<span class="number">3</span>,<span class="number">2</span>)									<span class="comment">#将2*3转为3*2</span></span><br></pre></td></tr></table></figure> 有些时候有些操作会让张量存储结构扭曲，比如转置，直接使用view会失败，可以用reshape方法 <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">m=b.reshape(<span class="number">3</span>,<span class="number">2</span>)</span><br></pre></td></tr></table></figure></p>
<h5 id="tensor和numpy数组">2.3 Tensor和numpy数组</h5>
<p>可以用numpy方法从Tensor得到numpy数组，也可以用torch.from_numpy从numpy数组得到Tensor。<strong>这两种方法关联的Tensor和numpy数组是共享数据内存的，即改变其中一个，另一个也会发生改变</strong>。因此如果不需要共享，可以用张量的<code>clone()</code>方法拷贝张量，中断这种关联 <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="comment">#torch.from_numpy函数从numpy数组转为Tensor</span></span><br><span class="line">arr = np.zeros(<span class="number">3</span>)</span><br><span class="line">tensor = torch.from_numpy(arr)</span><br><span class="line">np.add(arr,<span class="number">1</span>, out = arr) 		<span class="comment">#给arr增加1，tensor也随之改变</span></span><br><span class="line"><span class="built_in">print</span>(arr)</span><br><span class="line"><span class="built_in">print</span>(tensor)</span><br><span class="line"></span><br><span class="line"><span class="comment">#使用clone</span></span><br><span class="line"><span class="comment"># 可以用clone() 方法拷贝张量，中断这种关联</span></span><br><span class="line">tensor = torch.zeros(<span class="number">3</span>)</span><br><span class="line"><span class="comment">#使用clone方法拷贝张量, 拷贝后的张量和原始张量内存独立</span></span><br><span class="line">arr = tensor.clone().numpy() <span class="comment"># 也可以使用tensor.data.numpy()</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#将Torch Tensor转换为Numpy array</span></span><br><span class="line">a = torch.ones(<span class="number">5</span>)</span><br><span class="line">b = a.numpy()</span><br></pre></td></tr></table></figure> 输出 <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[<span class="number">1.</span> <span class="number">1.</span> <span class="number">1.</span>]</span><br><span class="line">tensor([<span class="number">1.</span>, <span class="number">1.</span>, <span class="number">1.</span>], dtype=torch.float64)</span><br></pre></td></tr></table></figure></p>
<h5 id="tensor操作">2.4 Tensor操作</h5>
<p>Tensor同样跟python一样支持切片、合并分割操作和相应的数学运算 ###### 2.4.1 索引切片 切片时支持缺省参数和省略号。可以通过索引和切片对部分元素进行修改。 <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">t=torch.randn(<span class="number">5</span>,<span class="number">6</span>)</span><br><span class="line">m=t[<span class="number">0</span>:-<span class="number">1</span>:<span class="number">2</span>,<span class="number">1</span>:-<span class="number">1</span>:<span class="number">3</span>]		<span class="comment">#表示从第一行到最后一行每隔一行取一行，从第二列到最后一列每隔两列取一列</span></span><br></pre></td></tr></table></figure></p>
<h6 id="合并分割">2.4.2 合并分割</h6>
<ul>
<li>可以用<code>torch.cat()</code>方法和<code>torch.stack()</code>方法将多个张量合并，</li>
<li>可以用<code>torch.split()</code>方法把一个张量分割成多个张量。</li>
<li><code>torch.cat()</code>和<code>torch.stack()</code>有略微的区别，<code>torch.cat()</code>是连接，不会增加维度，而<code>torch.stack()</code>是堆叠， 会增加维度</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">a=torch.tensor([[<span class="number">1</span>,<span class="number">2</span>],[<span class="number">3</span>,<span class="number">4</span>]])</span><br><span class="line">b=torch.tensor([[<span class="number">5</span>,<span class="number">6</span>],[<span class="number">7</span>,<span class="number">8</span>]])</span><br><span class="line">c=torch.tensor([[<span class="number">9</span>,<span class="number">10</span>],[<span class="number">11</span>,<span class="number">12</span>]])</span><br><span class="line">d=torch.cat([a,b,c])</span><br><span class="line"><span class="built_in">print</span>(d)</span><br><span class="line">e=torch.stack([a,b,c])</span><br><span class="line"><span class="built_in">print</span>(e)</span><br><span class="line">f,g,h=torch.split(d,split_size_or_sections=<span class="number">2</span>,dim=<span class="number">0</span>)</span><br><span class="line"><span class="built_in">print</span>(f,g,h)</span><br></pre></td></tr></table></figure>
<p>输出： <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">tensor([[ <span class="number">1</span>,  <span class="number">2</span>],</span><br><span class="line">        [ <span class="number">3</span>,  <span class="number">4</span>],</span><br><span class="line">        [ <span class="number">5</span>,  <span class="number">6</span>],</span><br><span class="line">        [ <span class="number">7</span>,  <span class="number">8</span>],</span><br><span class="line">        [ <span class="number">9</span>, <span class="number">10</span>],</span><br><span class="line">        [<span class="number">11</span>, <span class="number">12</span>]])</span><br><span class="line">tensor([[[ <span class="number">1</span>,  <span class="number">2</span>],</span><br><span class="line">         [ <span class="number">3</span>,  <span class="number">4</span>]],</span><br><span class="line"></span><br><span class="line">        [[ <span class="number">5</span>,  <span class="number">6</span>],</span><br><span class="line">         [ <span class="number">7</span>,  <span class="number">8</span>]],</span><br><span class="line"></span><br><span class="line">        [[ <span class="number">9</span>, <span class="number">10</span>],</span><br><span class="line">         [<span class="number">11</span>, <span class="number">12</span>]]])</span><br><span class="line">tensor([[<span class="number">1</span>, <span class="number">2</span>],</span><br><span class="line">        [<span class="number">3</span>, <span class="number">4</span>]]) tensor([[<span class="number">5</span>, <span class="number">6</span>],</span><br><span class="line">        [<span class="number">7</span>, <span class="number">8</span>]]) tensor([[ <span class="number">9</span>, <span class="number">10</span>],</span><br><span class="line">        [<span class="number">11</span>, <span class="number">12</span>]])</span><br><span class="line"></span><br></pre></td></tr></table></figure></p>
<h5 id="tensor的运算操作">2.5 Tensor的运算操作</h5>
<p>张量数学运算主要有：标量运算，向量运算，矩阵运算。</p>
<h6 id="标量运算">2.5.1 标量运算</h6>
<p>加减乘除乘方，以及三角函数，指数，对数等常见函数，逻辑比较运算符等都是标量运算符。标量运算符的特点是对张量实施逐元素运算。有些标量运算符对常用的数学运算符进行了重载，并且支持类似numpy的广播特性 <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#例1-3-1 张量的数学运算-标量运算</span></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">a = torch.tensor([[<span class="number">1.0</span>,<span class="number">2</span>],[-<span class="number">3</span>,<span class="number">4.0</span>]])</span><br><span class="line">b = torch.tensor([[<span class="number">5.0</span>,<span class="number">6</span>],[<span class="number">7.0</span>,<span class="number">8.0</span>]])</span><br><span class="line">a+b <span class="comment">#运算符重载</span></span><br><span class="line"> </span><br><span class="line">out:</span><br><span class="line">tensor([[ <span class="number">6.</span>,  <span class="number">8.</span>],</span><br><span class="line">        [ <span class="number">4.</span>, <span class="number">12.</span>]])</span><br><span class="line"> </span><br><span class="line">a-b</span><br><span class="line"> </span><br><span class="line">out:</span><br><span class="line">tensor([[ -<span class="number">4.</span>,  -<span class="number">4.</span>],</span><br><span class="line">        [-<span class="number">10.</span>,  -<span class="number">4.</span>]])</span><br><span class="line"> </span><br><span class="line">a*b</span><br><span class="line"> </span><br><span class="line">out:</span><br><span class="line">tensor([[  <span class="number">5.</span>,  <span class="number">12.</span>],</span><br><span class="line">        [-<span class="number">21.</span>,  <span class="number">32.</span>]])</span><br><span class="line"> </span><br><span class="line"> </span><br><span class="line">a/b</span><br><span class="line"> </span><br><span class="line">out:</span><br><span class="line">tensor([[ <span class="number">0.2000</span>,  <span class="number">0.3333</span>],</span><br><span class="line">        [-<span class="number">0.4286</span>,  <span class="number">0.5000</span>]])</span><br><span class="line"> </span><br><span class="line"> </span><br><span class="line">a**<span class="number">2</span></span><br><span class="line"> </span><br><span class="line">out:</span><br><span class="line">tensor([[ <span class="number">1.</span>,  <span class="number">4.</span>],</span><br><span class="line">        [ <span class="number">9.</span>, <span class="number">16.</span>]])</span><br><span class="line"> </span><br><span class="line"> </span><br><span class="line">a**(<span class="number">0.5</span>)</span><br><span class="line"> </span><br><span class="line">out:</span><br><span class="line">tensor([[<span class="number">1.0000</span>, <span class="number">1.4142</span>],</span><br><span class="line">        [   nan, <span class="number">2.0000</span>]])</span><br><span class="line"> </span><br><span class="line">a%<span class="number">3</span> <span class="comment">#求模</span></span><br><span class="line"> </span><br><span class="line">out:</span><br><span class="line">tensor([[<span class="number">1.</span>, <span class="number">2.</span>],</span><br><span class="line">        [-<span class="number">0.</span>, <span class="number">1.</span>]])</span><br><span class="line"> </span><br><span class="line"> </span><br><span class="line">a//<span class="number">3</span> <span class="comment">#地板除法</span></span><br><span class="line"> </span><br><span class="line">out:</span><br><span class="line">tensor([[ <span class="number">0.</span>,  <span class="number">0.</span>],</span><br><span class="line">        [-<span class="number">1.</span>,  <span class="number">1.</span>]])</span><br><span class="line"> </span><br><span class="line"> </span><br><span class="line">a&gt;=<span class="number">2</span> <span class="comment"># torch.ge(a,2) #ge: greater_equal缩写</span></span><br><span class="line"> </span><br><span class="line">out:</span><br><span class="line">tensor([[<span class="literal">False</span>,  <span class="literal">True</span>],</span><br><span class="line">        [<span class="literal">False</span>,  <span class="literal">True</span>]])</span><br><span class="line"> </span><br><span class="line"> </span><br><span class="line">(a&gt;=<span class="number">2</span>)&amp;(a&lt;=<span class="number">3</span>)</span><br><span class="line"> </span><br><span class="line">out:</span><br><span class="line">tensor([[<span class="literal">False</span>,  <span class="literal">True</span>],</span><br><span class="line">        [<span class="literal">False</span>, <span class="literal">False</span>]])</span><br><span class="line"> </span><br><span class="line"> </span><br><span class="line"> </span><br><span class="line"> </span><br><span class="line">(a&gt;=<span class="number">2</span>)|(a&lt;=<span class="number">3</span>)</span><br><span class="line"> </span><br><span class="line">out:</span><br><span class="line">tensor([[<span class="literal">True</span>, <span class="literal">True</span>],</span><br><span class="line">        [<span class="literal">True</span>, <span class="literal">True</span>]])</span><br><span class="line"> </span><br><span class="line">a==<span class="number">5</span> <span class="comment">#torch.eq(a,5)</span></span><br><span class="line"> </span><br><span class="line">out:</span><br><span class="line">tensor([[<span class="literal">False</span>, <span class="literal">False</span>],</span><br><span class="line">        [<span class="literal">False</span>, <span class="literal">False</span>]])</span><br><span class="line"></span><br><span class="line">torch.sqrt(a)</span><br><span class="line"> </span><br><span class="line">out:</span><br><span class="line">tensor([[<span class="number">1.0000</span>, <span class="number">1.4142</span>],</span><br><span class="line">        [   nan, <span class="number">2.0000</span>]])</span><br><span class="line"> </span><br><span class="line"> </span><br><span class="line">a = torch.tensor([<span class="number">1.0</span>,<span class="number">8.0</span>])</span><br><span class="line">b = torch.tensor([<span class="number">5.0</span>,<span class="number">6.0</span>])</span><br><span class="line">c = torch.tensor([<span class="number">6.0</span>,<span class="number">7.0</span>])</span><br><span class="line">d = a+b+c</span><br><span class="line"><span class="built_in">print</span>(d)</span><br><span class="line"> </span><br><span class="line">out:</span><br><span class="line">tensor([<span class="number">12.</span>, <span class="number">21.</span>])</span><br><span class="line"> </span><br><span class="line"> </span><br><span class="line"><span class="built_in">print</span>(torch.<span class="built_in">max</span>(a,b))</span><br><span class="line"> </span><br><span class="line">out:</span><br><span class="line">tensor([<span class="number">5.</span>, <span class="number">8.</span>])</span><br><span class="line"> </span><br><span class="line"> </span><br><span class="line"><span class="built_in">print</span>(torch.<span class="built_in">min</span>(a,b))</span><br><span class="line"> </span><br><span class="line">out:</span><br><span class="line">tensor([<span class="number">1.</span>, <span class="number">6.</span>])</span><br><span class="line"> </span><br><span class="line"> </span><br><span class="line">x = torch.tensor([<span class="number">2.6</span>,-<span class="number">2.7</span>])</span><br><span class="line"><span class="built_in">print</span>(torch.<span class="built_in">round</span>(x)) <span class="comment">#保留整数部分，四舍五入</span></span><br><span class="line"><span class="built_in">print</span>(torch.floor(x)) <span class="comment">#保留整数部分，向下归整</span></span><br><span class="line"><span class="built_in">print</span>(torch.ceil(x)) <span class="comment">#保留整数部分，向上归整</span></span><br><span class="line"><span class="built_in">print</span>(torch.trunc(x)) <span class="comment">#保留整数部分，向0归整</span></span><br><span class="line"> </span><br><span class="line">out:</span><br><span class="line">tensor([ <span class="number">3.</span>, -<span class="number">3.</span>])</span><br><span class="line">tensor([ <span class="number">2.</span>, -<span class="number">3.</span>])</span><br><span class="line">tensor([ <span class="number">3.</span>, -<span class="number">2.</span>])</span><br><span class="line">tensor([ <span class="number">2.</span>, -<span class="number">2.</span>])</span><br><span class="line"> </span><br><span class="line"> </span><br><span class="line">x = torch.tensor([<span class="number">2.6</span>,-<span class="number">2.7</span>])</span><br><span class="line"><span class="built_in">print</span>(torch.fmod(x,<span class="number">2</span>)) <span class="comment">#作除法取余数</span></span><br><span class="line"><span class="built_in">print</span>(torch.remainder(x,<span class="number">2</span>)) <span class="comment">#作除法取剩余的部分，结果恒正</span></span><br><span class="line"> </span><br><span class="line">out:</span><br><span class="line">tensor([ <span class="number">0.6000</span>, -<span class="number">0.7000</span>])</span><br><span class="line">tensor([<span class="number">0.6000</span>, <span class="number">1.3000</span>])</span><br><span class="line"> </span><br><span class="line"> </span><br><span class="line"><span class="comment"># 幅值裁剪</span></span><br><span class="line">x = torch.tensor([<span class="number">0.9</span>,-<span class="number">0.8</span>,<span class="number">100.0</span>,-<span class="number">20.0</span>,<span class="number">0.7</span>])</span><br><span class="line">y = torch.clamp(x,<span class="built_in">min</span>=-<span class="number">1</span>,<span class="built_in">max</span> = <span class="number">1</span>)</span><br><span class="line">z = torch.clamp(x,<span class="built_in">max</span> = <span class="number">1</span>)</span><br><span class="line"><span class="built_in">print</span>(y)</span><br><span class="line"><span class="built_in">print</span>(z)</span><br><span class="line"> </span><br><span class="line">out:</span><br><span class="line">tensor([ <span class="number">0.9000</span>, -<span class="number">0.8000</span>,  <span class="number">1.0000</span>, -<span class="number">1.0000</span>,  <span class="number">0.7000</span>])</span><br><span class="line">tensor([  <span class="number">0.9000</span>,  -<span class="number">0.8000</span>,   <span class="number">1.0000</span>, -<span class="number">20.0000</span>,   <span class="number">0.7000</span>])</span><br></pre></td></tr></table></figure></p>
<h6 id="向量运算">2.5.2 向量运算</h6>
<p>向量运算符只在一个特定轴上运算，将一个向量映射到一个标量或者另外一个向量。 <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#例1-3-2  张量的数学运算-向量运算</span></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="comment">#统计值</span></span><br><span class="line">a = torch.arange(<span class="number">1</span>,<span class="number">10</span>).<span class="built_in">float</span>()</span><br><span class="line"><span class="built_in">print</span>(torch.<span class="built_in">sum</span>(a))</span><br><span class="line"><span class="built_in">print</span>(torch.mean(a))</span><br><span class="line"><span class="built_in">print</span>(torch.<span class="built_in">max</span>(a))</span><br><span class="line"><span class="built_in">print</span>(torch.<span class="built_in">min</span>(a))</span><br><span class="line"><span class="built_in">print</span>(torch.prod(a)) <span class="comment">#累乘</span></span><br><span class="line"><span class="built_in">print</span>(torch.std(a)) <span class="comment">#标准差</span></span><br><span class="line"><span class="built_in">print</span>(torch.var(a)) <span class="comment">#方差</span></span><br><span class="line"><span class="built_in">print</span>(torch.median(a)) <span class="comment">#中位数</span></span><br><span class="line"> </span><br><span class="line">out:</span><br><span class="line">tensor(<span class="number">45.</span>)</span><br><span class="line">tensor(<span class="number">5.</span>)</span><br><span class="line">tensor(<span class="number">9.</span>)</span><br><span class="line">tensor(<span class="number">1.</span>)</span><br><span class="line">tensor(<span class="number">362880.</span>)</span><br><span class="line">tensor(<span class="number">2.7386</span>)</span><br><span class="line">tensor(<span class="number">7.5000</span>)</span><br><span class="line">tensor(<span class="number">5.</span>)</span><br><span class="line"> </span><br><span class="line"> </span><br><span class="line"><span class="comment">#指定维度计算统计值</span></span><br><span class="line">b = a.view(<span class="number">3</span>,<span class="number">3</span>)</span><br><span class="line"><span class="built_in">print</span>(b)</span><br><span class="line"><span class="built_in">print</span>(torch.<span class="built_in">max</span>(b,dim = <span class="number">0</span>))</span><br><span class="line"><span class="built_in">print</span>(torch.<span class="built_in">max</span>(b,dim = <span class="number">1</span>))</span><br><span class="line"> </span><br><span class="line">out:</span><br><span class="line">tensor([[<span class="number">1.</span>, <span class="number">2.</span>, <span class="number">3.</span>],</span><br><span class="line">        [<span class="number">4.</span>, <span class="number">5.</span>, <span class="number">6.</span>],</span><br><span class="line">        [<span class="number">7.</span>, <span class="number">8.</span>, <span class="number">9.</span>]])</span><br><span class="line">torch.return_types.<span class="built_in">max</span>(</span><br><span class="line">values=tensor([<span class="number">7.</span>, <span class="number">8.</span>, <span class="number">9.</span>]),</span><br><span class="line">indices=tensor([<span class="number">2</span>, <span class="number">2</span>, <span class="number">2</span>]))</span><br><span class="line">torch.return_types.<span class="built_in">max</span>(</span><br><span class="line">values=tensor([<span class="number">3.</span>, <span class="number">6.</span>, <span class="number">9.</span>]),</span><br><span class="line">indices=tensor([<span class="number">2</span>, <span class="number">2</span>, <span class="number">2</span>]))</span><br><span class="line"> </span><br><span class="line"> </span><br><span class="line"><span class="comment">#cum扫描</span></span><br><span class="line">a = torch.arange(<span class="number">1</span>,<span class="number">10</span>)</span><br><span class="line"><span class="built_in">print</span>(torch.cumsum(a,<span class="number">0</span>))</span><br><span class="line"><span class="built_in">print</span>(torch.cumprod(a,<span class="number">0</span>))</span><br><span class="line"><span class="built_in">print</span>(torch.cummax(a,<span class="number">0</span>).values)</span><br><span class="line"><span class="built_in">print</span>(torch.cummax(a,<span class="number">0</span>).indices)</span><br><span class="line"><span class="built_in">print</span>(torch.cummin(a,<span class="number">0</span>))</span><br><span class="line"> </span><br><span class="line">out:</span><br><span class="line">tensor([ <span class="number">1</span>,  <span class="number">3</span>,  <span class="number">6</span>, <span class="number">10</span>, <span class="number">15</span>, <span class="number">21</span>, <span class="number">28</span>, <span class="number">36</span>, <span class="number">45</span>])</span><br><span class="line">tensor([     <span class="number">1</span>,      <span class="number">2</span>,      <span class="number">6</span>,     <span class="number">24</span>,    <span class="number">120</span>,    <span class="number">720</span>,   <span class="number">5040</span>,  <span class="number">40320</span>, <span class="number">362880</span>])</span><br><span class="line">tensor([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>, <span class="number">7</span>, <span class="number">8</span>, <span class="number">9</span>])</span><br><span class="line">tensor([<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>, <span class="number">7</span>, <span class="number">8</span>])</span><br><span class="line">torch.return_types.cummin(</span><br><span class="line">values=tensor([<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>]),</span><br><span class="line">indices=tensor([<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>]))</span><br><span class="line"> </span><br><span class="line"> </span><br><span class="line"><span class="comment">#torch.sort和torch.topk可以对张量排序</span></span><br><span class="line">a = torch.tensor([[<span class="number">9</span>,<span class="number">7</span>,<span class="number">8</span>],[<span class="number">1</span>,<span class="number">3</span>,<span class="number">2</span>],[<span class="number">5</span>,<span class="number">6</span>,<span class="number">4</span>]]).<span class="built_in">float</span>()</span><br><span class="line"><span class="built_in">print</span>(torch.topk(a,<span class="number">2</span>,dim = <span class="number">0</span>),<span class="string">&quot;\n&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(torch.topk(a,<span class="number">2</span>,dim = <span class="number">1</span>),<span class="string">&quot;\n&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(torch.sort(a,dim = <span class="number">1</span>),<span class="string">&quot;\n&quot;</span>)</span><br><span class="line"> </span><br><span class="line">out:</span><br><span class="line">torch.return_types.topk(</span><br><span class="line">values=tensor([[<span class="number">9.</span>, <span class="number">7.</span>, <span class="number">8.</span>],</span><br><span class="line">        [<span class="number">5.</span>, <span class="number">6.</span>, <span class="number">4.</span>]]),</span><br><span class="line">indices=tensor([[<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>],</span><br><span class="line">        [<span class="number">2</span>, <span class="number">2</span>, <span class="number">2</span>]])) </span><br><span class="line"> </span><br><span class="line">torch.return_types.topk(</span><br><span class="line">values=tensor([[<span class="number">9.</span>, <span class="number">8.</span>],</span><br><span class="line">        [<span class="number">3.</span>, <span class="number">2.</span>],</span><br><span class="line">        [<span class="number">6.</span>, <span class="number">5.</span>]]),</span><br><span class="line">indices=tensor([[<span class="number">0</span>, <span class="number">2</span>],</span><br><span class="line">        [<span class="number">1</span>, <span class="number">2</span>],</span><br><span class="line">        [<span class="number">1</span>, <span class="number">0</span>]])) </span><br><span class="line"> </span><br><span class="line">torch.return_types.sort(</span><br><span class="line">values=tensor([[<span class="number">7.</span>, <span class="number">8.</span>, <span class="number">9.</span>],</span><br><span class="line">        [<span class="number">1.</span>, <span class="number">2.</span>, <span class="number">3.</span>],</span><br><span class="line">        [<span class="number">4.</span>, <span class="number">5.</span>, <span class="number">6.</span>]]),</span><br><span class="line">indices=tensor([[<span class="number">1</span>, <span class="number">2</span>, <span class="number">0</span>],</span><br><span class="line">        [<span class="number">0</span>, <span class="number">2</span>, <span class="number">1</span>],</span><br><span class="line">        [<span class="number">2</span>, <span class="number">0</span>, <span class="number">1</span>]]))</span><br></pre></td></tr></table></figure></p>
<h6 id="矩阵运算">2.5.3 矩阵运算</h6>
<p>矩阵必须是二维的，类似<code>torch.tensor([1,2,3])</code>这样的不是矩阵。矩阵运算包括：<strong>矩阵乘法，矩阵转置，矩阵逆，矩阵求迹，矩阵范数，矩阵行列式，矩阵求特征 值，矩阵分解等运算</strong> <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#例1-3-3 张量的数学运算-矩阵运算</span></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="comment">#矩阵乘法</span></span><br><span class="line">a = torch.tensor([[<span class="number">1</span>,<span class="number">2</span>],[<span class="number">3</span>,<span class="number">4</span>]])</span><br><span class="line">b = torch.tensor([[<span class="number">5</span>,<span class="number">6</span>],[<span class="number">7</span>,<span class="number">8</span>]]</span><br></pre></td></tr></table></figure> <strong>1. 矩阵乘法</strong>：矩阵乘法可以使用<code>a@b</code>，也可以函数<code>torch.matmul(a,b)</code>或者`torch.mm(a,b)</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">a@b</span><br><span class="line">torch.matmul(a,b)</span><br><span class="line">torch.mm(a,b)</span><br></pre></td></tr></table></figure>
<p>输出： <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">tensor([[<span class="number">19</span>, <span class="number">22</span>],</span><br><span class="line">        [<span class="number">43</span>, <span class="number">50</span>]])</span><br><span class="line">tensor([[<span class="number">19</span>, <span class="number">22</span>],</span><br><span class="line">        [<span class="number">43</span>, <span class="number">50</span>]])</span><br><span class="line">tensor([[<span class="number">19</span>, <span class="number">22</span>],</span><br><span class="line">        [<span class="number">43</span>, <span class="number">50</span>]])</span><br></pre></td></tr></table></figure></p>
<p><strong>2. 矩阵转置</strong>：转置直接使用其成员函数<code>t()</code> <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">a.t()</span><br><span class="line"></span><br><span class="line">out:</span><br><span class="line">tensor([[<span class="number">1.</span>, <span class="number">3.</span>],</span><br><span class="line">        [<span class="number">2.</span>, <span class="number">4.</span>]])</span><br></pre></td></tr></table></figure></p>
<p><strong>3. 逆矩阵</strong>：求逆使用Tensorde1<code>inverse()</code>函数。矩阵逆，必须为浮点类型 <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">torch.inverse(a)</span><br><span class="line"></span><br><span class="line">out:</span><br><span class="line">tensor([[-<span class="number">2.0000</span>,  <span class="number">1.0000</span>],</span><br><span class="line">        [ <span class="number">1.5000</span>, -<span class="number">0.5000</span>]], dtype=torch.float64)</span><br></pre></td></tr></table></figure></p>
<p><strong>4. 矩阵求Tr</strong> <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">a = torch.tensor([[<span class="number">1.0</span>,<span class="number">2</span>],[<span class="number">3</span>,<span class="number">4</span>]])</span><br><span class="line"><span class="built_in">print</span>(torch.trace(a))</span><br><span class="line"></span><br><span class="line">out:</span><br><span class="line">tensor(<span class="number">5.</span>)</span><br></pre></td></tr></table></figure></p>
<p><strong>5. 矩阵求范数</strong> <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">torch.norm(a)</span><br><span class="line"></span><br><span class="line">out:</span><br><span class="line">tensor(<span class="number">5.4772</span>)</span><br></pre></td></tr></table></figure></p>
<p><strong>6. 矩阵行列式</strong> <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">torch.det(a)</span><br><span class="line"></span><br><span class="line">out:</span><br><span class="line">tensor(-<span class="number">2.0000</span>)</span><br></pre></td></tr></table></figure></p>
<p><strong>7. 矩阵特征值和特征向量</strong> <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">k=torch.tensor([[<span class="number">1</span>,<span class="number">2</span>],[-<span class="number">5</span>,<span class="number">4</span>]],dtype=<span class="built_in">float</span>)</span><br><span class="line">L_complex,V_complex=torch.linalg.eig(k)</span><br><span class="line"><span class="built_in">print</span>(L_complex,V_complex)</span><br></pre></td></tr></table></figure></p>
<p><strong>8. 矩阵QR分解</strong>：将一个方阵分解为一个正交矩阵q和上三角矩阵r。QR分解实际上是对矩阵a实施Schmidt正交化得到q</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">a=torch.tensor([[<span class="number">1</span>,<span class="number">2</span>],[<span class="number">3</span>,<span class="number">4</span>]],dtype=<span class="built_in">float</span>)</span><br><span class="line">q,r=torch.linalg.qr(a)</span><br><span class="line"><span class="built_in">print</span>(q,r)</span><br></pre></td></tr></table></figure>
<p><strong>9. 矩阵svd分解：</strong>svd分解可以将任意一个矩阵分解为一个正交矩阵u,一个对角阵s和一个正交矩阵v.t()的乘积，svd常用于矩阵压缩和降维 <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">u,s,v=torch.linalg.svd(a)</span><br><span class="line"><span class="built_in">print</span>(u,<span class="string">&quot;\n&quot;</span>,s,<span class="string">&quot;\n&quot;</span>,v)</span><br></pre></td></tr></table></figure> 输出 <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">tensor([[-<span class="number">0.4046</span>, -<span class="number">0.9145</span>],</span><br><span class="line">        [-<span class="number">0.9145</span>,  <span class="number">0.4046</span>]], dtype=torch.float64) </span><br><span class="line">tensor([<span class="number">5.4650</span>, <span class="number">0.3660</span>], dtype=torch.float64) </span><br><span class="line">tensor([[-<span class="number">0.5760</span>, -<span class="number">0.8174</span>],</span><br><span class="line">        [ <span class="number">0.8174</span>, -<span class="number">0.5760</span>]], dtype=torch.float64)</span><br></pre></td></tr></table></figure></p>
<h5 id="广播机制">2.6 广播机制</h5>
<p>Pytorch的广播规则和numpy是一样的:</p>
<ul>
<li><p>1、如果张量的维度不同，将维度较小的张量进行扩展，直到两个张量的维度都一样。</p></li>
<li><p>2、如果两个张量在某个维度上的长度是相同的，或者其中一个张量在该维度上的长度为1， 那么我们就说这两个张量在该维度上是相容的。</p></li>
<li><p>3、如果两个张量在所有维度上都是相容的，它们就能使用广播。</p></li>
<li><p>4、广播之后，每个维度的长度将取两个张量在该维度长度的较大值。</p></li>
<li><p>5、在任何一个维度上，如果一个张量的长度为1，另一个张量长度大于1，那么在该维度上，就好像是对第一个张量进行了复制。 torch.broadcast_tensors可以将多个张量根据广播规则转换成相同的维度。</p></li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#例 1-3-4  广播机制</span></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line">a = torch.tensor([<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>])</span><br><span class="line">b = torch.tensor([[<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>],[<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>],[<span class="number">2</span>,<span class="number">2</span>,<span class="number">2</span>]])</span><br><span class="line"><span class="built_in">print</span>(b + a)</span><br><span class="line">a_broad,b_broad = torch.broadcast_tensors(a,b)</span><br><span class="line"><span class="built_in">print</span>(a_broad,<span class="string">&quot;\n&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(b_broad,<span class="string">&quot;\n&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(a_broad + b_broad)</span><br><span class="line"> </span><br><span class="line">out:</span><br><span class="line">tensor([[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>],</span><br><span class="line">        [<span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>],</span><br><span class="line">        [<span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>]])</span><br><span class="line">tensor([[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>],</span><br><span class="line">        [<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>],</span><br><span class="line">        [<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>]]) </span><br><span class="line"> </span><br><span class="line">tensor([[<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>],</span><br><span class="line">        [<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>],</span><br><span class="line">        [<span class="number">2</span>, <span class="number">2</span>, <span class="number">2</span>]]) </span><br><span class="line"> </span><br><span class="line">tensor([[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>],</span><br><span class="line">        [<span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>],</span><br><span class="line">        [<span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>]])</span><br></pre></td></tr></table></figure>
<h4 id="其他基础知识">3. 其他基础知识</h4>
<p>上面介绍了pytorch的数据结构Tensor以及Tensor的一些操作函数，这里介绍深度学习必须用到的微分求导和动态计算图。</p>
<h5 id="自动微分机制">3.1 自动微分机制</h5>
<p>神经网络通常依赖<strong>反向传播求梯度来更新网络参数</strong>，求梯度过程通常是一件非常复杂而容易出错的事情。而深度学习框架可以帮助我们自动地完成这种求梯度运算。这就是Pytorch的自动微分机制是指： - <strong>Pytorch一般通过反向传播<code>backward()</code>方法实现这种求梯度计算。</strong>该方法求得的梯度将存在对应自变量张量的<code>grad·属性下。 - 除此之外，也能够调用</code>torch.autograd.grad()`函数来实现求梯度计算。</p>
<h6 id="backward方法求导数">3.1.1 backward方法求导数</h6>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">torch.autograd.backward(</span><br><span class="line">    tensors: <span class="type">Union</span>[torch.Tensor, <span class="type">Sequence</span>[torch.Tensor]],</span><br><span class="line">    gradient: <span class="type">Union</span>[torch.Tensor, <span class="type">Sequence</span>[torch.Tensor], NoneType] = <span class="literal">None</span>,</span><br><span class="line">    retain_graph: <span class="type">Union</span>[<span class="built_in">bool</span>, NoneType] = <span class="literal">None</span>,</span><br><span class="line">    create_graph: <span class="built_in">bool</span> = <span class="literal">False</span>,</span><br><span class="line">    grad_variables: <span class="type">Union</span>[torch.Tensor, <span class="type">Sequence</span>[torch.Tensor], NoneType] = <span class="literal">None</span>,</span><br><span class="line">    inputs: <span class="type">Union</span>[torch.Tensor, <span class="type">Sequence</span>[torch.Tensor], NoneType] = <span class="literal">None</span>,</span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<ul>
<li><code>tensor</code>:表示用于求导的张量，如loss，</li>
<li><code>gradient</code>: 设置梯度权重，在计算矩阵的梯度时会用到，也是一个tensor，shape和前面的tensor保持一致</li>
<li><code>retain_graph</code>:表示保存计算图，由于pytorch采用了动态图机制，在每一次反向传播之后，计算图都会被释放掉。如果不想释放，就设置这个参数为True</li>
<li><code>create_graph</code>:创建导数计算图，用于高阶求导</li>
</ul>
<p><strong><em>注</em></strong>：tensor类的<code>backward()</code>函数内部调用了<code>torch.autograd.backward()</code></p>
<p><code>backward()</code>方法通常在一个标量张量上调用，该方法求得的梯度将存在对应自变量张量的<code>grad</code>属性下。<strong>如果调用的张量非标量，则要传入一个和它同形状的gradient参数张量,改张量是设置梯度权重的</strong>。相当于用该gradient参数张量与调用张量作向量点乘，得到的标量结果再反向传播。</p>
<p><strong>下面分别介绍标量和非标量的反向传播：</strong> <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#标量的反向传播</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">test</span>():</span><br><span class="line">    <span class="comment">#f=ax**2+bx+c</span></span><br><span class="line">    x=torch.tensor(<span class="number">1.0</span>,requires_grad=<span class="literal">True</span>)</span><br><span class="line">    a=torch.tensor(<span class="number">1.0</span>)</span><br><span class="line">    b=torch.tensor(-<span class="number">2.0</span>)</span><br><span class="line">    c=torch.tensor(<span class="number">5.6</span>)</span><br><span class="line">    y=a*torch.<span class="built_in">pow</span>(x,<span class="number">2</span>)+b*x+c</span><br><span class="line">    y.backward()</span><br><span class="line">    dy_dx=x.grad</span><br><span class="line">    <span class="built_in">print</span>(dy_dx)</span><br><span class="line"></span><br><span class="line">输出：</span><br><span class="line">tensor(<span class="number">0.</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#非标量的反向传播</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">test1</span>():</span><br><span class="line">    <span class="comment"># f=ax**2+bx+c</span></span><br><span class="line">    x=torch.tensor([[<span class="number">0.0</span>,<span class="number">1.0</span>],[<span class="number">5.0</span>,<span class="number">2.0</span>]],requires_grad=<span class="literal">True</span>)</span><br><span class="line">    a = torch.tensor(<span class="number">1.0</span>)</span><br><span class="line">    b = torch.tensor(-<span class="number">2.0</span>)</span><br><span class="line">    c = torch.tensor(<span class="number">5.6</span>)</span><br><span class="line">    y = a * torch.<span class="built_in">pow</span>(x, <span class="number">2</span>) + b * x + c</span><br><span class="line">    gradient = torch.tensor([[<span class="number">1.0</span>, <span class="number">1.0</span>], [<span class="number">1.0</span>, <span class="number">1.0</span>]])</span><br><span class="line">    y.backward(gradient=gradient)</span><br><span class="line">    dy_dx=x.grad</span><br><span class="line">    <span class="built_in">print</span>(dy_dx)</span><br><span class="line"></span><br><span class="line">输出：</span><br><span class="line">tensor([[-<span class="number">2.</span>,  <span class="number">0.</span>],</span><br><span class="line">        [ <span class="number">8.</span>,  <span class="number">2.</span>]])</span><br></pre></td></tr></table></figure></p>
<p><strong>注：非标量的反向传播也可以用标量的反向传播实现，如下只需加一句<code>z = torch.sum(y*gradient)</code>，然后以<code>z.backward()</code>即可</strong> <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">z = torch.<span class="built_in">sum</span>(y*gradient)</span><br><span class="line">z.backward()</span><br><span class="line">x_grad = x.grad</span><br><span class="line"><span class="built_in">print</span>(x_grad)</span><br></pre></td></tr></table></figure></p>
<h6 id="利用autograd.grad方法求导数">3.1.2 利用autograd.grad方法求导数</h6>
<p><code>torch.autograd.grad()</code>这个方法的功能也是求梯度，可以实现高阶的求导。 <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">torch.autograd.grad(</span><br><span class="line">    outputs: <span class="type">Union</span>[torch.Tensor, <span class="type">Sequence</span>[torch.Tensor]],</span><br><span class="line">    inputs: <span class="type">Union</span>[torch.Tensor, <span class="type">Sequence</span>[torch.Tensor]],</span><br><span class="line">    grad_outputs: <span class="type">Union</span>[torch.Tensor, <span class="type">Sequence</span>[torch.Tensor], NoneType] = <span class="literal">None</span>,</span><br><span class="line">    retain_graph: <span class="type">Union</span>[<span class="built_in">bool</span>, NoneType] = <span class="literal">None</span>,</span><br><span class="line">    create_graph: <span class="built_in">bool</span> = <span class="literal">False</span>,</span><br><span class="line">    only_inputs: <span class="built_in">bool</span> = <span class="literal">True</span>,</span><br><span class="line">    allow_unused: <span class="built_in">bool</span> = <span class="literal">False</span>,</span><br><span class="line">) -&gt; <span class="type">Tuple</span>[torch.Tensor, ...]</span><br></pre></td></tr></table></figure> - <code>outputs</code>：用于求导的张量； - <code>inputs</code>: 需要梯度的张量； - <code>create_graph</code>:创建导数计算图，用于高阶求导 - <code>retain_graph</code>:保存计算图 - <code>grad_outputs</code>:多梯度权重</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#例2-1-2 利用autograd.grad方法求导数</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="comment"># f(x) = a*x**2 + b*x + c的导数</span></span><br><span class="line">x = torch.tensor(<span class="number">0.0</span>,requires_grad = <span class="literal">True</span>) <span class="comment"># x需要被求导</span></span><br><span class="line">a = torch.tensor(<span class="number">1.0</span>)</span><br><span class="line">b = torch.tensor(-<span class="number">2.0</span>)</span><br><span class="line">c = torch.tensor(<span class="number">1.0</span>)</span><br><span class="line">y = a*torch.<span class="built_in">pow</span>(x,<span class="number">2</span>) + b*x + c</span><br><span class="line"><span class="comment"># create_graph 设置为 True 将允许创建更高阶的导数</span></span><br><span class="line">dy_dx = torch.autograd.grad(y,x,create_graph=<span class="literal">True</span>)[<span class="number">0</span>]</span><br><span class="line"><span class="built_in">print</span>(dy_dx.data)</span><br><span class="line"><span class="comment"># 求二阶导数</span></span><br><span class="line">dy2_dx2 = torch.autograd.grad(dy_dx,x)[<span class="number">0</span>]</span><br><span class="line"><span class="built_in">print</span>(dy2_dx2.data)</span><br><span class="line"> </span><br><span class="line">out：</span><br><span class="line">tensor(-<span class="number">2.</span>)</span><br><span class="line">tensor(<span class="number">2.</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#例1-2-2 利用autograd.grad方法求导数，对多个自变量求导数</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line">x1 = torch.tensor(<span class="number">1.0</span>,requires_grad = <span class="literal">True</span>) <span class="comment"># x需要被求导</span></span><br><span class="line">x2 = torch.tensor(<span class="number">2.0</span>,requires_grad = <span class="literal">True</span>)</span><br><span class="line">y1 = x1*x2</span><br><span class="line">y2 = x1+x2</span><br><span class="line"><span class="comment"># 允许同时对多个自变量求导数</span></span><br><span class="line">(dy1_dx1,dy1_dx2) = torch.autograd.grad(outputs=y1,inputs =</span><br><span class="line">[x1,x2],retain_graph = <span class="literal">True</span>)</span><br><span class="line"><span class="built_in">print</span>(dy1_dx1,dy1_dx2)</span><br><span class="line"><span class="comment"># 如果有多个因变量，相当于把多个因变量的梯度结果求和</span></span><br><span class="line">(dy12_dx1,dy12_dx2) = torch.autograd.grad(outputs=[y1,y2],inputs = [x1,x2])</span><br><span class="line"><span class="built_in">print</span>(dy12_dx1,dy12_dx2)</span><br><span class="line"> </span><br><span class="line">out：</span><br><span class="line">tensor(<span class="number">2.</span>) tensor(<span class="number">1.</span>)</span><br><span class="line">tensor(<span class="number">3.</span>) tensor(<span class="number">2.</span>)</span><br></pre></td></tr></table></figure>
<h6 id="利用自动微分和优化器求最小值">3.1.3 利用自动微分和优化器求最小值</h6>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#例2-1-3 利用自动微分和优化器求最小值</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="comment"># f(x) = a*x**2 + b*x + c的最小值</span></span><br><span class="line">x = torch.tensor(<span class="number">0.0</span>,requires_grad = <span class="literal">True</span>) <span class="comment"># x需要被求导</span></span><br><span class="line">a = torch.tensor(<span class="number">1.0</span>)</span><br><span class="line">b = torch.tensor(-<span class="number">2.0</span>)</span><br><span class="line">c = torch.tensor(<span class="number">1.0</span>)</span><br><span class="line">optimizer = torch.optim.SGD(params=[x],lr = <span class="number">0.01</span>)</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">f</span>(<span class="params">x</span>):</span><br><span class="line">	result = a*torch.<span class="built_in">pow</span>(x,<span class="number">2</span>) + b*x + c</span><br><span class="line">	<span class="keyword">return</span>(result)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">500</span>):</span><br><span class="line">	optimizer.zero_grad()</span><br><span class="line">	y = f(x)</span><br><span class="line">	y.backward()</span><br><span class="line">	optimizer.step()</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;y=&quot;</span>,f(x).data,<span class="string">&quot;;&quot;</span>,<span class="string">&quot;x=&quot;</span>,x.data)</span><br><span class="line"> </span><br><span class="line">out：</span><br><span class="line">y= tensor(<span class="number">0.</span>) ; x= tensor(<span class="number">1.0000</span>)</span><br></pre></td></tr></table></figure>
<p><strong><em>注</em></strong>：优化器后续讲解</p>
<h5 id="动态计算图">3.2 动态计算图</h5>
<p>Pytorch的计算图由节点和边组成，<strong>节点表示张量或者Function，边表示张量和Function之间的依赖关系</strong>。Pytorch中的计算图是动态图。 <img src="/2022/10/06/pytorch%E5%9F%BA%E7%A1%80/computgraph.png" width="400"> 从上面可以看出<span class="math inline">\(y = a × b\)</span>，而<span class="math inline">\(a = w + x, b = w + 1\)</span>，只要给出<span class="math inline">\(x\)</span>和<span class="math inline">\(w\)</span>的值，即可根据计算图得出<span class="math inline">\(y\)</span>的值。上图中用求y对w的导数，根据求导规则，如下： <span class="math display">\[
{δy\over δw}={δy\over δa}{δa\over δw}+{δy \over δb}{δb \over δw}\\
=b*1+a*1\\
=b+a\\
=(w+1)+(x+w)\\
=2*w+x+1\\
=2*1+2+1\\
=5
\]</span> 体现到计算图中，就是根节点 y 到叶子节点 w 有两条路径 y -&gt; a -&gt; w和y -&gt;b -&gt; w。根节点依次对每条路径的叶子节点求导，一直到叶子节点w，最后把每条路径的导数相加即可 <img src="/2022/10/06/pytorch%E5%9F%BA%E7%A1%80/compute.png" width="400"> <strong>在tensor中包含一个<code>is_leaf</code>(叶子节点)属性，叶子节点就是用户创建的节点，在上面的例子中，<span class="math inline">\(x\)</span> 和<span class="math inline">\(w\)</span> 是叶子节点，其他所有节点都依赖于叶子节点</strong>。叶子节点的概念主要是为了节省内存，在计算图中的一轮反向传播结束之后，非叶子节点的梯度是会被释放的。</p>
<p>只有叶子节点的梯度保留了下来，而非叶子的梯度为空，如果在反向传播之后仍需要保留非叶子节点的梯度，可以对节点使用<code>retain_grad=True</code> <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#查看是否是叶子节点</span></span><br><span class="line"><span class="built_in">print</span>(w.is_leaf, x.is_leaf, a.is_leaf, b.is_leaf)</span><br><span class="line"></span><br><span class="line"><span class="comment">###result</span></span><br><span class="line"><span class="literal">True</span> <span class="literal">True</span> <span class="literal">False</span> <span class="literal">False</span></span><br></pre></td></tr></table></figure></p>
<h6 id="何为动态">3.2.1 何为动态</h6>
<p><strong>这里的动态主要有两重含义:</strong></p>
<ul>
<li><p><strong>第一层含义是</strong>：计算图的正向传播是立即执行的。无需等待完整的计算图创建完毕，每条语句都会在计算图中动态添加节点和边，并立即执行正向传播得到计算结果。</p></li>
<li><p><strong>第二层含义是</strong>：计算图在反向传播后立即销毁。下次调用需要重新构建计算图。如果在程序中使用了<code>backward()</code>方法执行了反向传播，或者利用<code>torch.autograd.grad()</code>方法计算了梯度，那么创建的计算图会被立即销毁，释放存储空间，下次调用需要重新创建</p></li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#例2-2-1 计算图的正向传播是立即执行的</span></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line">w = torch.tensor([[<span class="number">3.0</span>,<span class="number">1.0</span>]],requires_grad=<span class="literal">True</span>)</span><br><span class="line">b = torch.tensor([[<span class="number">3.0</span>]],requires_grad=<span class="literal">True</span>)</span><br><span class="line">X = torch.randn(<span class="number">10</span>,<span class="number">2</span>)</span><br><span class="line">Y = torch.randn(<span class="number">10</span>,<span class="number">1</span>)</span><br><span class="line">Y_hat = X@w.t() + b <span class="comment"># Y_hat定义后其正向传播被立即执行，与其后面的loss创建语句无关</span></span><br><span class="line">loss = torch.mean(torch.<span class="built_in">pow</span>(Y_hat-Y,<span class="number">2</span>))</span><br><span class="line"><span class="built_in">print</span>(loss.data)</span><br><span class="line"><span class="built_in">print</span>(Y_hat.data)</span><br><span class="line"> </span><br><span class="line">out：</span><br><span class="line">tensor(<span class="number">25.9445</span>)</span><br><span class="line">tensor([[ <span class="number">5.8349</span>],</span><br><span class="line">        [ <span class="number">0.5817</span>],</span><br><span class="line">        [-<span class="number">4.2764</span>],</span><br><span class="line">        [ <span class="number">3.2476</span>],</span><br><span class="line">        [ <span class="number">3.6737</span>],</span><br><span class="line">        [ <span class="number">2.8748</span>],</span><br><span class="line">        [ <span class="number">8.3981</span>],</span><br><span class="line">        [ <span class="number">7.1418</span>],</span><br><span class="line">        [-<span class="number">4.8522</span>],</span><br><span class="line">        [ <span class="number">2.2610</span>]])</span><br><span class="line"> </span><br><span class="line"> </span><br><span class="line"><span class="comment">#计算图在反向传播后立即销毁</span></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line">w = torch.tensor([[<span class="number">3.0</span>,<span class="number">1.0</span>]],requires_grad=<span class="literal">True</span>)</span><br><span class="line">b = torch.tensor([[<span class="number">3.0</span>]],requires_grad=<span class="literal">True</span>)</span><br><span class="line">X = torch.randn(<span class="number">10</span>,<span class="number">2</span>)</span><br><span class="line">Y = torch.randn(<span class="number">10</span>,<span class="number">1</span>)</span><br><span class="line">Y_hat = X@w.t() + b <span class="comment"># Y_hat定义后其正向传播被立即执行，与其后面的loss创建语句无关</span></span><br><span class="line">loss = torch.mean(torch.<span class="built_in">pow</span>(Y_hat-Y,<span class="number">2</span>))</span><br><span class="line"><span class="comment">#计算图在反向传播后立即销毁，如果需要保留计算图, 需要设置retain_graph = True</span></span><br><span class="line">loss.backward() <span class="comment">#loss.backward(retain_graph = True) </span></span><br><span class="line"><span class="comment">#loss.backward() #如果再次执行反向传播将报错</span></span><br></pre></td></tr></table></figure>
<h6 id="动态图机制">3.2.1 动态图机制</h6>
<p>pytroch采用的是动态图机制，而tensorflow采用的是静态图机制。静态图是先搭建，后运算；动态图是运算和搭建同时进行，也就是可以先计算前面节点的值，再根据这些值搭建后面的计算图。优点是灵活，易调节，易调试。 <img src="/2022/10/06/pytorch%E5%9F%BA%E7%A1%80/dynamicR.png" width="600"></p>
<h4 id="数据的读取">4. 数据的读取</h4>
<p>机器学习的五大模块分别是：数据、模块、损失函数、优化器和迭代训练。这里我们介绍数据模块，要对模型进行训练必须要有数据，怎么讲数据读取进来存储是我们要解决的问题。数据模块又可分为以下几部分：</p>
<ul>
<li>数据的收集：<code>Image、label</code></li>
<li>数据的划分：<code>train、test、valid</code></li>
<li>数据的读取：<code>DataLoader</code>，有两个子模块，<code>Sampler</code>和<code>Dataset</code>，<code>Sampler</code>是对数据集生成索引，<code>DataSet</code>是根据索引读取数据</li>
<li>数据预处理：<code>torchvision.transforms</code>模块</li>
</ul>
<p><strong>Pytorch通常使用<code>Dataset</code>和<code>DataLoader</code>这两个工具类来构建数据管道:</strong></p>
<ul>
<li><p><code>Dataset</code>定义了数据集的内容，它相当于一个类似列表的数据结构，具有确定的长度，能够用索 引获取数据集中的元素。</p></li>
<li><p><code>DataLoader</code>定义了按<code>batch</code>加载数据集的方法，它是一个实现了<code>__iter__</code>方法的可迭代对象，每次迭代输出一个<code>batch</code>的数据。<code>DataLoader</code>能够控制<code>batch</code>的大小，<code>batch</code>中元素的采样方法，以及将batch结果整理成模型所需 输入形式的方法，并且能够使用多进程读取数据。</p></li>
<li><p>在绝大部分情况下，用户只需实现<code>Dataset</code>的<code>__len__</code>方法和<code>__getitem__</code>方法，就可以轻松构 建自己的数据集，并用默认数据管道进行加载</p></li>
</ul>
<h5 id="dataloader和dataset概述">4.1 DataLoader和DataSet概述</h5>
<h6 id="获取一个batch数据的步骤">4.1.1 获取一个batch数据的步骤</h6>
<p>让我们考虑一下从一个数据集中获取一个batch的数据需要哪些步骤。 (假定数据集的特征和标签分别表示为张量 <span class="math inline">\(X\)</span> 和 <span class="math inline">\(Y\)</span> ，数据集可以表示为 <span class="math inline">\((X,Y)\)</span> , 假定batch大小为<span class="math inline">\(m\)</span> )</p>
<ul>
<li><ol type="a">
<li>首先我们要确定数据集的长度<span class="math inline">\(n\)</span> ,假设$ n = 1000$ 。</li>
</ol></li>
<li><ol start="2" type="a">
<li>然后我们从<span class="math inline">\(0\)</span> 到<span class="math inline">\(n-1\)</span>的范围中抽样出<span class="math inline">\(m\)</span>个数(batch大小)。假定<span class="math inline">\(m=4\)</span>, 拿到的结果是一个列表，类似： <span class="math inline">\(indices = [1,4,8,9]\)</span></li>
</ol></li>
<li><ol start="3" type="a">
<li>接着我们从数据集中去取这 m 个数对应下标的元素。 拿到的结果是一个元组列表，类似： <span class="math inline">\(samples = [(X[1],Y[1]),(X[4],Y[4]),(X[8],Y[8]), (X[9],Y[9])]\)</span></li>
</ol></li>
<li><ol start="4" type="a">
<li>最后我们将结果整理成两个张量作为输出。 拿到的结果是两个张量，类似$ batch = (features,labels)$其中 <span class="math inline">\(features = torch.stack([X[1],X[4],X[8],X[9]]) labels = torch.stack([Y[1],Y[4],Y[8],Y[9]])\)</span></li>
</ol></li>
</ul>
<h6 id="dataset和dataloader的功能分工">4.1.2 Dataset和DataLoader的功能分工</h6>
<ul>
<li><p>上述第a个步骤确定数据集的长度是由<code>Dataset</code>的<code>__len__</code>方法实现的。</p></li>
<li><p>第b个步骤从<span class="math inline">\(0\)</span> 到<span class="math inline">\(n-1\)</span>的范围中抽样出 m 个数的方法是由 <code>DataLoader</code>的 <code>sampler</code> 和 <code>batch_sampler</code> 参数指定的。<code>sampler</code>参数指定单个元素抽样方法，一般无需用户设置，程序默认在<code>DataLoader</code>的参数<code>shuffle=True</code>时采用随机抽样， <code>shuffle=False</code> 时采用顺序抽样。<code>batch_sampler</code>参数将多个抽样的元素整理成一个列表，一般无需用户设置，默认方法在<code>DataLoader</code>的参数 <code>drop_last=True</code>时会丢弃数据集最后一个长度不能被batch大小整除的批次，在 <code>drop_last=False</code>时保留最后一个批次。</p></li>
<li><p>第c个步骤的核心逻辑根据下标取数据集中的元素 是由<code>Dataset</code>的 <code>__getitem__</code>方法实现的。</p></li>
<li><p>第d个步骤的逻辑由<code>DataLoader</code>的参数 <code>collate_fn</code> 指定。一般情况下也无需用户设置。</p></li>
</ul>
<h5 id="使用dataset创建数据集">4.2 使用Dataset创建数据集</h5>
<p>Dataset创建数据集常用的方法有以下几个：</p>
<ul>
<li><ol type="a">
<li>使用 <code>torch.utils.data.TensorDataset</code> 根据<code>Tensor</code>创建数据集(<code>numpy</code>的<code>array</code>，<code>Pandas</code>的 <code>DataFrame</code>需要先转换成<code>Tensor</code>)。</li>
</ol></li>
<li><ol start="2" type="a">
<li>使用 <code>torchvision.datasets.ImageFolder</code> 根据图片目录创建图片数据集。 <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">dataset=torchvision.datasets.ImageFolder(</span><br><span class="line">                       root, transform=<span class="literal">None</span>, </span><br><span class="line">                       target_transform=<span class="literal">None</span>, </span><br><span class="line">                       loader=&lt;function default_loader&gt;, </span><br><span class="line">                       is_valid_file=<span class="literal">None</span>)</span><br><span class="line">root：图片存储的根目录，即各类别文件夹所在目录的上一级目录。</span><br><span class="line">transform：对图片进行预处理的操作（函数），原始图片作为输入，返回一个转换后的图片。</span><br><span class="line">target_transform：对图片类别进行预处理的操作，输入为 target，输出对其的转换。如果不传该参数，即对 target 不做任何转换，返回的顺序索引 <span class="number">0</span>,<span class="number">1</span>, <span class="number">2</span>…</span><br><span class="line">loader：表示数据集加载方式，通常默认加载方式即可。</span><br><span class="line">is_valid_file：获取图像文件的路径并检查该文件是否为有效文件的函数(用于检查损坏文件)</span><br></pre></td></tr></table></figure></li>
</ol></li>
<li><ol start="3" type="a">
<li>继承<code>torch.utils.data.Dataset</code> 创建自定义数据集。</li>
</ol></li>
<li><ol start="4" type="a">
<li>此外，还可以通过 <code>torch.utils.data.random_split</code> 将一个数据集分割成多份，常用于分割训练集，验证集和测试集。</li>
</ol></li>
<li><ol start="5" type="a">
<li>调用<code>Dataset</code>的加法运算符( + )将多个数据集合并成一个数据集。</li>
</ol></li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#例4-2-1  根据Tensor创建数据集</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> TensorDataset,Dataset,DataLoader,random_split</span><br><span class="line"> </span><br><span class="line"><span class="comment"># 根据Tensor创建数据集</span></span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> datasets</span><br><span class="line">iris = datasets.load_iris()</span><br><span class="line">ds_iris = TensorDataset(torch.tensor(iris.data),torch.tensor(iris.target))</span><br><span class="line"><span class="comment"># 分割成训练集和预测集</span></span><br><span class="line">n_train = <span class="built_in">int</span>(<span class="built_in">len</span>(ds_iris)*<span class="number">0.8</span>)</span><br><span class="line">n_valid = <span class="built_in">len</span>(ds_iris) - n_train</span><br><span class="line">ds_train,ds_valid = random_split(ds_iris,[n_train,n_valid])</span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">type</span>(ds_iris))</span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">type</span>(ds_train))</span><br><span class="line"> </span><br><span class="line">out:</span><br><span class="line">&lt;<span class="keyword">class</span> <span class="string">&#x27;torch.utils.data.dataset.TensorDataset&#x27;</span>&gt;</span><br><span class="line">&lt;<span class="keyword">class</span> <span class="string">&#x27;torch.utils.data.dataset.Subset&#x27;</span>&gt;</span><br><span class="line"> </span><br><span class="line"> </span><br><span class="line"> </span><br><span class="line"> </span><br><span class="line"><span class="comment"># 使用DataLoader加载数据集</span></span><br><span class="line">dl_train,dl_valid = DataLoader(ds_train,batch_size =</span><br><span class="line"><span class="number">8</span>),DataLoader(ds_valid,batch_size = <span class="number">8</span>)</span><br><span class="line"><span class="keyword">for</span> features,labels <span class="keyword">in</span> dl_train:</span><br><span class="line"> <span class="built_in">print</span>(features,labels)</span><br><span class="line"> <span class="keyword">break</span></span><br><span class="line"> </span><br><span class="line">out:</span><br><span class="line">tensor([[<span class="number">6.5000</span>, <span class="number">3.0000</span>, <span class="number">5.2000</span>, <span class="number">2.0000</span>],</span><br><span class="line">        [<span class="number">6.3000</span>, <span class="number">3.4000</span>, <span class="number">5.6000</span>, <span class="number">2.4000</span>],</span><br><span class="line">        [<span class="number">4.9000</span>, <span class="number">2.4000</span>, <span class="number">3.3000</span>, <span class="number">1.0000</span>],</span><br><span class="line">        [<span class="number">6.7000</span>, <span class="number">3.1000</span>, <span class="number">4.7000</span>, <span class="number">1.5000</span>],</span><br><span class="line">        [<span class="number">4.5000</span>, <span class="number">2.3000</span>, <span class="number">1.3000</span>, <span class="number">0.3000</span>],</span><br><span class="line">        [<span class="number">5.7000</span>, <span class="number">2.5000</span>, <span class="number">5.0000</span>, <span class="number">2.0000</span>],</span><br><span class="line">        [<span class="number">5.2000</span>, <span class="number">4.1000</span>, <span class="number">1.5000</span>, <span class="number">0.1000</span>],</span><br><span class="line">        [<span class="number">5.7000</span>, <span class="number">2.6000</span>, <span class="number">3.5000</span>, <span class="number">1.0000</span>]], dtype=torch.float64) tensor([<span class="number">2</span>, <span class="number">2</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">2</span>, <span class="number">0</span>, <span class="number">1</span>], dtype=torch.int32)</span><br><span class="line"> </span><br><span class="line"> </span><br><span class="line"> </span><br><span class="line"><span class="comment"># 演示加法运算符（`+`）的合并作用</span></span><br><span class="line">ds_data = ds_train + ds_valid</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;len(ds_train) = &#x27;</span>,<span class="built_in">len</span>(ds_train))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;len(ds_valid) = &#x27;</span>,<span class="built_in">len</span>(ds_valid))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;len(ds_train+ds_valid) = &#x27;</span>,<span class="built_in">len</span>(ds_data))</span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">type</span>(ds_data))</span><br><span class="line"> </span><br><span class="line">out:</span><br><span class="line"><span class="built_in">len</span>(ds_train) =  <span class="number">120</span></span><br><span class="line"><span class="built_in">len</span>(ds_valid) =  <span class="number">30</span></span><br><span class="line"><span class="built_in">len</span>(ds_train+ds_valid) =  <span class="number">150</span></span><br><span class="line">&lt;<span class="keyword">class</span><span class="string">&#x27;torch.utils.data.dataset.ConcatDataset&#x27;</span>&gt;</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#4-2-2  根据图片目录创建图片数据集</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> torch </span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> DataLoader</span><br><span class="line"><span class="keyword">from</span> torchvision <span class="keyword">import</span> transforms,datasets</span><br><span class="line"> </span><br><span class="line"><span class="comment">#演示一些常用的图片增强操作</span></span><br><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image</span><br><span class="line">img = Image.<span class="built_in">open</span>(<span class="string">&#x27;./data/dog2.jpg&#x27;</span>)</span><br><span class="line">img</span><br><span class="line"> </span><br><span class="line"><span class="comment"># 随机数值翻转</span></span><br><span class="line">transforms.RandomVerticalFlip()(img)</span><br><span class="line"> </span><br><span class="line"><span class="comment">#随机旋转</span></span><br><span class="line">transforms.RandomRotation(<span class="number">45</span>)(img)</span><br><span class="line"> </span><br><span class="line"><span class="comment"># 定义图片增强操作</span></span><br><span class="line">transform_train = transforms.Compose([</span><br><span class="line">	 transforms.RandomHorizontalFlip(), <span class="comment">#随机水平翻转</span></span><br><span class="line">	 transforms.RandomVerticalFlip(), <span class="comment">#随机垂直翻转</span></span><br><span class="line">	 transforms.RandomRotation(<span class="number">45</span>), <span class="comment">#随机在45度角度内旋转</span></span><br><span class="line">	 transforms.ToTensor() <span class="comment">#转换成张量</span></span><br><span class="line">	 ]</span><br><span class="line">)</span><br><span class="line">transform_valid = transforms.Compose([</span><br><span class="line">	 transforms.ToTensor()</span><br><span class="line">	 ]</span><br><span class="line">)</span><br><span class="line"> </span><br><span class="line"><span class="comment"># 根据图片目录创建数据集</span></span><br><span class="line"><span class="comment"># 这里用到的animal数据集是我自己整理的，链接在文章末尾</span></span><br><span class="line"><span class="comment">#注意这里要在train 和  test 目录下按照图片类别分别新建文件夹，文件夹的名称就是类别名，然后把图片分别放入各个文件夹</span></span><br><span class="line">ds_train = datasets.ImageFolder(<span class="string">&quot;data/animal/train/&quot;</span>, transform = transform_train,target_transform= <span class="keyword">lambda</span> t:torch.tensor([t]).<span class="built_in">float</span>())</span><br><span class="line">ds_valid = datasets.ImageFolder(<span class="string">&quot;data/animal/test/&quot;</span>, transform = transform_valid,target_transform= <span class="keyword">lambda</span> t:torch.tensor([t]).<span class="built_in">float</span>())</span><br><span class="line"><span class="built_in">print</span>(ds_train.class_to_idx)</span><br><span class="line"> </span><br><span class="line"><span class="comment"># 使用DataLoader加载数据集</span></span><br><span class="line">dl_train = DataLoader(ds_train,batch_size = <span class="number">2</span>,shuffle = <span class="literal">True</span>,num_workers=<span class="number">1</span>)</span><br><span class="line">dl_valid = DataLoader(ds_valid,batch_size = <span class="number">2</span>,shuffle = <span class="literal">True</span>,num_workers=<span class="number">1</span>)</span><br><span class="line"> </span><br><span class="line"><span class="keyword">for</span> features,labels <span class="keyword">in</span> dl_train:</span><br><span class="line">    <span class="built_in">print</span>(features)</span><br><span class="line">    <span class="built_in">print</span>(labels)</span><br><span class="line">    <span class="keyword">break</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#例4-2-3  创建自定义数据集 </span></span><br><span class="line"><span class="comment">#下面通过继承Dataset类创建douban文本分类任务的自定义数据集。 douban数据集链接在文章末尾。</span></span><br><span class="line"><span class="comment">#大概思路如下：首先，对训练集文本分词构建词典。然后将训练集文本和测试集文本数据转换成 token单词编码。 接着将转换成单词编码的训练集数据和测试集数据按样本分割成多个文件，一个文件代表一个样本。 最后，我们可以根据文件名列表获取对应序号的样本内容，从而构建Dataset数据集。</span></span><br><span class="line"> </span><br><span class="line"> </span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> OrderedDict</span><br><span class="line"><span class="keyword">import</span> re,string,jieba,csv</span><br><span class="line"> </span><br><span class="line"><span class="comment">#from keras.datasets import imdb</span></span><br><span class="line"><span class="comment">#(train_data, train_labels), (test_data, test_labels) = imdb.load_data(num_words=10000)</span></span><br><span class="line"> </span><br><span class="line">MAX_WORDS = <span class="number">10000</span> <span class="comment"># 仅考虑最高频的10000个词</span></span><br><span class="line">MAX_LEN = <span class="number">200</span> <span class="comment"># 每个样本保留200个词的长度</span></span><br><span class="line">BATCH_SIZE = <span class="number">20</span></span><br><span class="line">train_data_path = <span class="string">&#x27;data/douban/train.csv&#x27;</span></span><br><span class="line">test_data_path = <span class="string">&#x27;data/douban/test.csv&#x27;</span></span><br><span class="line">train_token_path = <span class="string">&#x27;data/douban/train_token.csv&#x27;</span></span><br><span class="line">test_token_path = <span class="string">&#x27;data/douban/test_token.csv&#x27;</span></span><br><span class="line">train_samples_path = <span class="string">&#x27;data/douban/train_samples/&#x27;</span></span><br><span class="line">test_samples_path = <span class="string">&#x27;data/douban/test_samples/&#x27;</span></span><br><span class="line"> </span><br><span class="line"><span class="comment">#print(train_data[0])</span></span><br><span class="line"> </span><br><span class="line"> </span><br><span class="line"><span class="comment">##构建词典</span></span><br><span class="line">word_count_dict = &#123;&#125;</span><br><span class="line"><span class="comment">#清洗文本</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">clean_text</span>(<span class="params">text</span>):</span><br><span class="line">    bd=<span class="string">&#x27;[’!&quot;#$%&amp;\&#x27;()*+,-./:;&lt;=&gt;?@[\\]^_`&#123;|&#125;~]+，。！？“”《》：、． &#x27;</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> bd:</span><br><span class="line">        text=text.replace(i,<span class="string">&#x27;&#x27;</span>)   <span class="comment">#字符串替换去标点符号</span></span><br><span class="line">    fenci=jieba.lcut(text)</span><br><span class="line">    <span class="keyword">return</span> fenci</span><br><span class="line"> </span><br><span class="line"><span class="keyword">with</span> <span class="built_in">open</span>(train_data_path,<span class="string">&quot;r&quot;</span>,encoding = <span class="string">&#x27;utf-8&#x27;</span>,newline=<span class="string">&#x27;&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">    reader = csv.reader(f,delimiter=<span class="string">&#x27;,&#x27;</span>)</span><br><span class="line">    <span class="keyword">for</span> row <span class="keyword">in</span> reader:</span><br><span class="line">        <span class="comment">#print(row)</span></span><br><span class="line">        text = row[<span class="number">1</span>]</span><br><span class="line">        label = row[<span class="number">0</span>]</span><br><span class="line">        <span class="comment">#print(label,text)</span></span><br><span class="line">        cleaned_text = clean_text(text)</span><br><span class="line">        <span class="keyword">for</span> word <span class="keyword">in</span> cleaned_text:</span><br><span class="line">            <span class="comment">#print(word)</span></span><br><span class="line">            word_count_dict[word] = word_count_dict.get(word,<span class="number">0</span>)+<span class="number">1</span></span><br><span class="line"> </span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">len</span>(word_count_dict))</span><br><span class="line"> </span><br><span class="line">df_word_dict = pd.DataFrame(pd.Series(word_count_dict,name = <span class="string">&quot;count&quot;</span>))</span><br><span class="line">df_word_dict = df_word_dict.sort_values(by = <span class="string">&quot;count&quot;</span>,ascending =<span class="literal">False</span>)</span><br><span class="line">df_word_dict = df_word_dict[<span class="number">0</span>:MAX_WORDS-<span class="number">2</span>] <span class="comment"># </span></span><br><span class="line">df_word_dict[<span class="string">&quot;word_id&quot;</span>] = <span class="built_in">range</span>(<span class="number">2</span>,MAX_WORDS) <span class="comment">#编号0和1分别留给未知词&lt;unkown&gt;和填充&lt;padding&gt;</span></span><br><span class="line">word_id_dict = df_word_dict[<span class="string">&quot;word_id&quot;</span>].to_dict()</span><br><span class="line">df_word_dict.head(<span class="number">10</span>)</span><br><span class="line"> </span><br><span class="line">out:</span><br><span class="line">    count	word_id</span><br><span class="line">的	<span class="number">68229</span>	<span class="number">2</span></span><br><span class="line">了	<span class="number">20591</span>	<span class="number">3</span></span><br><span class="line">是	<span class="number">15321</span>	<span class="number">4</span></span><br><span class="line">我	<span class="number">9312</span>	<span class="number">5</span></span><br><span class="line">看	<span class="number">7423</span>	<span class="number">6</span></span><br><span class="line">很	<span class="number">7395</span>	<span class="number">7</span></span><br><span class="line">也	<span class="number">7256</span>	<span class="number">8</span></span><br><span class="line">都	<span class="number">7053</span>	<span class="number">9</span></span><br><span class="line">在	<span class="number">6753</span>	<span class="number">10</span></span><br><span class="line">和	<span class="number">6388</span>	<span class="number">11</span></span><br><span class="line"> </span><br><span class="line"> </span><br><span class="line"><span class="comment">#转换token</span></span><br><span class="line"><span class="comment"># 填充文本</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">pad</span>(<span class="params">data_list,pad_length</span>):</span><br><span class="line">    padded_list = data_list.copy()</span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">len</span>(data_list)&gt; pad_length:</span><br><span class="line">        padded_list = data_list[-pad_length:]</span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">len</span>(data_list)&lt; pad_length:</span><br><span class="line">        padded_list = [<span class="number">1</span>]*(pad_length-<span class="built_in">len</span>(data_list))+data_list</span><br><span class="line">    <span class="keyword">return</span> padded_list</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">text_to_token</span>(<span class="params">text_file,token_file</span>):</span><br><span class="line">    <span class="keyword">with</span> <span class="built_in">open</span>(train_data_path,<span class="string">&quot;r&quot;</span>,encoding = <span class="string">&#x27;utf-8&#x27;</span>,newline=<span class="string">&#x27;&#x27;</span>) <span class="keyword">as</span> f,\</span><br><span class="line">        <span class="built_in">open</span>(token_file,<span class="string">&quot;w&quot;</span>,encoding = <span class="string">&#x27;utf-8&#x27;</span>) <span class="keyword">as</span> fout:</span><br><span class="line">        reader = csv.reader(f,delimiter=<span class="string">&#x27;,&#x27;</span>)</span><br><span class="line">        <span class="keyword">for</span> row <span class="keyword">in</span> reader:</span><br><span class="line">            text = row[<span class="number">1</span>]</span><br><span class="line">            label = row[<span class="number">0</span>]</span><br><span class="line">            cleaned_text = clean_text(text)</span><br><span class="line">            word_token_list = [word_id_dict.get(word, <span class="number">0</span>) <span class="keyword">for</span> word <span class="keyword">in</span> cleaned_text]</span><br><span class="line">            pad_list = pad(word_token_list,MAX_LEN)</span><br><span class="line">            out_line = label+<span class="string">&quot;\t&quot;</span>+<span class="string">&quot; &quot;</span>.join([<span class="built_in">str</span>(x) <span class="keyword">for</span> x <span class="keyword">in</span> pad_list])</span><br><span class="line">            fout.write(out_line+<span class="string">&quot;\n&quot;</span>)</span><br><span class="line">text_to_token(train_data_path,train_token_path)</span><br><span class="line">text_to_token(test_data_path,test_token_path)</span><br><span class="line"> </span><br><span class="line"> </span><br><span class="line"> </span><br><span class="line"><span class="comment"># 分割样本</span></span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">if</span> <span class="keyword">not</span> os.path.exists(train_samples_path):</span><br><span class="line">    os.mkdir(train_samples_path)</span><br><span class="line"><span class="keyword">if</span> <span class="keyword">not</span> os.path.exists(test_samples_path):</span><br><span class="line">    os.mkdir(test_samples_path)</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">split_samples</span>(<span class="params">token_path,samples_dir</span>):</span><br><span class="line">    <span class="keyword">with</span> <span class="built_in">open</span>(token_path,<span class="string">&quot;r&quot;</span>,encoding = <span class="string">&#x27;utf-8&#x27;</span>) <span class="keyword">as</span> fin:</span><br><span class="line">        i = <span class="number">0</span></span><br><span class="line">        <span class="keyword">for</span> line <span class="keyword">in</span> fin:</span><br><span class="line">            <span class="keyword">with</span> <span class="built_in">open</span>(samples_dir+<span class="string">&quot;%d.txt&quot;</span>%i,<span class="string">&quot;w&quot;</span>,encoding = <span class="string">&quot;utf-8&quot;</span>) <span class="keyword">as</span> fout:</span><br><span class="line">                fout.write(line)</span><br><span class="line">            i = i+<span class="number">1</span></span><br><span class="line">split_samples(train_token_path,train_samples_path)</span><br><span class="line">split_samples(test_token_path,test_samples_path)</span><br><span class="line"> </span><br><span class="line"> </span><br><span class="line"> </span><br><span class="line"> </span><br><span class="line"> </span><br><span class="line"><span class="comment">#创建数据集</span></span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> DataLoader,Dataset</span><br><span class="line"><span class="keyword">from</span> torchvision <span class="keyword">import</span> transforms,datasets</span><br><span class="line"><span class="keyword">class</span> <span class="title class_">imdbDataset</span>(<span class="title class_ inherited__">Dataset</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self,samples_dir</span>):</span><br><span class="line">        self.samples_dir = samples_dir</span><br><span class="line">        self.samples_paths = os.listdir(samples_dir)</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__len__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">len</span>(self.samples_paths)</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__getitem__</span>(<span class="params">self,index</span>):</span><br><span class="line">        path = self.samples_dir + self.samples_paths[index]</span><br><span class="line">        <span class="keyword">with</span> <span class="built_in">open</span>(path,<span class="string">&quot;r&quot;</span>,encoding = <span class="string">&quot;utf-8&quot;</span>) <span class="keyword">as</span> f:</span><br><span class="line">            line = f.readline()</span><br><span class="line">            label,tokens = line.split(<span class="string">&quot;\t&quot;</span>)</span><br><span class="line">            label = torch.tensor([<span class="built_in">float</span>(label)],dtype = torch.<span class="built_in">float</span>)</span><br><span class="line">            feature = torch.tensor([<span class="built_in">int</span>(x) <span class="keyword">for</span> x <span class="keyword">in</span> tokens.split(<span class="string">&quot; &quot;</span>)],dtype = torch.long)</span><br><span class="line">            <span class="keyword">return</span> (feature,label)</span><br><span class="line">ds_train = imdbDataset(train_samples_path)</span><br><span class="line">ds_test = imdbDataset(test_samples_path)</span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">len</span>(ds_train))</span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">len</span>(ds_test))</span><br><span class="line"> </span><br><span class="line"> </span><br><span class="line">dl_train = DataLoader(ds_train,batch_size = BATCH_SIZE,shuffle = <span class="literal">True</span>,num_workers=<span class="number">4</span>)</span><br><span class="line">dl_test = DataLoader(ds_test,batch_size = BATCH_SIZE,num_workers=<span class="number">4</span>)</span><br><span class="line"><span class="keyword">for</span> features,labels <span class="keyword">in</span> dl_train:</span><br><span class="line">    <span class="built_in">print</span>(features)</span><br><span class="line">    <span class="built_in">print</span>(labels)</span><br><span class="line">    <span class="keyword">break</span></span><br><span class="line"> </span><br><span class="line">out:</span><br><span class="line"> </span><br><span class="line"> </span><br><span class="line"> </span><br><span class="line"><span class="comment">#创建模型</span></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">import</span> importlib</span><br><span class="line"><span class="keyword">from</span> torchkeras <span class="keyword">import</span> Model,summary</span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Net</span>(<span class="title class_ inherited__">Model</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(Net, self).__init__()   <span class="comment">#设置padding_idx参数后将在训练过程中将填充的token始终赋值为0向量</span></span><br><span class="line">        self.embedding = nn.Embedding(num_embeddings = MAX_WORDS,embedding_dim = <span class="number">3</span>,padding_idx = <span class="number">1</span>)</span><br><span class="line">        self.conv = nn.Sequential()</span><br><span class="line">        self.conv.add_module(<span class="string">&quot;conv_1&quot;</span>,nn.Conv1d(in_channels = <span class="number">3</span>,out_channels = <span class="number">16</span>,kernel_size = <span class="number">5</span>))</span><br><span class="line">        self.conv.add_module(<span class="string">&quot;pool_1&quot;</span>,nn.MaxPool1d(kernel_size = <span class="number">2</span>))</span><br><span class="line">        self.conv.add_module(<span class="string">&quot;relu_1&quot;</span>,nn.ReLU())</span><br><span class="line">        self.conv.add_module(<span class="string">&quot;conv_2&quot;</span>,nn.Conv1d(in_channels = <span class="number">16</span>,out_channels = <span class="number">128</span>,kernel_size = <span class="number">2</span>))</span><br><span class="line">        self.conv.add_module(<span class="string">&quot;pool_2&quot;</span>,nn.MaxPool1d(kernel_size = <span class="number">2</span>))</span><br><span class="line">        self.conv.add_module(<span class="string">&quot;relu_2&quot;</span>,nn.ReLU())</span><br><span class="line">        self.dense = nn.Sequential()</span><br><span class="line">        self.dense.add_module(<span class="string">&quot;flatten&quot;</span>,nn.Flatten())</span><br><span class="line">        self.dense.add_module(<span class="string">&quot;linear&quot;</span>,nn.Linear(<span class="number">6144</span>,<span class="number">1</span>))</span><br><span class="line">        self.dense.add_module(<span class="string">&quot;sigmoid&quot;</span>,nn.Sigmoid())</span><br><span class="line">        <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self,x</span>):</span><br><span class="line">            x = self.embedding(x).transpose(<span class="number">1</span>,<span class="number">2</span>)</span><br><span class="line">            x = self.conv(x)</span><br><span class="line">            y = self.dense(x)</span><br><span class="line">            <span class="keyword">return</span> y</span><br><span class="line">model = Net()</span><br><span class="line"><span class="built_in">print</span>(model)</span><br><span class="line">model.summary(input_shape = (<span class="number">200</span>,),input_dtype = torch.LongTensor)</span><br><span class="line"> </span><br><span class="line"> </span><br><span class="line"> </span><br><span class="line"><span class="comment"># 编译模型</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">accuracy</span>(<span class="params">y_pred,y_true</span>):</span><br><span class="line">    y_pred = torch.where(y_pred&gt;<span class="number">0.5</span>,torch.ones_like(y_pred,dtype = torch.float32),torch.zeros_like(y_pred,dtype = torch.float32))</span><br><span class="line">    acc = torch.mean(<span class="number">1</span>-torch.<span class="built_in">abs</span>(y_true-y_pred))</span><br><span class="line">    <span class="keyword">return</span> acc</span><br><span class="line">model.<span class="built_in">compile</span>(loss_func = nn.BCELoss(),optimizer=</span><br><span class="line">torch.optim.Adagrad(model.parameters(),lr = <span class="number">0.02</span>),</span><br><span class="line">metrics_dict=&#123;<span class="string">&quot;accuracy&quot;</span>:accuracy&#125;)</span><br><span class="line"><span class="comment"># 训练模型</span></span><br><span class="line">dfhistory = model.fit(<span class="number">10</span>,dl_train,dl_val=dl_test,log_step_freq= <span class="number">200</span>)</span><br></pre></td></tr></table></figure>
<h5 id="使用dataloader加载数据集">4.3 使用DataLoader加载数据集</h5>
<p>DataLoader能够控制batch的大小，batch中元素的采样方法，以及将batch结果整理成模型所需输入形式的方法，并且能够使用多进程读取数据。 <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">DataLoader(</span><br><span class="line"> dataset,</span><br><span class="line"> batch_size=<span class="number">1</span>,</span><br><span class="line"> shuffle=<span class="literal">False</span>,</span><br><span class="line"> sampler=<span class="literal">None</span>,</span><br><span class="line"> batch_sampler=<span class="literal">None</span>,</span><br><span class="line"> num_workers=<span class="number">0</span>,</span><br><span class="line"> collate_fn=<span class="literal">None</span>,</span><br><span class="line"> pin_memory=<span class="literal">False</span>,</span><br><span class="line"> drop_last=<span class="literal">False</span>,</span><br><span class="line"> timeout=<span class="number">0</span>,</span><br><span class="line"> worker_init_fn=<span class="literal">None</span>,</span><br><span class="line"> multiprocessing_context=<span class="literal">None</span>,</span><br><span class="line">)</span><br></pre></td></tr></table></figure> 一般情况下，我们仅仅会配置<code>dataset, batch_size, shuffle, num_workers, drop_last</code>这五个参 数，其他参数使用默认值即可。</p>
<ul>
<li><strong><code>dataset</code></strong> : 数据集</li>
<li><strong><code>batch_size</code></strong>: 批次大小</li>
<li><strong><code>shuffle</code></strong>: 是否乱序</li>
<li><code>sampler</code>: 样本采样函数，一般无需设置。</li>
<li><code>batch_sampler</code>: 批次采样函数，一般无需设置。</li>
<li><code>num_workers</code>: 使用多进程读取数据，设置的进程数。</li>
<li><code>collate_fn</code>: 整理一个批次数据的函数。</li>
<li><code>pin_memory</code>: 是否设置为锁业内存。默认为<code>False</code>，锁业内存不会使用虚拟内存(硬盘)，从锁 业内存拷贝到GPU上速度会更快。</li>
<li><strong><code>drop_last</code></strong>: 是否丢弃最后一个样本数量不足<code>batch_size</code>批次数据。</li>
<li><code>timeout</code>: 加载一个数据批次的最长等待时间，一般无需设置。</li>
<li><code>worker_init_fn</code>: 每个<code>worker</code>中<code>dataset</code>的初始化函数，常用于 <code>terableDataset</code>。一般不使用。</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#例3-3 使用DataLoader加载数据集</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> torch </span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> DataLoader,TensorDataset,Dataset</span><br><span class="line"><span class="keyword">from</span> torchvision <span class="keyword">import</span> transforms,datasets</span><br><span class="line"><span class="comment">#构建输入数据管道</span></span><br><span class="line">ds = TensorDataset(torch.arange(<span class="number">1</span>,<span class="number">50</span>))</span><br><span class="line">dl = DataLoader(ds,</span><br><span class="line"> batch_size = <span class="number">10</span>,</span><br><span class="line"> shuffle= <span class="literal">True</span>,</span><br><span class="line"> num_workers=<span class="number">2</span>,</span><br><span class="line"> drop_last = <span class="literal">True</span>)</span><br><span class="line"><span class="comment">#迭代数据</span></span><br><span class="line"><span class="keyword">for</span> batch, <span class="keyword">in</span> dl:</span><br><span class="line">    <span class="built_in">print</span>(batch)</span><br><span class="line"> </span><br><span class="line">out:</span><br><span class="line">tensor([<span class="number">35</span>, <span class="number">19</span>,  <span class="number">3</span>,  <span class="number">1</span>, <span class="number">24</span>, <span class="number">20</span>,  <span class="number">8</span>, <span class="number">37</span>, <span class="number">32</span>, <span class="number">38</span>])</span><br><span class="line">tensor([<span class="number">28</span>, <span class="number">26</span>,  <span class="number">7</span>, <span class="number">48</span>,  <span class="number">4</span>, <span class="number">41</span>, <span class="number">15</span>, <span class="number">45</span>, <span class="number">11</span>, <span class="number">14</span>])</span><br><span class="line">tensor([<span class="number">23</span>,  <span class="number">5</span>, <span class="number">10</span>,  <span class="number">6</span>, <span class="number">18</span>, <span class="number">39</span>, <span class="number">31</span>, <span class="number">22</span>, <span class="number">42</span>, <span class="number">12</span>])</span><br><span class="line">tensor([<span class="number">34</span>, <span class="number">47</span>, <span class="number">30</span>, <span class="number">25</span>, <span class="number">29</span>, <span class="number">49</span>, <span class="number">44</span>, <span class="number">46</span>, <span class="number">33</span>, <span class="number">13</span>])</span><br></pre></td></tr></table></figure>
<h4 id="数据的预处理模块">5. 数据的预处理模块</h4>
<p>transforms是pytorch中常用的图像预处理方法，这个在torchvision计算机视觉工具包中。在安装pytorch时顺便安装了torchvision，在torchvision中，有三个主要的模块：</p>
<ul>
<li><code>torchvision.transforms</code>:常用的图像预处理方法，比如：标准化、中心化、旋转、翻转等；</li>
<li><code>torchvision.datasets</code>:常用的数据集的<code>dataset</code>实现，例如：MNIST、CIFAR-10、ImageNet等；</li>
<li><code>torchvision.models</code>:常用的预训练模型，AlexNet、VGG、ResNet等。</li>
</ul>
<h5 id="裁剪">5.1 裁剪</h5>
<h6 id="随机裁剪transforms.randomcrop">5.1.1 随机裁剪：transforms.RandomCrop</h6>
<p>该函数根据给定的size进行随机裁剪 <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">transforms.RandomCrop(</span><br><span class="line">    size,</span><br><span class="line">    padding=<span class="literal">None</span>,</span><br><span class="line">    pad_if_needed=<span class="literal">False</span>,</span><br><span class="line">    fill=<span class="number">0</span>,</span><br><span class="line">    padding_mode=<span class="string">&#x27;constant&#x27;</span>,</span><br><span class="line">)</span><br></pre></td></tr></table></figure> - `size·：可为sequence or int，若为sequence，则为（h， w），若为int，则为（int， int);</p>
<ul>
<li><p>`padding·：可为int or sequence，此参数是设置填充多少个pixel；若为int，表示图像上下左右均填充int个pixel，例如padding=4，表示图像上下左右均填充4个pixel，若为32×32，则图像填充后为40×40；若为sequence，若为2个数，第一个数表示左右填充多少，第二个数表示上下填充多少；当有四个数时表示左、上、右、下</p></li>
<li><p>·pad_if_needed·：若图像小于设定的size，则填充；</p></li>
<li><p>fill：表示需要填充的值，默认为0.当值为int时，表示各通道均填充该值，当值为3时，表示RGB三个通道各需要填充的值；</p></li>
<li><code>padding_mode</code>：填充模式，有4中填充模式：
<ul>
<li>1、<code>constant</code>：常数填充；</li>
<li>2、<code>edge</code>：图像的边缘值填充；</li>
<li>3、<code>reflect</code>：镜像填充，最后一个像素不镜像，例如 [1, 2, 3, 4]. -&gt; [3, 2, 1, 2, 3, 4, 3, 2]；</li>
<li>4、<code>symmetric</code>:镜像填充，最后一个元素填充，例如：[1, 2, 3, 4] -&gt; [2, 1, 1, 2, 3, 4, 4, 3]</li>
</ul></li>
</ul>
<h6 id="中心裁剪transforms.centercrop">5.1.2 中心裁剪transforms.CenterCrop</h6>
<p>依据给定的参数进行中心裁剪 <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">torchvision.transforms.CenterCrop(size)</span><br><span class="line"></span><br><span class="line">size：若为sequence，则为（h, w）, 若为<span class="built_in">int</span>， 则为（<span class="built_in">int</span>， <span class="built_in">int</span>）</span><br></pre></td></tr></table></figure></p>
<h6 id="随机长宽比裁剪transforms.randomresizedcrop">5.1.3 随机长宽比裁剪transforms.RandomResizedCrop()</h6>
<p>随机大小，随机长宽比裁剪原始图片，最后将图片 resize 到设定好的 size <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">torchvision.transforms.RandomResizedCrop(</span><br><span class="line">    size,</span><br><span class="line">    scale=(<span class="number">0.08</span>, <span class="number">1.0</span>),</span><br><span class="line">    ratio=(<span class="number">0.75</span>, <span class="number">1.3333333333333333</span>),</span><br><span class="line">    interpolation=<span class="number">2</span>,</span><br><span class="line">)</span><br></pre></td></tr></table></figure> - <code>size</code>：所需裁减图片尺寸 - <code>scale</code>：随机 crop 的大小区间，如 scale=(0.08, 1.0)，表示随机 crop 出来的图片会在的 0.08 倍至 1 倍之间。 - <code>ratio</code>： 随机长宽比设置</p>
<h6 id="上下左右中心裁剪transforms.fivecrop">5.1.4 上下左右中心裁剪transforms.FiveCrop()</h6>
<p>对图片进行上下左右以及中心裁剪，获得 5 张图片，返回一个 4D-tensor <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torchvision.transforms.FiveCrop(size)</span><br></pre></td></tr></table></figure></p>
<h5 id="翻转和旋转">5.2 翻转和旋转</h5>
<h6 id="翻转">5.2.1 翻转</h6>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#依据概率 p 对 PIL 图片进行水平翻转</span></span><br><span class="line">torchvision.transforms.RandomHorizontalFlip(p=<span class="number">0.5</span>)</span><br><span class="line"><span class="comment">#依概率p垂直翻转</span></span><br><span class="line">torchvision.transforms.RandomVerticalFlip(p=<span class="number">0.5</span>)</span><br><span class="line">p为概率值</span><br></pre></td></tr></table></figure>
<h6 id="旋转">5.2.2 旋转</h6>
<p>依 degrees 随机旋转一定角度 <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">torchvision.transforms.RandomRotation(degrees, resample=<span class="literal">False</span>, </span><br><span class="line">                                      expand=<span class="literal">False</span>, center=<span class="literal">None</span>)</span><br></pre></td></tr></table></figure> - <code>degress</code>：(sequence or float or int) ，若为单个数，如 30，则表示在（-30，+30）之间随机旋转；若为sequence，如(30，60)，则表示在 30-60 度之间随机旋转； - <code>resample</code>：重采样方法选择，可选 PIL.Image.NEAREST, PIL.Image.BILINEAR,PIL.Image.BICUBIC，默认为最近邻; - <code>expand</code>: 是否扩大图片，以保持原图信息； - <code>center</code>: 设置旋转点，默认是中心旋转</p>
<h5 id="图像变换">5.3 图像变换</h5>
<h6 id="resize">5.3.1 resize</h6>
<p>重置图像分辨率 <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torchvision.transforms.Resize(size, interpolation=<span class="number">2</span>)</span><br></pre></td></tr></table></figure></p>
<h6 id="标准化">5.3.2 标准化</h6>
<p>对数据按通道进行标准化，即先减均值，再除以标准差，注意是 hwc <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torchvision.transforms.Normalize(mean, std)</span><br></pre></td></tr></table></figure></p>
<h6 id="转化为tensortransforms.totensor">5.3.3 转化为Tensor:transforms.ToTensor</h6>
<p>将 PIL Image 或者 ndarray 转换为 tensor，并且归一化至[0-1]。注意归一化至[0-1]是直接除以 255，若自己的 ndarray 数据尺度有变化，则需要自行修改。 <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torchvision.transforms.ToTensor()</span><br></pre></td></tr></table></figure> ###### 5.3.4 例子 data_transforms是一个字典，其指定了所有图像训练集和检验集预处理操作 <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#transfomrs.Compose()表示按顺序执行</span></span><br><span class="line">data_transforms = &#123;</span><br><span class="line">    <span class="string">&#x27;train&#x27;</span>: </span><br><span class="line">        transforms.Compose([</span><br><span class="line">        transforms.Resize([<span class="number">96</span>, <span class="number">96</span>]),</span><br><span class="line">        transforms.RandomRotation(<span class="number">45</span>),<span class="comment">#随机旋转，-45到45度之间随机选</span></span><br><span class="line">        transforms.CenterCrop(<span class="number">64</span>),<span class="comment">#从中心开始裁剪</span></span><br><span class="line">        transforms.RandomHorizontalFlip(p=<span class="number">0.5</span>),<span class="comment">#随机水平翻转 选择一个概率概率</span></span><br><span class="line">        transforms.RandomVerticalFlip(p=<span class="number">0.5</span>),<span class="comment">#随机垂直翻转</span></span><br><span class="line">        transforms.ColorJitter(brightness=<span class="number">0.2</span>, contrast=<span class="number">0.1</span>, saturation=<span class="number">0.1</span>, hue=<span class="number">0.1</span>),<span class="comment">#参数1为亮度，参数2为对比度，参数3为饱和度，参数4为色相</span></span><br><span class="line">        transforms.RandomGrayscale(p=<span class="number">0.025</span>),<span class="comment">#概率转换成灰度率，3通道就是R=G=B</span></span><br><span class="line">        transforms.ToTensor(),</span><br><span class="line">        transforms.Normalize([<span class="number">0.485</span>, <span class="number">0.456</span>, <span class="number">0.406</span>], [<span class="number">0.229</span>, <span class="number">0.224</span>, <span class="number">0.225</span>])<span class="comment">#均值，标准差</span></span><br><span class="line">    ]),</span><br><span class="line">    <span class="string">&#x27;valid&#x27;</span>: </span><br><span class="line">        transforms.Compose([</span><br><span class="line">        transforms.Resize([<span class="number">64</span>, <span class="number">64</span>]),</span><br><span class="line">        transforms.ToTensor(),</span><br><span class="line">        transforms.Normalize([<span class="number">0.485</span>, <span class="number">0.456</span>, <span class="number">0.406</span>], [<span class="number">0.229</span>, <span class="number">0.224</span>, <span class="number">0.225</span>])</span><br><span class="line">    ]),</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">#使用datasets.ImageFloder()读取数据</span></span><br><span class="line">image_datasets = &#123;x: datasets.ImageFolder(os.path.join(data_dir, x), data_transforms[x]) <span class="keyword">for</span> x <span class="keyword">in</span> [<span class="string">&#x27;train&#x27;</span>, <span class="string">&#x27;valid&#x27;</span>]&#125;</span><br></pre></td></tr></table></figure></p>
<h4 id="pytorch的模型搭建torch.nn模块">6. pytorch的模型搭建torch.nn模块</h4>
<p>机器学习的五大板块之一就是模型，好的模型事半功倍。</p>
<h5 id="nn.functional-和-nn.module">6.1 nn.functional 和 nn.Module</h5>
<p>Pytorch和神经网络相关的功能组件大多都封装在 torch.nn模块下。 这些功能组件的绝大部分既有函数形式<code>nn.funtional</code>实现，也有类形式<code>nn.Module</code>实现。</p>
<p>其中nn.functional(一般引入后改名为F)有各种功能组件的函数实现,例如: - <strong>激活函数</strong>: <code>F.relu 、F.sigmoid 、F.tanh 、F.softmax</code></p>
<ul>
<li><p><strong>模型层</strong>： <code>F.linear(全连接)、F.conv2d(2d卷积)、F.max_pool2d(2d最大池化)、F.dropout2d、 F.embedding</code></p></li>
<li><p><strong>损失函数</strong>：<code>F.binary_cross_entropy 、F.mse_loss 、F.cross_entropy(交叉熵损失函数)</code></p></li>
</ul>
<p><strong>但为了进一步便于对参数进行管理，一般通过继承 nn.Module 转换成为类的实现形式，并直接封装在 nn 模块下，例如：</strong></p>
<ul>
<li><p><strong>激活函数</strong>： <code>nn.ReLU 、 nn.Sigmoid 、 nn.Tanh 、nn.Softmax</code></p></li>
<li><p><strong>模型层</strong>： <code>nn.Linear 、nn.Conv2d 、nn.MaxPool2d 、nn.Dropout2d 、nn.Embedding</code></p></li>
<li><p><strong>损失函数</strong>：<code>nn.BCELoss 、 nn.MSELoss 、 nn.CrossEntropyLoss</code></p></li>
</ul>
<p>实际上nn.Module除了可以管理其引用的各种参数，还可以管理其引用的子模块，功能十分强大</p>
<h5 id="使用nn.module来管理参数">6.2 使用nn.Module来管理参数</h5>
<p>在Pytorch中，模型的参数是需要被优化器训练的，因此，通常要设置参数为<code>requires_grad = True</code>的张量。 同时，在一个模型中，往往有许多的参数，要手动管理这些参数并不是一件容易的事情。 Pytorch一般将参数用<code>nn.Parameter</code>来表示，并且用<code>nn.Module</code>来管理其结构下的所有参数。 <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#模型，模型自动生成权值矩阵\卷积核</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">CNN</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(CNN, self).__init__()</span><br><span class="line">        self.conv1 = nn.Sequential(         <span class="comment"># 输入大小 (1, 28, 28)</span></span><br><span class="line">            nn.Conv2d(</span><br><span class="line">                in_channels=<span class="number">1</span>,              <span class="comment"># 灰度图</span></span><br><span class="line">                out_channels=<span class="number">16</span>,            <span class="comment"># 要得到几多少个特征图</span></span><br><span class="line">                kernel_size=<span class="number">5</span>,              <span class="comment"># 卷积核大小</span></span><br><span class="line">                stride=<span class="number">1</span>,                   <span class="comment"># 步长</span></span><br><span class="line">                padding=<span class="number">2</span>,                  <span class="comment"># 如果希望卷积后大小跟原来一样，需要设置padding=(kernel_size-1)/2 if stride=1</span></span><br><span class="line">            ),                              <span class="comment"># 输出的特征图为 (16, 28, 28)</span></span><br><span class="line">            nn.ReLU(),                      <span class="comment"># relu层</span></span><br><span class="line">            nn.MaxPool2d(kernel_size=<span class="number">2</span>),    <span class="comment"># 进行池化操作（2x2 区域）, 输出结果为： (16, 14, 14)</span></span><br><span class="line">        )</span><br><span class="line">        self.conv2 = nn.Sequential(         <span class="comment"># 下一个套餐的输入 (16, 14, 14)</span></span><br><span class="line">            nn.Conv2d(<span class="number">16</span>, <span class="number">32</span>, <span class="number">5</span>, <span class="number">1</span>, <span class="number">2</span>),     <span class="comment"># 输出 (32, 14, 14)</span></span><br><span class="line">            nn.ReLU(),                      <span class="comment"># relu层</span></span><br><span class="line">            nn.Conv2d(<span class="number">32</span>, <span class="number">32</span>, <span class="number">5</span>, <span class="number">1</span>, <span class="number">2</span>),</span><br><span class="line">            nn.ReLU(),</span><br><span class="line">            nn.MaxPool2d(<span class="number">2</span>),                <span class="comment"># 输出 (32, 7, 7)</span></span><br><span class="line">        )</span><br><span class="line">        </span><br><span class="line">        self.conv3 = nn.Sequential(         <span class="comment"># 下一个套餐的输入 (16, 14, 14)</span></span><br><span class="line">            nn.Conv2d(<span class="number">32</span>, <span class="number">64</span>, <span class="number">5</span>, <span class="number">1</span>, <span class="number">2</span>),     <span class="comment"># 输出 (32, 14, 14)</span></span><br><span class="line">            nn.ReLU(),             <span class="comment"># 输出 (32, 7, 7)</span></span><br><span class="line">        )</span><br><span class="line">        </span><br><span class="line">        self.out = nn.Linear(<span class="number">64</span> * <span class="number">7</span> * <span class="number">7</span>, <span class="number">10</span>)   <span class="comment"># 全连接层得到的结果</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        x = self.conv1(x)</span><br><span class="line">        x = self.conv2(x)</span><br><span class="line">        x = self.conv3(x)</span><br><span class="line">        x = x.view(x.size(<span class="number">0</span>), -<span class="number">1</span>)           <span class="comment"># flatten操作，结果为：(batch_size, 32 * 7 * 7)</span></span><br><span class="line">        output = self.out(x)</span><br><span class="line">        <span class="keyword">return</span> output</span><br><span class="line"></span><br><span class="line"><span class="comment"># 实例化</span></span><br><span class="line">net = CNN() </span><br><span class="line"><span class="comment">#损失函数</span></span><br><span class="line">criterion = nn.CrossEntropyLoss() </span><br><span class="line"><span class="comment">#优化器，nn.paarameters管理参数</span></span><br><span class="line">optimizer = optim.Adam(net.parameters(), lr=<span class="number">0.001</span>) <span class="comment">#定义优化器，普通的随机梯度下降算法</span></span><br></pre></td></tr></table></figure></p>
<h5 id="使用nn.module来管理子模块">6.2 使用nn.Module来管理子模块</h5>
<p>一般情况下，我们都很少直接使用 nn.Parameter来定义参数构建模型，<strong>而是通过一些拼装一些常用的模型层来构造模型，如上面的<code>CNN</code>类拼接了模型层，自动生成参数值由<code>nn.parameters</code>管理。</strong>这些模型层也是继承自<code>nn.Module</code>的对象,本身也包括参数，<strong>模型层属于我们要定义的模块的子模块。 nn.Module提供了一些方法可以管理这些子模块。</strong></p>
<ul>
<li><p><code>children()</code>: 返回生成器，包括模块下的所有子模块。</p></li>
<li><p><code>named_children()</code>：返回一个生成器，只有模块下的所有子模块，以及它们的名字。</p></li>
<li><p><code>modules()</code>：返回一个生成器，不仅返回模块下的子模块，连子模块下的子模块也会被返回，包括模块本身。</p></li>
<li><p><code>named_modules()</code>：返回一个生成器，包括模块下的所有各个层级的模块以及它们的名字，包括模块本身。</p></li>
</ul>
<p>上面函数返回一个可迭代的生成器，可通过for循环查看返回值。见<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/349156416">PyTorch中的modules()和children()相关函数简析</a></p>
<h5 id="模型层">6.3 模型层</h5>
<p>深度学习模型一般由各种模型层组合而成，<code>torch.nn</code>中内置了非常丰富的各种模型层。它们都属于<code>nn.Module</code>的子类，具备参数管理功能,如上面提到的<code>nn.Linear、 nn.Flatten、 nn.Dropout, nn.BatchNorm2d、nn.Conv2d、nn.AvgPool2d、nn.Conv1d、nn.ConvTranspose2d、nn.Embedding、nn.GRU、nn.LSTM、nn.Transformer</code>，下面一小结我们将列举pytorch内置的模型层</p>
<p><strong><em>如果这些内置模型层不能够满足需求，我们也可以通过继承nn.Module基类构建自定义的模型层。实际上，pytorch不区分模型和模型层，都是通过继承nn.Module进行构建。因此，我们只要继承nn.Module基类并实现forward方法即可自定义模型层。</em></strong></p>
<h5 id="内置模型层">6.4 内置模型层</h5>
<p>这里只对相应的模型层做一个简介，详细的API文档查阅<a target="_blank" rel="noopener" href="https://pytorch.org/vision/stable/models.html">官方文档</a> ###### 6.4.1 基础层 - <code>nn.Linear</code>：全连接层。参数个数 = 输入层特征数× 输出层特征数(weight)＋ 输出层特征数 (bias)</p>
<ul>
<li><p><code>nn.Flatten</code>：压平层，用于将多维张量样本压成一维张量样本。</p></li>
<li><p><code>nn.BatchNorm1d</code>：一维批标准化层。通过线性变换将输入批次缩放平移到稳定的均值和标准差。可以增强模型对输入不同分布的适应性，加快模型训练速度，有轻微正则化效果。一般在激活函数之前使用。可以用afine参数设置该层是否含有可以训练的参数。</p></li>
<li><p><code>nn.BatchNorm2d</code>：二维批标准化层。</p></li>
<li><p><code>nn.BatchNorm3d</code>：三维批标准化层。</p></li>
<li><p><code>nn.Dropout</code>：一维随机丢弃层。一种正则化手段。</p></li>
<li><p><code>nn.Dropout2d</code>：二维随机丢弃层。</p></li>
<li><p><code>nn.Dropout3d</code>：三维随机丢弃层。</p></li>
<li><p><code>nn.Threshold</code>：限幅层。当输入大于或小于阈值范围时，截断之。</p></li>
<li><p><code>nn.ConstantPad2d</code>： 二维常数填充层。对二维张量样本填充常数扩展长度。</p></li>
<li><p><code>nn.ReplicationPad1d</code>： 一维复制填充层。对一维张量样本通过复制边缘值填充扩展长度。</p></li>
<li><p><code>nn.ZeroPad2d</code>：二维零值填充层。对二维张量样本在边缘填充0值.</p></li>
<li><p><code>nn.GroupNorm</code>：组归一化。一种替代批归一化的方法，将通道分成若干组进行归一。不受 batch大小限制，据称性能和效果都优于BatchNorm。</p></li>
<li><p><code>nn.LayerNorm</code>：层归一化。较少使用。</p></li>
<li><p><code>nn.InstanceNorm2d</code>: 样本归一化。较少使用。</p></li>
</ul>
<h6 id="卷积网络相关层">6.4.2 卷积网络相关层</h6>
<ul>
<li><p><code>nn.Conv1d</code>：普通一维卷积，常用于文本。<code>参数个数 = 输入通道数×卷积核尺寸(如3)×卷积核个数 + 卷积核个数</code></p></li>
<li><p><code>nn.Conv2d</code>：普通二维卷积，常用于图像。<code>参数个数 = 输入通道数×卷积核尺寸(如3乘3)×卷积核个数 + 卷积核个数</code>。通过调整<code>dilation</code>参数大于1，可以变成空洞卷积，增大卷积核感受野。 通过调整<code>groups</code>参数不为1，可以变成分组卷积。分组卷积中不同分组使用相同的卷积核，显著减少参数数量。 当<code>groups</code>参数等于通道数时，相当于tensorflow中的二维深度卷积层<code>tf.keras.layers.DepthwiseConv2D</code>。 利用分组卷积和1乘1卷积的组合操作，可以构造相当于<code>Keras</code>中的二维深度可分离卷积层<code>tf.keras.layers.SeparableConv2D</code>。</p></li>
<li><p><code>nn.Conv3d</code>：普通三维卷积，常用于视频。<code>参数个数 = 输入通道数×卷积核尺寸(如3乘3乘3)× 卷积核个数 + 卷积核个数</code> 。</p></li>
<li><p><code>nn.MaxPool1d</code>: 一维最大池化。</p></li>
<li><p><code>nn.MaxPool2d</code>：二维最大池化。一种下采样方式。没有需要训练的参数。</p></li>
<li><p><code>nn.MaxPool3d</code>：三维最大池化。</p></li>
<li><p><code>nn.AdaptiveMaxPool2d</code>：二维自适应最大池化。无论输入图像的尺寸如何变化，输出的图像尺寸是固定的。 该函数的实现原理，大概是通过输入图像的尺寸和要得到的输出图像的 尺寸来反向推算池化算子的<code>padding,stride</code>等参数。</p></li>
<li><p>nn.FractionalMaxPool2d：二维分数最大池化。普通最大池化通常输入尺寸是输出的整数 倍。而分数最大池化则可以不必是整数。分数最大池化使用了一些随机采样策略，有一定的 正则效果，可以用它来代替普通最大池化和Dropout层。</p></li>
<li><p><code>nn.AvgPool2d</code>：二维平均池化。</p></li>
<li><p><code>nn.AdaptiveAvgPool2d</code>：二维自适应平均池化。无论输入的维度如何变化，输出的维度是固定的。</p></li>
<li><p><code>nn.ConvTranspose2d</code>：二维卷积转置层，俗称反卷积层。并非卷积的逆操作，但在卷积核相同的情况下，当其输入尺寸是卷积操作输出尺寸的情况下，卷积转置的输出尺寸恰好是卷积操作的输入尺寸。在语义分割中可用于上采样。</p></li>
<li><p><code>nn.Upsample</code>：上采样层，操作效果和池化相反。可以通过mode参数控制上采样策略 为”nearest”最邻近策略或”linear”线性插值策略。</p></li>
<li><p><code>nn.Unfold</code>：滑动窗口提取层。其参数和卷积操作<code>nn.Conv2d</code>相同。实际上，卷积操作可以等价于<code>nn.Unfold</code>和<code>nn.Linear</code>以及<code>nn.Fold</code>的一个组合。 其中<code>nn.Unfold</code>操作可以从输入中提取各个滑动窗口的数值矩阵，并将其压平成一维。利用<code>nn.Linear</code>将<code>nn.Unfold</code>的输出和卷积核做乘法后，再使用 <code>nn.Fold</code>操作将结果转换成输出图片形状。</p></li>
<li><p><code>nn.Fold</code>：逆滑动窗口提取层。</p></li>
</ul>
<p><strong><em>注意</em></strong>：</p>
<ul>
<li>卷积核个数==输出的feature map（activation map）个数，比如输入是一个<span class="math inline">\(32x32x3\)</span>的图像，<span class="math inline">\(3\)</span>表示RGB三通道，每个<code>filter/kernel</code>是<span class="math inline">\(5x5x3\)</span>，一个卷积核产生一个<span class="math inline">\(feature map\)</span>，下图中，有<span class="math inline">\(6\)</span>个 <span class="math inline">\(5x5x3\)</span>的卷积核，故输出<span class="math inline">\(6\)</span>个<code>feature map（activation map）</code>，大小为<span class="math inline">\(28x28x6\)</span>。 <img src="/2022/10/06/pytorch%E5%9F%BA%E7%A1%80/kernel.png" width="600"></li>
<li>参数个数为：卷积核个数*(卷积核大小)+卷积核个数(偏置数)</li>
<li>偏置数==卷积核个数</li>
</ul>
<h6 id="循环网络相关层">6.4.3 循环网络相关层</h6>
<ul>
<li><p><code>nn.Embedding</code>：嵌入层。一种比<code>Onehot</code>更加有效的对离散特征进行编码的方法。一般用于将输入中的单词映射为稠密向量。嵌入层的参数需要学习。</p></li>
<li><p><code>nn.LSTM</code>：长短记忆循环网络层【支持多层】。最普遍使用的循环网络层。具有携带轨道，遗忘门，更新门，输出门。可以较为有效地缓解梯度消失问题，从而能够适用长期依赖问题。设置<code>bidirectional = True</code>时可以得到双向<code>LSTM</code>。需要注意的时，默认的输入和输出形状是<code>(seq,batch,feature)</code>, 如果需要将<code>batch</code>维度放在第0维，则要设置<code>batch_first</code>参数设置为<code>True</code>。</p></li>
<li><p><code>nn.GRU</code>：门控循环网络层【支持多层】。LSTM的低配版，不具有携带轨道，参数数量少于 LSTM，训练速度更快。</p></li>
<li><p><code>nn.RNN</code>：简单循环网络层【支持多层】。容易存在梯度消失，不能够适用长期依赖问题。 一般较少使用。</p></li>
<li><p><code>nn.LSTMCell</code>：长短记忆循环网络单元。和<code>nn.LSTM</code>在整个序列上迭代相比，它仅在序列上迭代一步。一般较少使用。</p></li>
<li><p><code>nn.GRUCell</code>：门控循环网络单元。和<code>nn.GRU</code>在整个序列上迭代相比，它仅在序列上迭代一步。一般较少使用。</p></li>
<li><p><code>nn.RNNCell</code>：简单循环网络单元。和<code>nn.RNN</code>在整个序列上迭代相比，它仅在序列上迭代一步。一般较少使用。</p></li>
</ul>
<h6 id="transformer相关层">6.4.4 Transformer相关层</h6>
<p><code>Transformer</code>网络结构是替代循环网络的一种结构，解决了循环网络难以并行，难以捕捉长期依赖的缺陷。它是目前NLP任务的主流模型的主要构成部分。<code>Transformer</code>网络结构由<code>TransformerEncoder</code>编码器和<code>TransformerDecoder</code>解码器组成。编码器和解码器的核心是<code>MultiheadAttention</code>多头注意力层。</p>
<ul>
<li><p><code>nn.TransformerEncoder</code>：<code>Transformer</code>编码器结构。由多个<code>nn.TransformerEncoderLayer</code>编 码器层组成。</p></li>
<li><p><code>nn.TransformerDecoder</code>：<code>Transformer</code>解码器结构。由多个<code>nn.TransformerDecoderLayer</code>解码器层组成。</p></li>
<li><p><code>nn.TransformerEncoderLayer</code>：<code>Transformer</code>的编码器层。</p></li>
<li><p><code>nn.TransformerDecoderLayer</code>：<code>Transformer</code>的解码器层。</p></li>
<li><p><code>nn.MultiheadAttention</code>：多头注意力层。</p></li>
</ul>
<h6 id="自定义模型层">6.4.5 自定义模型层</h6>
<p>如果Pytorch的内置模型层不能够满足需求，<strong>我们也可以通过继承<code>nn.Module</code>基类构建自定义的模型层。实际上，pytorch不区分模型和模型层，都是通过继承<code>nn.Module</code>进行构建。 因此，我们只要继承nn.Module基类并实现forward方法即可自定义模型层</strong>。 下面是Pytorch的nn.Linear层的源码，我们可以仿照它来自定义模型层</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/2022/09/28/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="trluper">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Trluper">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/09/28/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" class="post-title-link" itemprop="url">深度学习</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2022-09-28 20:22:08" itemprop="dateCreated datePublished" datetime="2022-09-28T20:22:08+08:00">2022-09-28</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2022-10-06 10:36:35" itemprop="dateModified" datetime="2022-10-06T10:36:35+08:00">2022-10-06</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95/" itemprop="url" rel="index"><span itemprop="name">深度学习算法</span></a>
                </span>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine：</span>
    
    <a title="valine" href="/2022/09/28/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2022/09/28/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" itemprop="commentCount"></span>
    </a>
  </span>
  
  <br>
            <span class="post-meta-item" title="本文字数">
              <span class="post-meta-item-icon">
                <i class="far fa-file-word"></i>
              </span>
                <span class="post-meta-item-text">本文字数：</span>
              <span>8.3k</span>
            </span>
            <span class="post-meta-item" title="阅读时长">
              <span class="post-meta-item-icon">
                <i class="far fa-clock"></i>
              </span>
                <span class="post-meta-item-text">阅读时长 &asymp;</span>
              <span>8 分钟</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h4 id="深度学习简述">1.深度学习简述</h4>
<p>为了学习一种好的表示，需要构建具有一定“深度”的模型，并通过学习算法来让模型自动学习出好的特征表示（从底层特征，到中层特征，再到高层特征），从而最终提升预测模型的准确率．<strong>所谓“深度”是指原始数据进行非线性特征转换的次数</strong> <img src="/2022/09/28/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/DL.png" width="500"></p>
<p>深度学习是将原始的数据特征通过多步的特征转换得到一种特征表示，并进一步输入到预测函数得到最终结果．和“浅层学习”不同，<strong>深度学习需要解决的关键问题是贡献度分配问题（Credit Assignment Problem，CAP）[Minsky,1961]</strong>，即一个系统中不同的组件（component）或其参数对最终系统输出结果的贡献或影响.从某种意义上讲，深度学习可以看作一种强化学习（Reinforcement Learning，RL），每个内部组件并不能直接得到监督信息，需要通过整个模型的最终监督信息（奖励）得到，并且有一定的延时性</p>
<p><strong>深度学习采用的模型主要是神经网络模型，其主要原因是神经网络模型可以使用误差反向传播算法，从而可以比较好地解决贡献度分配问题</strong>．</p>
<h5 id="表示学习">1.1 表示学习</h5>
<p>为了提高机器学习系统的准确率，我们就需要将输入信息转换为有效的特 征，或者更一般性地称为表示（Representation）。如果有一种算法可以自动地学习出有效的特征，并提高最终机器学习模型的性能，那么这种学习就可以叫作<strong>表示学习</strong>。</p>
<p>表示学习的关键是解决<strong>语义鸿沟（Semantic Gap）问题</strong>．语义鸿沟问题是指输入数据的底层特征和高层语义信息之间的不一致性和差异性．比如给定一些关于“车”的图片，由于图片中每辆车的颜色和形状等属性都不尽相同，因此不同图片在像素级别上的表示（即底层特征）差异性也会非常大．但是我们理解这些图片是建立在比较抽象的高层语义概念上的．如果一个预测模型直接建立在底层特征之上，会导致对预测模型的能力要求过高．如果可以有一个好的表示在某种程度上能够反映出数据的高层语义特征，那么我们就能相对容易地构建后续的机器学习模型</p>
<h6 id="局部和分布式表示">4.1.1 局部和分布式表示</h6>
<p>在机器学习中，我们经常使用两种方式来表示特征：<strong>局部表示（Local Representation）和分布式表示（Distributed Representation）．</strong></p>
<ul>
<li><strong>局部表示</strong>：离散表示，one-Hot向量，单一值表示一个东西。
<ul>
<li><strong>优点：</strong>
<ul>
<li>这种离散的表示方式具有很好的解释性，有利于人工归纳和总结特征，并通过特征组合进行高效的特征工程</li>
<li>通过多种特征组合得到的表示向量通常是稀疏的二值向量，当用于线性模型时计算效率非常高</li>
</ul></li>
<li><strong>缺点</strong>：
<ul>
<li>one-hot向量的维数很高，且不能扩展．如果有一种新的颜色，我们就需要增加一维来表示</li>
<li>不同颜色之间的相似度都为0，即我们无法知道“红色”和“中国红”的相似度要高于“红色”和“黑色”的相似度</li>
</ul></li>
</ul></li>
<li><strong>分布式表示</strong>：压缩、低维的稠密向量，使用多个值表示一个东西，如表示颜色的方法是用RGB值来表示颜色，不同颜色对应到R、G、B三维空间中一个点，这种表示方式叫作分布式表示
<ul>
<li>分布式表示的表示能力要强很多，分布式表示的向量维度一般都比较低．我们只需要用一个三维的稠密向量就可以表示所有颜色．并且，分布式表示也很容易表示新的颜色名．此外，不同颜色之间的相似度也很容易计算</li>
</ul></li>
</ul>
<h5 id="端到端学习">1.2 端到端学习</h5>
<p><strong>端到端学习（End-to-End Learning），也称端到端训练，是指在学习过程中不进行分模块或分阶段训练，直接优化任务的总体目标</strong>．在端到端学习中，一般不需要明确地给出不同模块或阶段的功能，中间过程不需要人为干预．端到端学习的训练数据为“输入-输出”对的形式，无须提供其他额外信息．因此，端到端学习和深度学习一样，都是要解决贡献度分配问题．目前，大部分采用神经网络模型的深度学习也可以看作一种端到端的学习</p>
<h4 id="前馈神经网络">2. 前馈神经网络</h4>
<h5 id="神经元">2.1 神经元</h5>
<p>神经网络一般可以看作一个非线性模型，其基本组成单元为具有非线性激活函数的神经元，通过大量神经元之间的连接，使得神经网络成为一种高度非线性的模型．<strong>神经元之间的连接权重就是需要学习的参数，可以在机器学习的框架下通过梯度下降方法来进行学习</strong>． <img src="/2022/09/28/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/Neuron.png" width="300"></p>
<p>激活函数在神经元中非常重要的．为了增强网络的表示能力和学习能力，<strong>激活函数</strong>需要具备以下几点性质：</p>
<ul>
<li>（1） 连续并可导（允许少数点上不可导）的非线性函数．可导的激活函数可以直接利用数值优化的方法来学习网络参数．</li>
<li>（2） 激活函数及其导函数要尽可能的简单，有利于提高网络计算效率．</li>
<li>（3） 激活函数的导函数的值域要在一个合适的区间内，不能太大也不能太小，否则会影响训练的效率和稳定性．</li>
</ul>
<p>下面介绍几种在神经网络中常用的激活函数．</p>
<h6 id="sigmoid函数">2.1.1 sigmoid函数</h6>
<p><code>Sigmoid</code>型函数是指一类 S 型曲线函数，为两端饱和函数．常用的<code>Sigmoid</code>型函数<strong>有<code>Logistic</code>函数和<code>Tanh</code>函数</strong>．<strong>所谓饱和是指</strong>对于函数 <span class="math inline">\(f(x)\)</span>，若 <span class="math inline">\(x → −∞\)</span> 时，其导数 <span class="math inline">\(f^′(x) → 0\)</span>，则称其为左饱和．若<span class="math inline">\(x → +∞\)</span>时，其导数<span class="math inline">\(f′(x) → 0\)</span>，则称其为右饱和．当同时满足左、右饱和时，就称为两端饱和。</p>
<p>在机器学习篇章我们以及介绍果LR函数： <span class="math display">\[
σ(x)={1\over 1+exp(-x)}
\]</span></p>
<p><strong>Logistic 函数可以看成是一个“挤压”函数，把一个实数域的输入“挤压”到(0, 1)</strong>．当输入值在0附近时，Sigmoid型函数近似为线性函数；当输入值靠近两端时，对输入进行抑制．输入越小，越接近于 0；输入越大，越接近于 1．和感知器使用的阶跃激活函数相比，Logistic函数是连续可导的，其数学性质更好．</p>
<p>因为Logistic函数的性质，使得装备了Logistic激活函数的神经元具有以下两点性质： - 1）其输出直接可以看作概率分布，使得神经网络可以更好地和统计学习模型进行结合． - 2）其可以看作一个软性门（Soft Gate），用来控制其他神经元输出信息的数量．</p>
<p><img src="/2022/09/28/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/sigmoid.png" width="300"></p>
<p><strong><code>Tanh</code>函数</strong>： <span class="math display">\[
tanh(x)={exp(x)-exp(-x)\over exp(x)+exp(-x)}
\]</span></p>
<h6 id="relu函数">2.1.2 Relu函数</h6>
<p>ReLU（Rectified Linear Unit，修正线性单元）是目前深度神经网络中经常使用的激活函数．<strong>ReLU实际上是一个斜坡（ramp）函数</strong>，定义为： <span class="math display">\[
Relu(x)=\begin{cases}
x&amp;x≥0\\
0&amp;x&lt;0
\end{cases}
\]</span></p>
<p><strong>优点：</strong></p>
<ul>
<li>采用 ReLU 的神经元只需要进行加、乘和比较的操作，计算上更加高效</li>
<li>具有生物学合理性，比如单侧抑制、宽兴奋边界。Sigmoid 型激活函数会导致一个非稀疏的神经网络，而 ReLU 却具有很好的稀疏性，大约50%的神经元会处于激活状态</li>
<li>在优化方面，相比于Sigmoid型函数的两端饱和，ReLU函数为左饱和函数，且在 $x &gt; 0 $时导数为 1，在一定程度上缓解了神经网络的梯度消失问题，加速梯度下降的收敛速度．</li>
</ul>
<p><strong>缺点</strong>：</p>
<ul>
<li>ReLU 函数的输出是非零中心化的，给后一层的神经网络引入偏置偏移，会影响梯度下降的效率</li>
<li>ReLU 神经元在训练时比较容易“死亡”．在训练时，如果参数在一次不恰当的更新后，第一个隐藏层中的某个 ReLU 神经元在所有的训练数据上都不能被激活，那么这个神经元自身参数的梯度永远都会是0，在以后的训练过程中永远不能被激活．这种现象称为死亡 ReLU 问题</li>
</ul>
<p>为克服上述缺陷，几种变种Relu被广泛使用：</p>
<ul>
<li><p><strong>带泄露的Relu</strong>:带泄露的ReLU（Leaky ReLU）在输入 <span class="math inline">\(x&lt; 0\)</span>时，保持一个很小的梯度𝛾．这样当神经元非激活时也能有一个非零的梯度可以更新参数，避免永远不能被激活[Maas et al., 2013]．带泄露的ReLU的定义如下： <span class="math display">\[
LeakyRelu(x)=\begin{cases}
x&amp;x&gt;0\\
γx&amp;x≤0
\end{cases}
\]</span> 其中<span class="math inline">\(γ\)</span>是一个很小的常数，如0.001</p></li>
<li><p><strong>带参数的ReLU：</strong>带参数的 ReLU（Parametric ReLU，PReLU）引入一个可学习的参数，不同神经元可以有不同的参数 [He et al., 2015]．对于第<span class="math inline">\(i\)</span>个神经元，其 PReLU的定义为 <span class="math display">\[
PRelu(x)=\begin{cases}
x&amp;x&gt;0\\
γ_ix&amp;x≤0
\end{cases}
\]</span> 其中<span class="math inline">\(γ_i\)</span>为 <span class="math inline">\(x≤ 0\)</span> 时函数的斜率．因此，PReLU 是非饱和函数．如果<span class="math inline">\(γ_i=0\)</span>，那么PReLU就退化为ReLU．如果<span class="math inline">\(γ_i\)</span>为一个很小的常数，则PReLU可以看作带泄露的ReLU．PReLU可以允许不同神经元具有不同的参数</p></li>
<li><p><strong>ELU（Exponential Linear Unit，指数线性单元）</strong>：[Clevert et al., 2015] 是一个近似的零中心化的非线性函数，其定义为: <span class="math display">\[
Elu(x)=\begin{cases}
x&amp;x&gt;0\\
γ(exp(x)-1)&amp;x≤0
\end{cases}
\]</span> 其中<span class="math inline">\(γ ≥ 0\)</span>是一个超参数，决定<span class="math inline">\(x≤ 0\)</span>时的饱和曲线，并调整输出均值在0附近</p></li>
<li><p><strong>Softplus函数：</strong>Softplus 函数[Dugas et al., 2001] 可以看作 Rectifier 函数的平滑版本，其定义为 <span class="math display">\[
softplus(x)=log(1+exp(x))
\]</span> Softplus函数其导数刚好是Logistic函数．Softplus函数虽然也具有单侧抑制、宽兴奋边界的特性，却没有稀疏激活性 <img src="/2022/09/28/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/Relu.png" width="400"></p></li>
</ul>
<h5 id="swish函数">2.1.3 Swish函数</h5>
<p>Swish 函数[Ramachandran et al., 2017] 是一种<strong>自门控（Self-Gated）激活函数</strong>，定义为: <span class="math display">\[
swish(x)=xσ(βx)
\]</span> 其中<strong><span class="math inline">\(δ(⋅)\)</span>为 Logistic 函数</strong>，<span class="math inline">\(β\)</span>为可学习的参数或一个固定超参数.<span class="math inline">\(δ(⋅) ∈ (0, 1)\)</span>可以看作一种软性的门控机制．当<span class="math inline">\(δ(βx)\)</span>接近于1时，门处于“开”状态，激活函数的输出近似于<span class="math inline">\(x\)</span>本身；当<span class="math inline">\(δ(βx)\)</span>)接近于0时，门的状态为“关”，激活函数的输出近似于0． <img src="/2022/09/28/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/swish.png" width="400"></p>
<p>Swish函数可以看作线性函数和ReLU函数之间的非线性插值函数，其程度由参数<span class="math inline">\(β\)</span>控制．</p>
<h6 id="gelu函数">2.1.4 GELU函数</h6>
<p>GELU（Gaussian Error Linear Unit，高斯误差线性单元）[Hendrycks et al.,2016] 也是一种通过<strong>门控机制来调整其输出值的激活函数</strong>，和 Swish 函数比较类似 <span class="math display">\[
GELU(x)=xp(X≤x)
\]</span> 其中<span class="math inline">\(p(X ≤ x)\)</span>是高斯分布<span class="math inline">\(N(μ, σ^2)\)</span>的累积分布函数，其中<span class="math inline">\(μ, σ\)</span>为超参数，一般设<span class="math inline">\(μ= 0, σ= 1\)</span>即可．由于高斯分布的累积分布函数为S型函数，因此GELU函数可以用Tanh函数或Logistic函数来近似， <span class="math display">\[
GELU(x)=xσ(1.702x)
\]</span></p>
<h6 id="maxout单元">2.1.5 Maxout单元</h6>
<p>Maxout 单元[Goodfellow et al., 2013] 也是一种<strong>分段线性函数函数</strong>。不同的是Sigmoid 、ReLU 等激活函数的输入是神经元的净输入<span class="math inline">\(z\)</span>，是一个标量而 Maxout 单元的输入是上一层神经元的全部原始输出，是一个向量<span class="math inline">\(x = [x_1; x_2; ⋯ ; x_D]\)</span>．</p>
<p>每个Maxout单元有<span class="math inline">\(K\)</span>个权重向量$w_k ∈ ℝ^D <span class="math inline">\(和偏置\)</span>b_k(1 ≤ k ≤ K)<span class="math inline">\(．对于输入\)</span>x<span class="math inline">\(，可以得到\)</span>K<span class="math inline">\(个净输入\)</span>z_k, 1 ≤ k ≤ K$ <span class="math display">\[
z_k=w^T_k+b_k
\]</span> 其中<span class="math inline">\(w_k= [w_{k,1}, ⋯ , w_{k,D}]^T\)</span> 为第<span class="math inline">\(k\)</span>个权重向量</p>
<h5 id="网络结构">2.2 网络结构</h5>
<p>样通过一定的连接方式或信息传递方式进行协作的神经元可以看作一个网络，就是神经网络．．目前常用的神经网络结构有以下三种：</p>
<ul>
<li><p><strong>前馈网络</strong>：前馈网络中各个神经元按接收信息的先后分为不同的组．<strong>每一组可以看作一个神经层．每一层中的神经元接收前一层神经元的输出，并输出到下一层神经元．整个网络中的信息是朝一个方向传播</strong>，没有反向的信息传播，可以用一个有向无环路图表示。<strong>前馈网络包括全连接前馈网络和卷积神经网络等</strong>．</p></li>
<li><p><strong>记忆网络</strong>：记忆网络，也称为反馈网络，<strong>网络中的神经元不但可以接收其他神经元的信息，也可以接收自己的历史信息．和前馈网络相比，记忆网络中的神经元具有记忆功能，在不同的时刻具有不同的状态．记忆神经网络中的信息传播可以是单向或双向传递</strong>。记忆网络包括循环神经网络、Hopfield 网络、玻尔兹曼机、受限玻尔兹曼机等</p></li>
<li><p><strong>图网络</strong>：<strong>图网络是定义在图结构数据上的神经网络．图中每个节点都由一个或一组神经元构成．节点之间的连接可以是有向的，也可以是无向的．每个节点可以收到来自相邻节点或自身的信息</strong>。图网络是前馈网络和记忆网络的泛化，包含很多不同的实现方式，比如<em>图卷积网络（Graph Convolutional Network，<strong>GCN</strong>）[Kipf et al., 2016]、图注意力网络（Graph Attention Network，<strong>GAT</strong>）[Veličković et al., 2017]、消息传递神经网络（Message Passing Neural Network，<strong>MPNN</strong>）[Gilmer et al., 2017]等</em>． <img src="/2022/09/28/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/neuralStructure.png" width="500"></p></li>
</ul>
<h5 id="反向传播算法">2.3 反向传播算法</h5>
<p>假设采用随机梯度下降进行神经网络参数学习，给定一个样本<span class="math inline">\((x, y)\)</span>，将其输入到神经网络模型中，得到网络输出为<span class="math inline">\(\tilde{𝒚}\)</span>．假设损失函数为$ ℒ(𝒚,)$，<strong>要进行参数学习就需要计算损失函数关于每个参数的导数</strong>．</p>
<p>不失一般性，对第<span class="math inline">\(l\)</span>层中的参数<span class="math inline">\(W^{(l)}\)</span> 和<span class="math inline">\(b{(l)}\)</span>计算偏导数．因为<span class="math inline">\(𝜕ℒ(𝒚,\tilde{y})\over𝜕𝑾^{(𝑙)}\)</span>的计算涉及向量对矩阵的微分，十分繁琐，因此我们先计算<span class="math inline">\(ℒ(𝒚,\tilde{𝒚})\)</span> 关于参数矩阵中每个元素的偏导数$ ℒ(y,)w^{(l)}_{ij}$．根据链式法则: <span class="math display">\[
{δℒ(𝒚,\tilde{y}) \over δw^{(l)}_{ij}}={δz^{(l)}\over δw^{(l)}_{ij} }{δℒ(𝒚,\tilde{y})\over δz^{(l)}} \\
{δℒ(𝒚,\tilde{y}) \over δb^{(l)}}={δz^{(l)}\over δb^{(l)}}{δℒ(𝒚,\tilde{y})\over δz^{(l)}}
\]</span> 上式的第二项都是目标函数关于第<span class="math inline">\(l\)</span>层的神经元<span class="math inline">\(z^{(l)}\)</span>的偏导数，称为误差项，可以一次计算得到．这样我们只需要计算三个偏导数，分别为<span class="math inline">\({δz^{(l)}\over δw^{(l)}_{ij}}、{δz^{(l)}\over δb^{(l)}}、{δℒ(𝒚,\tilde{y})\over δz^{(l)}}\)</span></p>
<p>下面分别计算它们：</p>
<ul>
<li><span class="math inline">\({δz^{(l)}\over δw^{(l)}_{ij}}\)</span>：因为<span class="math inline">\(z=Wx+b\)</span>，则其导数为可以很方便计算得到</li>
<li><span class="math inline">\({δℒ(𝒚,\tilde{y})\over δz^{(l)}}\)</span>：<strong>该偏导数表示第<span class="math inline">\(l\)</span>层神经元对最终损失的影响，也反映了最终损失对第<span class="math inline">\(l\)</span>层神经元的敏感度</strong>，因此一般称为误差项<span class="math inline">\(δ^{(l)}\)</span>，<strong>它也间接反映了神经元对网络的共享程度</strong>，是解决贡献的分配问题的一个指标。详细公式见书p94</li>
</ul>
<h5 id="梯度消失">2.4 梯度消失</h5>
<p>上面可以知道在神经网络中误差反向传播的迭代公式为 <span class="math display">\[
δ^{(l)}=f^{&#39;}_l(z^{(l)})⊙(W^{(l+1)})^Tδ^{(l+1)}
\]</span> 误差从输出层反向传播时，在每一层都要乘以该层的激活函数的导数，当采取Sigmoid型激活函数时，由于其导数<code>&lt;1</code>,经过深度学习，一层一层下去，其误差项不断趋近于0，梯度就会不断衰减,甚至消失，导致网络难以训练.这就是<strong>梯度消失</strong></p>
<p>在深度神经网络中，减轻梯度消失问题的方法有很多种．一种简单有效的方式是使用导数比较大的激活函数，比如ReLU等．</p>
<h4 id="卷积神经网络">3 卷积神经网络</h4>
<p>卷积神经网络（Convolutional Neural Network，CNN或ConvNet）是一种具有<strong>局部连接、权重共享</strong>等特性的深层前馈神经网络．卷积神经网络有个重要的专有词叫<strong>感受野</strong>，，即神经元只接受其所支配的刺激区域内的信号。卷积神经网络的出现解决了全连接前馈网络的两个问题<strong>参数太多</strong>和<strong>局部不变性特征</strong>。</p>
<p><strong>目前的卷积神经网络一般是由卷积层、汇聚层和全连接层交叉堆叠而成的前馈神经网络．卷积神经网络有三个结构上的特性：局部连接、权重共享以及汇聚</strong>．这些特性使得卷积神经网络具有一定程度上的平移、缩放和旋转不变性．和前馈神经网络相比，<strong>卷积神经网络的参数更少</strong>．</p>
<h5 id="卷积和卷积核">3.1 卷积和卷积核</h5>
<p>卷积（Convolution），也叫褶积，是分析数学中一种重要的运算．在信号处理或图像处理中，经常使用一维或二维卷积．</p>
<p>如二维卷积<span class="math inline">\(Y=W*X\)</span>为卷积操作，其中<span class="math inline">\(W\)</span>称为卷积核，<span class="math inline">\(X\)</span>为特征矩阵。 <img src="/2022/09/28/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/Convolution.png" width="400"></p>
<h5 id="互相关">3.2 互相关</h5>
<p>在机器学习和图像处理领域，卷积的主要功能是在一个图像（或某种特征）上滑动一个卷积核（即滤波器），通过卷积操作得到一组新的特征．在计算卷积的过程中，需要进行卷积核翻转。而在计算<strong>自相关</strong>时,则不需要翻转，所谓的翻转是对<strong>卷积核</strong>进行从上到下，从左到右的的颠倒。<strong>因此互相关和卷积的区别仅仅在于卷积核是否进行翻转</strong></p>
<p>在深度学习中我们一般使用自相关，它也称为<strong>不翻转卷积</strong>。</p>
<p><em>注：卷积核是否进行翻转和其特征抽取的能力无关．特别是当卷积核是可学习的参数时，卷积和互相关在能力上是等价的．因此，为了实现上（或描述上）的方便起见，我们用互相关来代替卷积</em></p>
<h5 id="卷积神经网络-1">3.3 卷积神经网络</h5>
<p>卷积神经网络一般由卷积层、汇聚层和全连接层构成．</p>
<h6 id="卷积层">3.3.1 卷积层</h6>
<p>卷积层的作用是提取一个局部区域的特征，<strong>不同的卷积核相当于不同的特征提取器</strong>。而图像为二维结构，因此为了更充分地利用图像的局部信息，通常将神经元组织为三维结构的神经层，其大小为高度M×宽度N×深度D，由<span class="math inline">\(D\)</span>个<span class="math inline">\(M × N\)</span>大小的特征映射构成。<strong>特征映射（Feature Map）为一幅图像（或其他特征映射）在经过卷积提取到的特征</strong>，每个特征映射可以作为一类抽取的图像特征．为了提高卷积网络的表示能力，可以在每一层使用多个不同的特征映射，以更好地表示图像的特征． <img src="/2022/09/28/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/featureMap.png" width="400"> <strong>在输入层，特征映射就是图像本身．如果是灰度图像，就是有一个特征映射，输入层的深度 <span class="math inline">\(D = 1\)</span>；如果是彩色图像，分别有 RGB 三个颜色通道的特征映射，输入层的深度<span class="math inline">\(D = 3\)</span>．</strong></p>
<p>假设现在有一个卷积层的结构如下：</p>
<ul>
<li><p>输入特征映射组：<span class="math inline">\(X∈ ℝ^{M×N×D}\)</span>为三维张量（Tensor），其中每个切片（Slice）矩阵<span class="math inline">\(X^d ∈ ℝ^{M×N }\)</span>为一个输入特征映射，<span class="math inline">\(1 ≤ d ≤D\)</span>；</p></li>
<li><p>输出特征映射组：<span class="math inline">\(y∈ ℝ^{M×N×P}\)</span>为三维张量，其中每个切片矩阵<span class="math inline">\(Y^p ∈ ℝ^{M×N}\)</span>为一个输出特征映射，<span class="math inline">\(1 ≤ p ≤ P\)</span>；</p></li>
<li><p>卷积核：<span class="math inline">\(W∈ ℝ^{M×N×P×D}\)</span>为四维张量，其中每个切片矩阵<span class="math inline">\(W^{pd}∈ ℝ^{U×V}\)</span> 为一个二维卷积核，<span class="math inline">\(1 ≤ p ≤P , 1 ≤ d ≤ D\)</span>．</p></li>
</ul>
<p>因此参数个数为：每一个输出特征映射都需要<span class="math inline">\(D\)</span>个卷积核以及一个偏置．假设每个卷积核的大小为<span class="math inline">\(U × V\)</span>，那么输出<span class="math inline">\(P\)</span>个特征映射共需要<span class="math inline">\(P × D × (U × V ) + P\)</span> 个参数</p>
<h6 id="池化层汇聚层">3.3.2 池化层(汇聚层)</h6>
<p>卷积层虽然可以显著减少网络中连接的数量，但特征映射组中的神经元个数并没有显著减少．如果后面接一个分类器，分类器的输入维数依然很高，很容易出现过拟合．为了解决这个问题，可以在卷积层之后加上一个<strong>汇聚层</strong>，从而降低特征维数，避免过拟合．<strong>汇聚层（Pooling Layer）也叫子采样层（Subsampling Layer），其作用是进行特征选择，降低特征数量，从而减少参数数量</strong>，也叫<strong>池化</strong></p>
<p>假设汇聚层的输入特征映射组为<span class="math inline">\(X∈ ℝ^{M×N×D}\)</span>，对于其中每一个特征映射<span class="math inline">\(X^d ∈ ℝ^{M×N }，1 ≤ d ≤D\)</span>，将其划分为很多区域，这些区域可以重叠，也可以不重叠．<strong>汇聚（Pooling）是指对每个区域进行下采样（Down Sampling）得到一个值，作为这个区域的概括．</strong></p>
<p><strong>常用的池化(汇聚)函数有：</strong></p>
<ul>
<li>最大池化(最大汇聚)：对于一个区域，选择这个区域内所有神经元的最大活性值作为这个区域的表示，即<span class="math inline">\(x^d=max x_i\)</span></li>
<li>平均池化(平均汇聚)：一般是取区域内所有神经元活性值的平均值，即<span class="math inline">\(y^d=avg(\sum x_i)\)</span></li>
</ul>
<p>下图给出了采样最大汇聚进行子采样操作的示例．可以看出，汇聚层不但可以有效地减少神经元的数量，还可以使得网络对一些小的局部形态改变保持不变性，并拥有更大的感受野． <img src="/2022/09/28/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/poolinglayer.png" width="400"></p>
<h4 id="网络优化和正则化">4. 网络优化和正则化</h4>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  


  
  <nav class="pagination">
    <span class="page-number current">1</span><a class="page-number" href="/page/2/">2</a><span class="space">&hellip;</span><a class="page-number" href="/page/4/">4</a><a class="extend next" rel="next" href="/page/2/"><i class="fa fa-angle-right" aria-label="下一页"></i></a>
  </nav>



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="trluper"
      src="/images/avatar.gif">
  <p class="site-author-name" itemprop="name">trluper</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">33</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">29</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">25</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://github.com/trluper" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;trluper" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
  </div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2022</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Trluper</span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-chart-area"></i>
    </span>
    <span title="站点总字数">558k</span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-coffee"></i>
    </span>
    <span title="站点阅读时长">8:27</span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://mist.theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Mist</a> 强力驱动
  </div>

        
<div class="busuanzi-count">
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
    <span class="post-meta-item" id="busuanzi_container_site_uv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="fa fa-user"></i>
      </span>
      <span class="site-uv" title="总访客量">
        <span id="busuanzi_value_site_uv"></span>
      </span>
    </span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item" id="busuanzi_container_site_pv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="fa fa-eye"></i>
      </span>
      <span class="site-pv" title="总访问量">
        <span id="busuanzi_value_site_pv"></span>
      </span>
    </span>
</div>








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/muse.js"></script>


<script src="/js/next-boot.js"></script>

<script src="/js/bookmark.js"></script>




  




  
<script src="/js/local-search.js"></script>













  

  
      

<script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
          load: ['[tex]/mhchem'],
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
          packages: {'[+]': ['mhchem']},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }
</script>

    

  


<script>
NexT.utils.loadComments(document.querySelector('#valine-comments'), () => {
  NexT.utils.getScript('//unpkg.com/valine/dist/Valine.min.js', () => {
    var GUEST = ['nick', 'mail', 'link'];
    var guest = 'nick,mail,link';
    guest = guest.split(',').filter(item => {
      return GUEST.includes(item);
    });
    new Valine({
      el         : '#valine-comments',
      verify     : false,
      notify     : false,
      appId      : 'TXhU0V217D2k9wXcjdYcmYLt-gzGzoHsz',
      appKey     : '7vt5hNdJIFoHxQb92AoWuR3E',
      placeholder: "骚言骚语",
      avatar     : 'wavatar',
      meta       : guest,
      pageSize   : '10' || 10,
      visitor    : false,
      lang       : '' || 'zh-cn',
      path       : location.pathname,
      recordIP   : true,
      serverURLs : ''
    });
  }, window.Valine);
});
</script>

</body>
</html>
