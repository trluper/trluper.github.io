<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 6.2.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"example.com","root":"/","scheme":"Mist","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":true,"show_result":true,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":true,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},"path":"search.xml"};
  </script>

  <meta property="og:type" content="website">
<meta property="og:title" content="Trluper">
<meta property="og:url" content="http://example.com/index.html">
<meta property="og:site_name" content="Trluper">
<meta property="og:locale" content="zh_CN">
<meta property="article:author" content="trluper">
<meta name="twitter:card" content="summary">

<link rel="canonical" href="http://example.com/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : true,
    isPost : false,
    lang   : 'zh-CN'
  };
</script>

  <title>Trluper</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">Trluper</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>关于</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
        <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off"
           placeholder="搜索..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div id="search-result">
  <div id="no-result">
    <i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>
  </div>
</div>

    </div>
  </div>

</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>
  <div class="reading-progress-bar"></div>
  <a role="button" class="book-mark-link book-mark-link-fixed"></a>

  <a href="https://github.com/trluper" class="github-corner" title="Follow me on GitHub" aria-label="Follow me on GitHub" rel="noopener" target="_blank"><svg width="80" height="80" viewBox="0 0 250 250" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg></a>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content index posts-expand">
            
      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/2022/10/31/MMLab%E7%B3%BB%E5%88%97%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="trluper">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Trluper">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/10/31/MMLab%E7%B3%BB%E5%88%97%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/" class="post-title-link" itemprop="url">MMLab系列学习笔记</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2022-10-31 09:20:26" itemprop="dateCreated datePublished" datetime="2022-10-31T09:20:26+08:00">2022-10-31</time>
            </span>

          <br>
            <span class="post-meta-item" title="本文字数">
              <span class="post-meta-item-icon">
                <i class="far fa-file-word"></i>
              </span>
                <span class="post-meta-item-text">本文字数：</span>
              <span>0</span>
            </span>
            <span class="post-meta-item" title="阅读时长">
              <span class="post-meta-item-icon">
                <i class="far fa-clock"></i>
              </span>
                <span class="post-meta-item-text">阅读时长 &asymp;</span>
              <span>1 分钟</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/2022/10/29/Effective-C/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="trluper">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Trluper">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/10/29/Effective-C/" class="post-title-link" itemprop="url">Effective C++</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>
              

              <time title="创建时间：2022-10-29 18:26:41 / 修改时间：21:01:48" itemprop="dateCreated datePublished" datetime="2022-10-29T18:26:41+08:00">2022-10-29</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Effective-C/" itemprop="url" rel="index"><span itemprop="name">Effective C++</span></a>
                </span>
                  ，
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Effective-C/C/" itemprop="url" rel="index"><span itemprop="name">C++</span></a>
                </span>
                  ，
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Effective-C/C/%E9%9D%A2%E8%AF%95/" itemprop="url" rel="index"><span itemprop="name">面试</span></a>
                </span>
            </span>

          <br>
            <span class="post-meta-item" title="本文字数">
              <span class="post-meta-item-icon">
                <i class="far fa-file-word"></i>
              </span>
                <span class="post-meta-item-text">本文字数：</span>
              <span>1.1k</span>
            </span>
            <span class="post-meta-item" title="阅读时长">
              <span class="post-meta-item-icon">
                <i class="far fa-clock"></i>
              </span>
                <span class="post-meta-item-text">阅读时长 &asymp;</span>
              <span>1 分钟</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p><strong>在本笔记中主要对《Effective C++》一书中的重要条款做学习笔记，提取当中主要的知识点和面试考点，读者在进行<code>C plus plus</code>岗位面试前可细读该篇文章的（重要）部分。</strong></p>
<h5 id="条款1视c为一个语言联邦">1 条款1：视C++为一个语言联邦</h5>
<p>在C++中我们总会有一种错觉，那就是人为C++主要就是面向对象的编程，但是这是不全面的，作为从C中延伸的语言，它保留了C的特性，也集成了许多新的功能，你可理解成:</p>
<ul>
<li>C：以C为基础。</li>
<li>面向对象的C++：添加面向对象特性。</li>
<li>模板C++：泛型编程概念，使用模板。</li>
<li>STL：使用STL的容器、迭代器、算法、及函数对象。</li>
</ul>
<p>四者的集合</p>
<h5 id="条款2尽量用编译器替换预处理器">2 条款2：尽量用编译器替换预处理器</h5>
<p>在C/C++中，我们明白一个程序流程为<code>预处理--&gt;编译--&gt;汇编--&gt;链接</code>，在预处理阶段中，我们对文件内的进行<strong>头文件展开、宏替换和去注释</strong>等预处理操作。那么也就是说<strong>像宏定义相关的定义从未被编译器看见</strong> <figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">define</span> ASPECT_RATIO 1.645</span></span><br></pre></td></tr></table></figure> 上面的记号名称<code>ASPECT_RATIO</code>也许在编译器开始处理源码之前它就被预处理器移走了，于是该记号就没有进入<strong>记号表（在二进制文件即生成的<code>.o</code>文件中）</strong>。那么当你运用此常量但获得一个编译错误信息时，这个错误信息也许只会提到<code>1.653</code>而不是<code>ASPECT_ARTIO</code>，而它又被定义在一个非你所写的头文件，那么你对<code>1.635</code>来自何处毫无概念，你要耗费大量时间寻找它。</p>
<p>用<code>const</code>替换<code>#define</code>: <figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">const</span> <span class="type">double</span> AspectRatio=<span class="number">1.635</span>;</span><br></pre></td></tr></table></figure> 作为一个常量，它肯定能被编译器看到并放到符号表内。</p>
<p>基于上述的一个讲解，因此对于条款2有一下准则：</p>
<ul>
<li>对于单纯常量，尽量以<code>const</code>对象或<code>enums</code>枚举来代替#define。</li>
<li>若用<code>define</code>的可能会导致程序出出现多份目标码，而常量不会出现这种情况</li>
<li>取一<code>enum</code>的地址就不合法，这种行为和<code>define</code>类似因此可以用此来代替<code>define</code> ，如果你不想让别人获得一个<code>pointer</code>或<code>reference</code>指向你的某个整数常量，<code>enum</code>可以帮助你实现这个约束。</li>
<li>对于函数宏，用<code>inline</code>函数代替<code>#define</code>（<code>define</code>是死板的替换，容易产生传递计算式类似累加多次的问题）</li>
</ul>
<h5 id="条款3尽可能使用const重要">3. 条款3：尽可能使用const(重要)</h5>
<p><code>const</code>是C++中最为常用、也是最为强大的一个关键字，也是面试过程中极有可能问到的。<strong><code>const</code>可以修饰变量、指针、引用、函数、static</strong>。在说其详细应用时，我们必须先去了解<code>const</code>规则：</p>
<ul>
<li><code>const</code>修饰指针时分为底层<code>const</code>和顶层<code>const</code>。</li>
<li>非<code>const</code>变量可以赋值给<code>const</code>变量，反之不行。</li>
<li><code>const</code>常量必须被初始化</li>
<li>后续不能对<code>const</code>常量做改变</li>
</ul>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/2022/10/29/pytorch%E5%B8%B8%E7%94%A8%E5%87%BD%E6%95%B0/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="trluper">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Trluper">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/10/29/pytorch%E5%B8%B8%E7%94%A8%E5%87%BD%E6%95%B0/" class="post-title-link" itemprop="url">pytorch常用函数</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>
              

              <time title="创建时间：2022-10-29 15:50:21 / 修改时间：17:11:49" itemprop="dateCreated datePublished" datetime="2022-10-29T15:50:21+08:00">2022-10-29</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95/" itemprop="url" rel="index"><span itemprop="name">深度学习算法</span></a>
                </span>
            </span>

          <br>
            <span class="post-meta-item" title="本文字数">
              <span class="post-meta-item-icon">
                <i class="far fa-file-word"></i>
              </span>
                <span class="post-meta-item-text">本文字数：</span>
              <span>5.4k</span>
            </span>
            <span class="post-meta-item" title="阅读时长">
              <span class="post-meta-item-icon">
                <i class="far fa-clock"></i>
              </span>
                <span class="post-meta-item-text">阅读时长 &asymp;</span>
              <span>5 分钟</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h4 id="torch">1. torch</h4>
<p>包 torch 包含了多维张量的数据结构以及基于其上的多种数学操作。另外，它也提供了多种工具，其中一些可以更有效地对张量和任意类型进行序列化。</p>
<p>它有 CUDA 的对应实现，可以在 NVIDIA GPU 上进行张量运算(计算能力&gt;=2.0)。</p>
<h5 id="tensor张量">1.1 Tensor张量</h5>
<h6 id="torch.numel">1.1.1 torch.numel</h6>
<p><code>torch.numel(input)-&gt;int</code>，返回 input 张量中的元素个数。</p>
<ul>
<li>参数: input (Tensor)为输入张量对象 <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>a = torch.randn(<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.numel(a)</span><br><span class="line"><span class="number">120</span></span><br></pre></td></tr></table></figure></li>
</ul>
<h5 id="创建操作">1.2 创建操作</h5>
<h6 id="torch.eye">1.2.1 torch.eye</h6>
<p><code>torch.eye(n, m=None, out=None)</code>,返回一个 2 维张量，对角线位置全 1，其它位置全 0</p>
<ul>
<li><code>n (int )</code> – 行数</li>
<li><code>m (int, optional)</code> – 列数.如果为 None,则默认为 n</li>
<li><code>out (Tensor, optinal)</code> - Output tensor</li>
</ul>
<p>返回值: 对角线位置全 1，其它位置全 0 的 2 维张量 返回值类型: Tensor <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.eye(<span class="number">3</span>)</span><br><span class="line"><span class="number">1</span> <span class="number">0</span> <span class="number">0</span></span><br><span class="line"><span class="number">0</span> <span class="number">1</span> <span class="number">0</span></span><br><span class="line"><span class="number">0</span> <span class="number">0</span> <span class="number">1</span></span><br><span class="line">[torch.FloatTensor of size 3x3]</span><br></pre></td></tr></table></figure></p>
<h6 id="from_numpy">1.2.2 from_numpy</h6>
<p><code>torch.from_numpy(ndarray) → Tensor</code>。将 numpy.ndarray 转换为 pytorch 的 Tensor。返回的张量 tensor 和 numpy的 ndarray 共享同一内存空间。修改一个会导致另外一个也被修改。返回的张量不能改变大小。 <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>a = numpy.array([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>t = torch.from_numpy(a)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>t</span><br><span class="line">torch.LongTensor([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>t[<span class="number">0</span>] = -<span class="number">1</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>a</span><br><span class="line">array([-<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>])</span><br></pre></td></tr></table></figure></p>
<h6 id="torch.linspace">1.2.3 torch.linspace</h6>
<p><code>torch.linspace(start, end, steps=100, out=None) → Tensor</code>返回一个 1 维张量，包含在区间 <code>start</code> 和 <code>end</code> 上均匀间隔的 step 个点。 输出 1 维张量的长度为</p>
<ul>
<li><code>start (float)</code> – 序列的起始点</li>
<li><code>end (float)</code> – 序列的最终值</li>
<li><code>steps (int)</code> – 在 start 和 end 间生成的样本数</li>
<li><code>out (Tensor, optional)</code> – 结果张量 <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.linspace(<span class="number">3</span>, <span class="number">10</span>, steps=<span class="number">5</span>)</span><br><span class="line"><span class="number">3.0000</span></span><br><span class="line"><span class="number">4.7500</span></span><br><span class="line"><span class="number">6.5000</span></span><br><span class="line"><span class="number">8.2500</span></span><br><span class="line"><span class="number">10.0000</span></span><br><span class="line">[torch.FloatTensor of size <span class="number">5</span>]</span><br></pre></td></tr></table></figure></li>
</ul>
<h6 id="torch.ones">1.2.4 torch.ones</h6>
<p><code>torch.ones(*sizes, out=None) → Tensor</code>.返回一个全为 1 的张量，形状由可变参数 sizes 定义。 参数:</p>
<ul>
<li><code>sizes (int...)</code> – 整数序列，定义了输出形状</li>
<li><code>out (Tensor, optional)</code> – 结果张量 <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.ones(<span class="number">2</span>, <span class="number">3</span>)</span><br><span class="line"><span class="number">1</span> <span class="number">1</span> <span class="number">1</span></span><br><span class="line"><span class="number">1</span> <span class="number">1</span> <span class="number">1</span></span><br><span class="line">[torch.FloatTensor of size 2x3]</span><br></pre></td></tr></table></figure></li>
</ul>
<h6 id="torch.rand">1.2.5 torch.rand</h6>
<p><code>torch.rand(*sizes, out=None) → Tensor</code>返回一个张量，包含了从区间[0,1)的均匀分布中抽取的一组随机数，形状由可变参数sizes 定义。 参数:</p>
<ul>
<li><code>sizes (int...)</code> – 整数序列，定义了输出形状</li>
<li><code>out (Tensor, optinal)</code> - 结果张量 <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.rand(<span class="number">4</span>)</span><br><span class="line"><span class="number">0.9193</span></span><br><span class="line"><span class="number">0.3347</span></span><br><span class="line"><span class="number">0.3232</span></span><br><span class="line"><span class="number">0.7715</span></span><br><span class="line">[torch.FloatTensor of size <span class="number">4</span>]</span><br></pre></td></tr></table></figure></li>
</ul>
<h6 id="torch.randn">1.2.6 torch.randn</h6>
<p><code>torch.randn(*sizes, out=None) → Tensor</code>.返回一个张量，包含了从标准正态分布(均值为 0，方差为 1，即高斯白噪声)中抽取一组随机数，形状由可变参数 sizes 定义。 参数:</p>
<ul>
<li><code>sizes (int...)</code> – 整数序列，定义了输出形状</li>
<li><code>out (Tensor, optinal)</code> - 结果张量 <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">torch.randn(<span class="number">2</span>, <span class="number">3</span>)</span><br><span class="line"><span class="number">1.4339</span> <span class="number">0.3351</span> -<span class="number">1.0999</span></span><br><span class="line"><span class="number">1.5458</span> -<span class="number">0.9643</span> -<span class="number">0.3558</span></span><br><span class="line">[torch.FloatTensor of size 2x3]</span><br></pre></td></tr></table></figure></li>
</ul>
<h6 id="torch.randperm">1.2.7 torch.randperm</h6>
<p><code>torch.randperm(n, out=None) → LongTensor</code>.给定参数 n，返回一个从 0 到 n-1 的随机整数排列。 参数:</p>
<ul>
<li><code>n (int)</code> – 上边界(不包含) <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.randperm(<span class="number">4</span>)</span><br><span class="line"><span class="number">2</span></span><br><span class="line"><span class="number">1</span></span><br><span class="line"><span class="number">3</span></span><br><span class="line"><span class="number">0</span></span><br><span class="line">[torch.LongTensor of size <span class="number">4</span>]</span><br></pre></td></tr></table></figure></li>
</ul>
<h6 id="torch.arange">1.2.8 torch.arange</h6>
<p><code>torch.arange(start, end, step=1, out=None) → Tensor</code>。返回一个 1 维张量，长度为 <code>floor((end−start)/step)</code>。包含从<code>start</code>到<code>end</code>，以 <code>step</code> 为步长的一组序列值(默认步长为 1)。 参数:</p>
<ul>
<li><code>start (float)</code> – 序列的起始点</li>
<li><code>end (float)</code> – 序列的终止点</li>
<li><code>step (float)</code> – 相邻点的间隔大小 <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.arange(<span class="number">1</span>, <span class="number">4</span>)</span><br><span class="line"><span class="number">1</span></span><br><span class="line"><span class="number">2</span></span><br><span class="line"><span class="number">3</span></span><br><span class="line">[torch.FloatTensor of size <span class="number">3</span>]</span><br></pre></td></tr></table></figure></li>
</ul>
<h6 id="torch.range">1.2.9 torch.range</h6>
<p><code>torch.range(start, end, step=1, out=None) → Tensor</code>。返回一个 1 维张量，有 <code>floor((end−start)/step)+1</code>个元素。包含在半开区间<code>[start, end）</code>从 start开始，以 step 为步长的一组值。 step 是两个值之间的间隔，即 <code>xi+1=xi+step</code></p>
<p><strong><em>警告：建议使用函数 torch.arange()</em></strong></p>
<ul>
<li><code>start (float)</code> – 序列的起始点</li>
<li><code>end (float)</code> – 序列的最终值</li>
<li><code>step (int)</code> – 相邻点的间隔大小 <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt;torch.<span class="built_in">range</span>(<span class="number">1</span>, <span class="number">4</span>)</span><br><span class="line"><span class="number">1</span></span><br><span class="line"><span class="number">2</span></span><br><span class="line"><span class="number">3</span></span><br><span class="line"><span class="number">4</span></span><br><span class="line">[torch.FloatTensor of size <span class="number">4</span>]</span><br></pre></td></tr></table></figure></li>
</ul>
<h6 id="torch.tensor">1.2.10 torch.tensor</h6>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">torch.tensor(data, </span><br><span class="line">             dtype=<span class="literal">None</span>, </span><br><span class="line">             device=<span class="literal">None</span>, </span><br><span class="line">             requires_grad=<span class="literal">False</span>, </span><br><span class="line">             pin_memory=<span class="literal">False</span>) -&gt; Tensor</span><br></pre></td></tr></table></figure>
<ul>
<li><code>data</code>：数据，可以是list，也可以是numpy</li>
<li><code>dtype</code>：数据类型，默认和data一致</li>
<li><code>device</code>：tensor所在的设备</li>
<li><code>requires_grad</code>：是否需要梯度，默认False，在搭建神经网络时需要将求导的参数设为True</li>
<li><code>pin_memory</code>：是否存于锁页内存，默认False</li>
</ul>
<h6 id="torch.zeros">1.2.11 torch.zeros</h6>
<p>返回一个全为标量 0 的张量，形状由可变参数 sizes 定义。 <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">torch.zeros(*size, </span><br><span class="line">            out=<span class="literal">None</span>, </span><br><span class="line">            dtype=<span class="literal">None</span>, </span><br><span class="line">            layout=torch.strided, </span><br><span class="line">            device=<span class="literal">None</span>, </span><br><span class="line">            requires_grad=<span class="literal">False</span>) -&gt; Tensor</span><br></pre></td></tr></table></figure> - <code>size</code>：张量的形状，如（3，3） - <code>layout</code> ：这个是内存中的布局形式,有strided和sparse_coo等 - <code>out</code>：表示输出张量，就是再把这个张量赋值给别的一个张量，但是这两个张量时一样的，指的同一个内存地址 - <code>device</code>：所在的设备，gpu/cpu - <code>requires_grad</code>：是否需要梯度</p>
<h5 id="tensor的常用操作">1.3 Tensor的常用操作</h5>
<p>对于tensor数据我们经常会进行切片、索引、降维、升维、连接等操作。这里介绍一下我们经常用到的。</p>
<h6 id="torch.cat">1.3.1 torch.cat</h6>
<p><code>torch.cat(inputs, dimension=0) → Tensor</code>。在<strong>给定维度</strong>上对输入的张量序列 seq 进行连接操作。<code>torch.cat()</code>可以看做 <code>torch.split()</code>和 <code>torch.chunk()</code>的反操作。</p>
<ul>
<li><code>inputs (sequence of Tensors)</code> – 可以是任意相同 Tensor 类型的 python 序列</li>
<li><code>dimension (int, optional)</code> – 沿着此维连接张量序列。 <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#沿着0维合并</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>x = torch.randn(<span class="number">2</span>, <span class="number">3</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>x</span><br><span class="line"><span class="number">0.5983</span> -<span class="number">0.0341</span> <span class="number">2.4918</span></span><br><span class="line"><span class="number">1.5981</span> -<span class="number">0.5265</span> -<span class="number">0.8735</span></span><br><span class="line">[torch.FloatTensor of size 2x3]</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.cat((x, x, x), <span class="number">0</span>)</span><br><span class="line"><span class="number">0.5983</span> -<span class="number">0.0341</span> <span class="number">2.4918</span></span><br><span class="line"><span class="number">1.5981</span> -<span class="number">0.5265</span> -<span class="number">0.8735</span></span><br><span class="line"><span class="number">0.5983</span> -<span class="number">0.0341</span> <span class="number">2.4918</span></span><br><span class="line"><span class="number">1.5981</span> -<span class="number">0.5265</span> -<span class="number">0.8735</span></span><br><span class="line"><span class="number">0.5983</span> -<span class="number">0.0341</span> <span class="number">2.4918</span></span><br><span class="line"><span class="number">1.5981</span> -<span class="number">0.5265</span> -<span class="number">0.8735</span></span><br><span class="line">[torch.FloatTensor of size 6x3]</span><br></pre></td></tr></table></figure></li>
</ul>
<h6 id="torch.chunk">1.3.2 torch.chunk</h6>
<p><code>torch.chunk(tensor, chunks, dim=0)</code>。在给定维度(轴)上将输入张量进行分块儿。</p>
<ul>
<li><code>tensor (Tensor)</code> – 待分块的输入张量</li>
<li><code>chunks (int)</code> – 分块的个数</li>
<li><code>dim (int)</code> – 沿着此维度进行分块</li>
</ul>
<h6 id="torch.split">1.3.3 torch.split</h6>
<p><code>torch.split(tensor, split_size, dim=0)</code>。将输入张量分割成相等形状的 chunks（如果可分）。 如果沿指定维的张量形状大小不能被<code>split_size</code>整分， 则最后一个分块会小于其它分块。</p>
<ul>
<li><code>tensor (Tensor)</code> – 待分割张量</li>
<li><code>split_size (int)</code> – 单个分块的形状大小</li>
<li><code>dim (int)</code> – 沿着此维进行分割</li>
</ul>
<h6 id="torch.squeeze">1.3.4 torch.squeeze</h6>
<p><code>torch.squeeze(input, dim=None, out=None)</code>。当未给定<code>dim</code>时，将输入张量形状中维度为1 去除并返回。 如果输入是形如<code>(A×1×B×1×C×1×D)</code>，那么输出形状就为：<code>(A×B×C×D)</code></p>
<p>当给定 dim 时 ， 那 么 挤 压 操 作 只 在 给 定 维 度 上 。 例 如 ， 输 入 形 状为: <code>(A×1×B)</code>, <code>squeeze(input, 0)</code> 将会保持张量不变，只有用 <code>squeeze(input, 1)</code>，形状会变成<code>(A×B)</code>。</p>
<p><strong><em>注意： 返回张量与输入张量共享内存，所以改变其中一个的内容会改变另一个。</em></strong> 参数:</p>
<ul>
<li><code>input (Tensor)</code> – 输入张量</li>
<li><code>dim (int, optional)</code> – 如果给定，则 input 只会在给定维度挤压</li>
<li><code>out (Tensor, optional)</code> – 输出张量 <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>x = torch.zeros(<span class="number">2</span>,<span class="number">1</span>,<span class="number">2</span>,<span class="number">1</span>,<span class="number">2</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>x.size()</span><br><span class="line">(<span class="number">2L</span>, <span class="number">1L</span>, <span class="number">2L</span>, <span class="number">1L</span>, <span class="number">2L</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>y = torch.squeeze(x)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>y.size()</span><br><span class="line">(<span class="number">2L</span>, <span class="number">2L</span>, <span class="number">2L</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>y = torch.squeeze(x, <span class="number">0</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>y.size()</span><br><span class="line">(<span class="number">2L</span>, <span class="number">1L</span>, <span class="number">2L</span>, <span class="number">1L</span>, <span class="number">2L</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>y = torch.squeeze(x, <span class="number">1</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>y.size()</span><br><span class="line">(<span class="number">2L</span>, <span class="number">2L</span>, <span class="number">1L</span>, <span class="number">2L</span>)</span><br></pre></td></tr></table></figure></li>
</ul>
<h6 id="torch.stacksource">1.3.5 torch.stack[source]</h6>
<p><code>torch.stack(sequence, dim=0)</code>。沿着一个新维度对输入张量序列进行连接。 序列中所有的张量都应该为相同形状。</p>
<p>参数:</p>
<ul>
<li><code>sqequence (Sequence)</code> – 待连接的张量序列</li>
<li><code>dim (int)</code> – 插入的维度。必须介于 0 与 待连接的张量序列数之间。</li>
</ul>
<h6 id="torch.transpose">1.3.6 torch.transpose</h6>
<p><code>torch.transpose(input, dim0, dim1, out=None) → Tensor</code>。交换维度 dim0 和 dim1。 输出张量与输入张量共享内存，所以改变其中一个会导致另外一个也被修改。</p>
<ul>
<li><code>input (Tensor)</code> – 输入张量</li>
<li><code>dim0 (int)</code> – 转置的第一维</li>
<li><code>dim1 (int)</code> – 转置的第二维 <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>x = torch.randn(<span class="number">2</span>, <span class="number">3</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>x</span><br><span class="line"><span class="number">0.5983</span> -<span class="number">0.0341</span> <span class="number">2.4918</span></span><br><span class="line"><span class="number">1.5981</span> -<span class="number">0.5265</span> -<span class="number">0.8735</span></span><br><span class="line">[torch.FloatTensor of size 2x3]</span><br></pre></td></tr></table></figure></li>
</ul>
<h6 id="torch.unsqueeze">1.3.7 torch.unsqueeze</h6>
<p><code>torch.unsqueeze(input, dim, out=None)</code>。返回一个新的张量，对输入的制定位置插入维度1 注意： 返回张量与输入张量共享内存，所以改变其中一个的内容会改变另一个。如果 <code>dim</code> 为负，则将会被转化 <code>dim+input.dim()+1</code> 参数:</p>
<ul>
<li><code>tensor (Tensor)</code> – 输入张量</li>
<li><code>dim (int)</code> – 插入维度的索引</li>
<li><code>out (Tensor, optional)</code> – 结果张量</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>x = torch.Tensor([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.unsqueeze(x, <span class="number">0</span>)</span><br><span class="line"><span class="number">1</span> <span class="number">2</span> <span class="number">3</span> <span class="number">4</span></span><br><span class="line">[torch.FloatTensor of size 1x4]</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.unsqueeze(x, <span class="number">1</span>)</span><br><span class="line"><span class="number">1</span></span><br><span class="line"><span class="number">2</span></span><br><span class="line"><span class="number">3</span></span><br><span class="line"><span class="number">4</span></span><br><span class="line">[torch.FloatTensor of size 4x1]</span><br></pre></td></tr></table></figure>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/2022/10/26/Transformer/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="trluper">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Trluper">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/10/26/Transformer/" class="post-title-link" itemprop="url">Transformer</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>
              

              <time title="创建时间：2022-10-26 10:38:28 / 修改时间：21:39:10" itemprop="dateCreated datePublished" datetime="2022-10-26T10:38:28+08:00">2022-10-26</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">深度学习</span></a>
                </span>
                  ，
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/Transformer/" itemprop="url" rel="index"><span itemprop="name">Transformer</span></a>
                </span>
            </span>

          <br>
            <span class="post-meta-item" title="本文字数">
              <span class="post-meta-item-icon">
                <i class="far fa-file-word"></i>
              </span>
                <span class="post-meta-item-text">本文字数：</span>
              <span>2k</span>
            </span>
            <span class="post-meta-item" title="阅读时长">
              <span class="post-meta-item-icon">
                <i class="far fa-clock"></i>
              </span>
                <span class="post-meta-item-text">阅读时长 &asymp;</span>
              <span>2 分钟</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h4 id="transformer是什么">1. Transformer是什么</h4>
<p><code>Transformer</code>是一个利用注意力机制来提高模型训练速度的模型。关于注意力机制可以参看<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/52119092">这篇文章</a>，<strong>trasnformer可以说是完全基于自注意力机制的一个深度学习模型</strong>，因为它适用于并行化计算，和它本身模型的复杂程度导致它在精度和性能上都要高于之前流行的RNN循环神经网络。</p>
<p>对于RNN来说它一般只关注附近的关系关联，而没有从全局寻找关系紧密的相关特征。因此对于一些特定的场景，不能够很好的处理。而Transforme放眼全局,通过计算权重得到相应的向量，得到更为全面的特征</p>
<h4 id="transformer的构成">2. Transformer的构成</h4>
<p>里面主要有两部分组成：<code>Encoder</code>和<code>Decoder</code> <img src="/2022/10/26/Transformer/Constitue.png" width="500"> 当我输入一个文本<code>机器学习</code>的时候，该文本数据会先经过一个叫Encoders的模块，对该文本进行编码，然后将编码后的数据再传入一个叫Decoders的模块进行解码，解码后就得到了翻译后的文本，对应的我们称Encoders为编码器，Decoders为解码器。</p>
<p>这个编码模块里边，有很多小的编码器，一般情况下，<code>Encoders</code>里边有6个小编码器，同样的，<code>Decoders</code>里边有6个小解码器。 <img src="/2022/10/26/Transformer/encorder.png" width="500"></p>
<p>放大一个encoder，发现里边的结构是一个自注意力机制加上一个前馈神经网络。 <img src="/2022/10/26/Transformer/forward.png" width="500"></p>
<h4 id="encoder">3 Encoder</h4>
<h5 id="self-attention的步骤">3.1 self-attention的步骤</h5>
<p>这里我们以文本词向量为例。总结来说Transformer时将输入的特征信息以向量为一组，计算该向量与别的向量之间的紧密程度，即将Q与K进行点积得到权值值，经过softmax后得到所占的比重值，然后整合各向量的V*权重得到该向量的新的特征向量。</p>
<ul>
<li><p><code>transformer</code>首先将词向量乘上三个矩阵，得到三个新的向量，之所以乘上三个矩阵参数而不是直接用原本的词向量是因为这样增加更多的参数，提高模型效果。对于输入<code>X1</code>(机器)，乘上三个矩阵后分别得到<code>Q1,K1,V1</code>，同样的，对于输入<code>X2</code>(学习)，也乘上三个不同的矩阵得到<code>Q2,K2,V2</code>。Q维要去查询的，K为等着被查的，V为实际的特征信息 <img src="/2022/10/26/Transformer/Three.png" width="500"></p></li>
<li><p>计算注意力得分了，这个得分是通过计算Q与各个单词的K向量的点积得到的。我们以X1为例，分别将Q1和K1、K2进行点积运算，假设分别得到得分112和96. <img src="/2022/10/26/Transformer/qk.png" width="500"></p></li>
<li><p>将得分分别除以一个特定数值8（K向量的维度的平方根，这里以K向量的维度是64为例）这能让梯度更加稳定，则得到结果如下： <img src="/2022/10/26/Transformer/score.png" width="500"></p></li>
<li><p>进行softmax运算得到，softmax主要将分数标准化，使他们都是正数并且加起来等于1 <img src="/2022/10/26/Transformer/softmax.png" width="500"></p></li>
<li><p>V向量乘上softmax的结果，这个思想主要是为了保持我们想要关注的单词的值不变，而掩盖掉那些不相关的单词（例如将他们乘上很小的数字） <img src="/2022/10/26/Transformer/cheng.png" width="500"></p></li>
<li><p>将带权重的各个V向量加起来，至此，产生在这个位置上（第一个单词）的self-attention层的输出，其余位置的self-attention输出也是同样的计算方式 <img src="/2022/10/26/Transformer/out.png" width="500"></p></li>
</ul>
<p>将上述的过程总结为一个公式就可以用下图表示： <img src="/2022/10/26/Transformer/comprehension.png" width="500"></p>
<p>上面所举的例子我们可以发现每一个输入向量只是产生了一组Q、K、V，为了进一步提高自注意力的层的性能，产生了<strong>多头注意力机制，它通过不同的head得到多个特征表达式（理解为多个个互不干扰自的注意力机制运算，每一组的Q/K/V都不相同。然后，得到多个个不同的权重矩阵Z，每个权重矩阵被用来将输入向量投射到不同的表示子空间。），然后将所以特征拼接起来，得到更为丰富的输出，然后经过一层全连接实现降维。</strong></p>
<p>如下我们得到了八个矩阵Z，将他们拼接: <img src="/2022/10/26/Transformer/cat.png" width="500"> 后进行全连接后得到我们作为前馈神经网络的输入： <img src="/2022/10/26/Transformer/fc.png" width="500"></p>
<p><strong><em>注意：为了解决梯度消失的问题，在Encoders和Decoder中都是用了残差神经网络的结构，即每一个前馈神经网络的输入不光包含上述self-attention的输出Z，还包含最原始的输入。</em></strong></p>
<h5 id="前馈神经网络">3.2 前馈神经网络</h5>
<p>前馈神经网络的输入是self-attention的输出，即上图的Z,是一个矩阵，矩阵的维度是（序列长度×D词向量），之后前馈神经网络的输出也是同样的维度</p>
<h4 id="decorder">4. Decorder</h4>
<p>decoder中使用的也是同样的结构。也是首先对输出<code>machine learning</code>计算自注意力得分，不同的地方在于，进行过自注意力机制后，将self-attention的输出再与Decoders模块的输出计算一遍交叉注意力机制得分，之后，再进入前馈神经网络模块</p>
<ul>
<li>交叉注意力：指两个模块的矩阵进行注意力计算</li>
<li>自注意力：指在同一模块内进行注意力计算</li>
</ul>
<h4 id="对位置应用认识">5. 对位置应用认识</h4>
<p>上面所举的例子当中只是单纯的由Q、K、V得到权值计算拼接后的东西。但是思考一下下面两个句子： <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">chen Love wu</span><br><span class="line">wu love chen</span><br></pre></td></tr></table></figure> 这两个句子表达的意思事完全不同的，但是相应词的Q、K、V是一样的，经过计算后得到的特征也是一样。这就是所谓的在self-attention中每个词都会考虑整个序列的加权。所以其出现位置并不会对结果产生影响，相当于放哪都无所谓，但这跟实际就有些不符合了，我们希望模型对位置有额外的认识。</p>
<p>因此我们希望能够对位置进行编码，一般对位置编码不进行更新（大佬作品，水论文可以更）</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/2022/10/06/pytorch%E5%9F%BA%E7%A1%80/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="trluper">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Trluper">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/10/06/pytorch%E5%9F%BA%E7%A1%80/" class="post-title-link" itemprop="url">pytorch基础</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2022-10-06 09:33:40" itemprop="dateCreated datePublished" datetime="2022-10-06T09:33:40+08:00">2022-10-06</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2022-10-10 20:08:58" itemprop="dateModified" datetime="2022-10-10T20:08:58+08:00">2022-10-10</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95/" itemprop="url" rel="index"><span itemprop="name">深度学习算法</span></a>
                </span>
            </span>

          <br>
            <span class="post-meta-item" title="本文字数">
              <span class="post-meta-item-icon">
                <i class="far fa-file-word"></i>
              </span>
                <span class="post-meta-item-text">本文字数：</span>
              <span>36k</span>
            </span>
            <span class="post-meta-item" title="阅读时长">
              <span class="post-meta-item-icon">
                <i class="far fa-clock"></i>
              </span>
                <span class="post-meta-item-text">阅读时长 &asymp;</span>
              <span>33 分钟</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h4 id="pytorch简介">1、pytorch简介</h4>
<p>pytorch是一个基于Python的科学计算包，PyTorch 的设计遵循tensor→variable(autograd)→nn.Module 三个由低到高的抽象层次，分别代表高维数组（张量）、自动求导（变量）和神经网络（层/模块），而且这三个抽象之间联系紧密，可以同时进行修改和操作。它主要有两个用途：</p>
<ul>
<li>类似于Numpy但是能利用GPU加速</li>
<li>一个非常灵活和快速用于深度学习的研究平台</li>
</ul>
<h4 id="基本数据结构tensor">2、基本数据结构：Tensor</h4>
<p>Tensor在pttorch中负责存储基本数据，ptyTorch针对Tensor也提供了丰富的函数和方法，所以pyTorch中的Tensor与Numpy的数组具有极高的相似性。Tensor是一种高级的API。</p>
<p><strong>Tensor即张量，张量是Pytorch的核心概念，pytorch的计算都是基于张量的计算，是PyTorch中的基本操作对象，可以看做是包含单一数据类型元素的多维矩阵</strong>。从使用角度来看，Tensor与NumPy的ndarrays非常类似，相互之间也可以自由转换，只不过Tensor还支持GPU的加速</p>
<table>
<thead>
<tr class="header">
<th>数据类型</th>
<th>CPU Tensor</th>
<th>GPU Tensor</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>32位浮点</td>
<td><code>torch.FloatTensor</code></td>
<td><code>torch.cuda.FloatTensor</code></td>
</tr>
<tr class="even">
<td>64位浮点</td>
<td><code>torch.DoubleTensor</code></td>
<td><code>torch.cuda.DoubleTensor</code></td>
</tr>
<tr class="odd">
<td>16位半精度浮点</td>
<td><code>N/A</code></td>
<td><code>torch.cuda.HalfTensor</code></td>
</tr>
<tr class="even">
<td>8位无符号整型</td>
<td><code>torch.ByteTensor</code></td>
<td><code>torch.cuda.ByteTensor</code></td>
</tr>
<tr class="odd">
<td>8位有符号整型</td>
<td><code>torch.charTensor</code></td>
<td><code>torch.cuda.charTensor</code></td>
</tr>
<tr class="even">
<td>16位有符号整型</td>
<td><code>torch.ShortTensor</code></td>
<td><code>torch.cuda.ShortTensor</code></td>
</tr>
<tr class="odd">
<td>32位有符号整型</td>
<td><code>torch.IntTensor</code></td>
<td><code>torch.cuda.IntTensor</code></td>
</tr>
<tr class="even">
<td>64位有符号整型</td>
<td><code>torch.LongTensor</code></td>
<td><code>torch.cuda.LongTensor</code></td>
</tr>
</tbody>
</table>
<p>pytorch不支持str类型</p>
<h5 id="tensor的创建">2.1 Tensor的创建</h5>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">torch.tensor(data, </span><br><span class="line">             dtype=<span class="literal">None</span>, </span><br><span class="line">             device=<span class="literal">None</span>, </span><br><span class="line">             requires_grad=<span class="literal">False</span>, </span><br><span class="line">             pin_memory=<span class="literal">False</span>) -&gt; Tensor</span><br></pre></td></tr></table></figure>
<ul>
<li><code>data</code>：数据，可以是list，也可以是numpy</li>
<li><code>dtype</code>：数据类型，默认和data一致</li>
<li><code>device</code>：tensor所在的设备</li>
<li><code>requires_grad</code>：是否需要梯度，默认False，在搭建神经网络时需要将求导的参数设为True</li>
<li><code>pin_memory</code>：是否存于锁页内存，默认False</li>
</ul>
<p>还有其他的按数值创建的方法，这里只列举一个： <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">torch.zeros(*size, </span><br><span class="line">            out=<span class="literal">None</span>, </span><br><span class="line">            dtype=<span class="literal">None</span>, </span><br><span class="line">            layout=torch.strided, </span><br><span class="line">            device=<span class="literal">None</span>, </span><br><span class="line">            requires_grad=<span class="literal">False</span>) -&gt; Tensor</span><br></pre></td></tr></table></figure> - <code>size</code>：张量的形状，如（3，3） - <code>layout</code> ：这个是内存中的布局形式,有strided和sparse_coo等 - <code>out</code>：表示输出张量，就是再把这个张量赋值给别的一个张量，但是这两个张量时一样的，指的同一个内存地址 - <code>device</code>：所在的设备，gpu/cpu - <code>requires_grad</code>：是否需要梯度</p>
<p><img src="/2022/10/06/pytorch%E5%9F%BA%E7%A1%80/build.png" width="500"></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#使用特定类型构造函数创建</span></span><br><span class="line">i=torch.FloatTensor(<span class="number">2</span>,<span class="number">3</span>)						<span class="comment">#构造了一个2*3的32位浮点矩阵，初始值为0</span></span><br><span class="line">b=torch.FloatTensor([[<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>],[<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>]])			<span class="comment">#使用列表构造一个2*3的2维张量</span></span><br><span class="line"><span class="comment">#使用tensor函数</span></span><br><span class="line">a=torch.tensor([[<span class="number">2</span>,<span class="number">5</span>,<span class="number">7</span>],[<span class="number">10</span>,<span class="number">2</span>,<span class="number">4</span>]],dtype=<span class="built_in">float</span>)	<span class="comment">#dtype指定类型，如果省略则自动推断</span></span><br><span class="line"><span class="comment">#使用其他函数</span></span><br><span class="line">t=torch.randn(<span class="number">2</span>,<span class="number">2</span>)								<span class="comment">#生一个2*2随机2维张量</span></span><br><span class="line"><span class="comment">#如果张量中只有一个元素, 可以用.item()将值取出, 作为一个python number</span></span><br><span class="line">x = torch.randn(<span class="number">1</span>)</span><br><span class="line"><span class="built_in">print</span>(x)</span><br><span class="line"><span class="built_in">print</span>(x.item())</span><br></pre></td></tr></table></figure>
<h5 id="张量的尺寸">2.2 张量的尺寸</h5>
<p><strong>可以使用<code>shape</code>属性或者<code>size()</code>方法查看张量在每一维的长度.</strong> <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">t=torch.randn(<span class="number">2</span>,<span class="number">2</span>)</span><br><span class="line"><span class="built_in">print</span>(t.shape,t.size())</span><br></pre></td></tr></table></figure> 输出： <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.Size([<span class="number">2</span>, <span class="number">2</span>]) torch.Size([<span class="number">2</span>, <span class="number">2</span>])</span><br></pre></td></tr></table></figure></p>
<p>也能使用可以使用<code>view()</code>方法改变张量的尺寸。如果<code>view()</code>方法改变尺寸失败，可以使用<code>reshape()</code>方法 <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">b=torch.FloatTensor([[<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>],[<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>]])</span><br><span class="line">m=b.view(<span class="number">3</span>,<span class="number">2</span>)									<span class="comment">#将2*3转为3*2</span></span><br></pre></td></tr></table></figure> 有些时候有些操作会让张量存储结构扭曲，比如转置，直接使用view会失败，可以用reshape方法 <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">m=b.reshape(<span class="number">3</span>,<span class="number">2</span>)</span><br></pre></td></tr></table></figure></p>
<h5 id="tensor和numpy数组">2.3 Tensor和numpy数组</h5>
<p>可以用numpy方法从Tensor得到numpy数组，也可以用torch.from_numpy从numpy数组得到Tensor。<strong>这两种方法关联的Tensor和numpy数组是共享数据内存的，即改变其中一个，另一个也会发生改变</strong>。因此如果不需要共享，可以用张量的<code>clone()</code>方法拷贝张量，中断这种关联 <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="comment">#torch.from_numpy函数从numpy数组转为Tensor</span></span><br><span class="line">arr = np.zeros(<span class="number">3</span>)</span><br><span class="line">tensor = torch.from_numpy(arr)</span><br><span class="line">np.add(arr,<span class="number">1</span>, out = arr) 		<span class="comment">#给arr增加1，tensor也随之改变</span></span><br><span class="line"><span class="built_in">print</span>(arr)</span><br><span class="line"><span class="built_in">print</span>(tensor)</span><br><span class="line"></span><br><span class="line"><span class="comment">#使用clone</span></span><br><span class="line"><span class="comment"># 可以用clone() 方法拷贝张量，中断这种关联</span></span><br><span class="line">tensor = torch.zeros(<span class="number">3</span>)</span><br><span class="line"><span class="comment">#使用clone方法拷贝张量, 拷贝后的张量和原始张量内存独立</span></span><br><span class="line">arr = tensor.clone().numpy() <span class="comment"># 也可以使用tensor.data.numpy()</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#将Torch Tensor转换为Numpy array</span></span><br><span class="line">a = torch.ones(<span class="number">5</span>)</span><br><span class="line">b = a.numpy()</span><br></pre></td></tr></table></figure> 输出 <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[<span class="number">1.</span> <span class="number">1.</span> <span class="number">1.</span>]</span><br><span class="line">tensor([<span class="number">1.</span>, <span class="number">1.</span>, <span class="number">1.</span>], dtype=torch.float64)</span><br></pre></td></tr></table></figure></p>
<h5 id="tensor操作">2.4 Tensor操作</h5>
<p>Tensor同样跟python一样支持切片、合并分割操作和相应的数学运算 ###### 2.4.1 索引切片 切片时支持缺省参数和省略号。可以通过索引和切片对部分元素进行修改。 <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">t=torch.randn(<span class="number">5</span>,<span class="number">6</span>)</span><br><span class="line">m=t[<span class="number">0</span>:-<span class="number">1</span>:<span class="number">2</span>,<span class="number">1</span>:-<span class="number">1</span>:<span class="number">3</span>]		<span class="comment">#表示从第一行到最后一行每隔一行取一行，从第二列到最后一列每隔两列取一列</span></span><br></pre></td></tr></table></figure></p>
<h6 id="合并分割">2.4.2 合并分割</h6>
<ul>
<li>可以用<code>torch.cat()</code>方法和<code>torch.stack()</code>方法将多个张量合并，</li>
<li>可以用<code>torch.split()</code>方法把一个张量分割成多个张量。</li>
<li><code>torch.cat()</code>和<code>torch.stack()</code>有略微的区别，<code>torch.cat()</code>是连接，不会增加维度，而<code>torch.stack()</code>是堆叠， 会增加维度</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">a=torch.tensor([[<span class="number">1</span>,<span class="number">2</span>],[<span class="number">3</span>,<span class="number">4</span>]])</span><br><span class="line">b=torch.tensor([[<span class="number">5</span>,<span class="number">6</span>],[<span class="number">7</span>,<span class="number">8</span>]])</span><br><span class="line">c=torch.tensor([[<span class="number">9</span>,<span class="number">10</span>],[<span class="number">11</span>,<span class="number">12</span>]])</span><br><span class="line">d=torch.cat([a,b,c])</span><br><span class="line"><span class="built_in">print</span>(d)</span><br><span class="line">e=torch.stack([a,b,c])</span><br><span class="line"><span class="built_in">print</span>(e)</span><br><span class="line">f,g,h=torch.split(d,split_size_or_sections=<span class="number">2</span>,dim=<span class="number">0</span>)</span><br><span class="line"><span class="built_in">print</span>(f,g,h)</span><br></pre></td></tr></table></figure>
<p>输出： <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">tensor([[ <span class="number">1</span>,  <span class="number">2</span>],</span><br><span class="line">        [ <span class="number">3</span>,  <span class="number">4</span>],</span><br><span class="line">        [ <span class="number">5</span>,  <span class="number">6</span>],</span><br><span class="line">        [ <span class="number">7</span>,  <span class="number">8</span>],</span><br><span class="line">        [ <span class="number">9</span>, <span class="number">10</span>],</span><br><span class="line">        [<span class="number">11</span>, <span class="number">12</span>]])</span><br><span class="line">tensor([[[ <span class="number">1</span>,  <span class="number">2</span>],</span><br><span class="line">         [ <span class="number">3</span>,  <span class="number">4</span>]],</span><br><span class="line"></span><br><span class="line">        [[ <span class="number">5</span>,  <span class="number">6</span>],</span><br><span class="line">         [ <span class="number">7</span>,  <span class="number">8</span>]],</span><br><span class="line"></span><br><span class="line">        [[ <span class="number">9</span>, <span class="number">10</span>],</span><br><span class="line">         [<span class="number">11</span>, <span class="number">12</span>]]])</span><br><span class="line">tensor([[<span class="number">1</span>, <span class="number">2</span>],</span><br><span class="line">        [<span class="number">3</span>, <span class="number">4</span>]]) tensor([[<span class="number">5</span>, <span class="number">6</span>],</span><br><span class="line">        [<span class="number">7</span>, <span class="number">8</span>]]) tensor([[ <span class="number">9</span>, <span class="number">10</span>],</span><br><span class="line">        [<span class="number">11</span>, <span class="number">12</span>]])</span><br><span class="line"></span><br></pre></td></tr></table></figure></p>
<h5 id="tensor的运算操作">2.5 Tensor的运算操作</h5>
<p>张量数学运算主要有：标量运算，向量运算，矩阵运算。</p>
<h6 id="标量运算">2.5.1 标量运算</h6>
<p>加减乘除乘方，以及三角函数，指数，对数等常见函数，逻辑比较运算符等都是标量运算符。标量运算符的特点是对张量实施逐元素运算。有些标量运算符对常用的数学运算符进行了重载，并且支持类似numpy的广播特性 <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#例1-3-1 张量的数学运算-标量运算</span></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">a = torch.tensor([[<span class="number">1.0</span>,<span class="number">2</span>],[-<span class="number">3</span>,<span class="number">4.0</span>]])</span><br><span class="line">b = torch.tensor([[<span class="number">5.0</span>,<span class="number">6</span>],[<span class="number">7.0</span>,<span class="number">8.0</span>]])</span><br><span class="line">a+b <span class="comment">#运算符重载</span></span><br><span class="line"> </span><br><span class="line">out:</span><br><span class="line">tensor([[ <span class="number">6.</span>,  <span class="number">8.</span>],</span><br><span class="line">        [ <span class="number">4.</span>, <span class="number">12.</span>]])</span><br><span class="line"> </span><br><span class="line">a-b</span><br><span class="line"> </span><br><span class="line">out:</span><br><span class="line">tensor([[ -<span class="number">4.</span>,  -<span class="number">4.</span>],</span><br><span class="line">        [-<span class="number">10.</span>,  -<span class="number">4.</span>]])</span><br><span class="line"> </span><br><span class="line">a*b</span><br><span class="line"> </span><br><span class="line">out:</span><br><span class="line">tensor([[  <span class="number">5.</span>,  <span class="number">12.</span>],</span><br><span class="line">        [-<span class="number">21.</span>,  <span class="number">32.</span>]])</span><br><span class="line"> </span><br><span class="line"> </span><br><span class="line">a/b</span><br><span class="line"> </span><br><span class="line">out:</span><br><span class="line">tensor([[ <span class="number">0.2000</span>,  <span class="number">0.3333</span>],</span><br><span class="line">        [-<span class="number">0.4286</span>,  <span class="number">0.5000</span>]])</span><br><span class="line"> </span><br><span class="line"> </span><br><span class="line">a**<span class="number">2</span></span><br><span class="line"> </span><br><span class="line">out:</span><br><span class="line">tensor([[ <span class="number">1.</span>,  <span class="number">4.</span>],</span><br><span class="line">        [ <span class="number">9.</span>, <span class="number">16.</span>]])</span><br><span class="line"> </span><br><span class="line"> </span><br><span class="line">a**(<span class="number">0.5</span>)</span><br><span class="line"> </span><br><span class="line">out:</span><br><span class="line">tensor([[<span class="number">1.0000</span>, <span class="number">1.4142</span>],</span><br><span class="line">        [   nan, <span class="number">2.0000</span>]])</span><br><span class="line"> </span><br><span class="line">a%<span class="number">3</span> <span class="comment">#求模</span></span><br><span class="line"> </span><br><span class="line">out:</span><br><span class="line">tensor([[<span class="number">1.</span>, <span class="number">2.</span>],</span><br><span class="line">        [-<span class="number">0.</span>, <span class="number">1.</span>]])</span><br><span class="line"> </span><br><span class="line"> </span><br><span class="line">a//<span class="number">3</span> <span class="comment">#地板除法</span></span><br><span class="line"> </span><br><span class="line">out:</span><br><span class="line">tensor([[ <span class="number">0.</span>,  <span class="number">0.</span>],</span><br><span class="line">        [-<span class="number">1.</span>,  <span class="number">1.</span>]])</span><br><span class="line"> </span><br><span class="line"> </span><br><span class="line">a&gt;=<span class="number">2</span> <span class="comment"># torch.ge(a,2) #ge: greater_equal缩写</span></span><br><span class="line"> </span><br><span class="line">out:</span><br><span class="line">tensor([[<span class="literal">False</span>,  <span class="literal">True</span>],</span><br><span class="line">        [<span class="literal">False</span>,  <span class="literal">True</span>]])</span><br><span class="line"> </span><br><span class="line"> </span><br><span class="line">(a&gt;=<span class="number">2</span>)&amp;(a&lt;=<span class="number">3</span>)</span><br><span class="line"> </span><br><span class="line">out:</span><br><span class="line">tensor([[<span class="literal">False</span>,  <span class="literal">True</span>],</span><br><span class="line">        [<span class="literal">False</span>, <span class="literal">False</span>]])</span><br><span class="line"> </span><br><span class="line"> </span><br><span class="line"> </span><br><span class="line"> </span><br><span class="line">(a&gt;=<span class="number">2</span>)|(a&lt;=<span class="number">3</span>)</span><br><span class="line"> </span><br><span class="line">out:</span><br><span class="line">tensor([[<span class="literal">True</span>, <span class="literal">True</span>],</span><br><span class="line">        [<span class="literal">True</span>, <span class="literal">True</span>]])</span><br><span class="line"> </span><br><span class="line">a==<span class="number">5</span> <span class="comment">#torch.eq(a,5)</span></span><br><span class="line"> </span><br><span class="line">out:</span><br><span class="line">tensor([[<span class="literal">False</span>, <span class="literal">False</span>],</span><br><span class="line">        [<span class="literal">False</span>, <span class="literal">False</span>]])</span><br><span class="line"></span><br><span class="line">torch.sqrt(a)</span><br><span class="line"> </span><br><span class="line">out:</span><br><span class="line">tensor([[<span class="number">1.0000</span>, <span class="number">1.4142</span>],</span><br><span class="line">        [   nan, <span class="number">2.0000</span>]])</span><br><span class="line"> </span><br><span class="line"> </span><br><span class="line">a = torch.tensor([<span class="number">1.0</span>,<span class="number">8.0</span>])</span><br><span class="line">b = torch.tensor([<span class="number">5.0</span>,<span class="number">6.0</span>])</span><br><span class="line">c = torch.tensor([<span class="number">6.0</span>,<span class="number">7.0</span>])</span><br><span class="line">d = a+b+c</span><br><span class="line"><span class="built_in">print</span>(d)</span><br><span class="line"> </span><br><span class="line">out:</span><br><span class="line">tensor([<span class="number">12.</span>, <span class="number">21.</span>])</span><br><span class="line"> </span><br><span class="line"> </span><br><span class="line"><span class="built_in">print</span>(torch.<span class="built_in">max</span>(a,b))</span><br><span class="line"> </span><br><span class="line">out:</span><br><span class="line">tensor([<span class="number">5.</span>, <span class="number">8.</span>])</span><br><span class="line"> </span><br><span class="line"> </span><br><span class="line"><span class="built_in">print</span>(torch.<span class="built_in">min</span>(a,b))</span><br><span class="line"> </span><br><span class="line">out:</span><br><span class="line">tensor([<span class="number">1.</span>, <span class="number">6.</span>])</span><br><span class="line"> </span><br><span class="line"> </span><br><span class="line">x = torch.tensor([<span class="number">2.6</span>,-<span class="number">2.7</span>])</span><br><span class="line"><span class="built_in">print</span>(torch.<span class="built_in">round</span>(x)) <span class="comment">#保留整数部分，四舍五入</span></span><br><span class="line"><span class="built_in">print</span>(torch.floor(x)) <span class="comment">#保留整数部分，向下归整</span></span><br><span class="line"><span class="built_in">print</span>(torch.ceil(x)) <span class="comment">#保留整数部分，向上归整</span></span><br><span class="line"><span class="built_in">print</span>(torch.trunc(x)) <span class="comment">#保留整数部分，向0归整</span></span><br><span class="line"> </span><br><span class="line">out:</span><br><span class="line">tensor([ <span class="number">3.</span>, -<span class="number">3.</span>])</span><br><span class="line">tensor([ <span class="number">2.</span>, -<span class="number">3.</span>])</span><br><span class="line">tensor([ <span class="number">3.</span>, -<span class="number">2.</span>])</span><br><span class="line">tensor([ <span class="number">2.</span>, -<span class="number">2.</span>])</span><br><span class="line"> </span><br><span class="line"> </span><br><span class="line">x = torch.tensor([<span class="number">2.6</span>,-<span class="number">2.7</span>])</span><br><span class="line"><span class="built_in">print</span>(torch.fmod(x,<span class="number">2</span>)) <span class="comment">#作除法取余数</span></span><br><span class="line"><span class="built_in">print</span>(torch.remainder(x,<span class="number">2</span>)) <span class="comment">#作除法取剩余的部分，结果恒正</span></span><br><span class="line"> </span><br><span class="line">out:</span><br><span class="line">tensor([ <span class="number">0.6000</span>, -<span class="number">0.7000</span>])</span><br><span class="line">tensor([<span class="number">0.6000</span>, <span class="number">1.3000</span>])</span><br><span class="line"> </span><br><span class="line"> </span><br><span class="line"><span class="comment"># 幅值裁剪</span></span><br><span class="line">x = torch.tensor([<span class="number">0.9</span>,-<span class="number">0.8</span>,<span class="number">100.0</span>,-<span class="number">20.0</span>,<span class="number">0.7</span>])</span><br><span class="line">y = torch.clamp(x,<span class="built_in">min</span>=-<span class="number">1</span>,<span class="built_in">max</span> = <span class="number">1</span>)</span><br><span class="line">z = torch.clamp(x,<span class="built_in">max</span> = <span class="number">1</span>)</span><br><span class="line"><span class="built_in">print</span>(y)</span><br><span class="line"><span class="built_in">print</span>(z)</span><br><span class="line"> </span><br><span class="line">out:</span><br><span class="line">tensor([ <span class="number">0.9000</span>, -<span class="number">0.8000</span>,  <span class="number">1.0000</span>, -<span class="number">1.0000</span>,  <span class="number">0.7000</span>])</span><br><span class="line">tensor([  <span class="number">0.9000</span>,  -<span class="number">0.8000</span>,   <span class="number">1.0000</span>, -<span class="number">20.0000</span>,   <span class="number">0.7000</span>])</span><br></pre></td></tr></table></figure></p>
<h6 id="向量运算">2.5.2 向量运算</h6>
<p>向量运算符只在一个特定轴上运算，将一个向量映射到一个标量或者另外一个向量。 <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#例1-3-2  张量的数学运算-向量运算</span></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="comment">#统计值</span></span><br><span class="line">a = torch.arange(<span class="number">1</span>,<span class="number">10</span>).<span class="built_in">float</span>()</span><br><span class="line"><span class="built_in">print</span>(torch.<span class="built_in">sum</span>(a))</span><br><span class="line"><span class="built_in">print</span>(torch.mean(a))</span><br><span class="line"><span class="built_in">print</span>(torch.<span class="built_in">max</span>(a))</span><br><span class="line"><span class="built_in">print</span>(torch.<span class="built_in">min</span>(a))</span><br><span class="line"><span class="built_in">print</span>(torch.prod(a)) <span class="comment">#累乘</span></span><br><span class="line"><span class="built_in">print</span>(torch.std(a)) <span class="comment">#标准差</span></span><br><span class="line"><span class="built_in">print</span>(torch.var(a)) <span class="comment">#方差</span></span><br><span class="line"><span class="built_in">print</span>(torch.median(a)) <span class="comment">#中位数</span></span><br><span class="line"> </span><br><span class="line">out:</span><br><span class="line">tensor(<span class="number">45.</span>)</span><br><span class="line">tensor(<span class="number">5.</span>)</span><br><span class="line">tensor(<span class="number">9.</span>)</span><br><span class="line">tensor(<span class="number">1.</span>)</span><br><span class="line">tensor(<span class="number">362880.</span>)</span><br><span class="line">tensor(<span class="number">2.7386</span>)</span><br><span class="line">tensor(<span class="number">7.5000</span>)</span><br><span class="line">tensor(<span class="number">5.</span>)</span><br><span class="line"> </span><br><span class="line"> </span><br><span class="line"><span class="comment">#指定维度计算统计值</span></span><br><span class="line">b = a.view(<span class="number">3</span>,<span class="number">3</span>)</span><br><span class="line"><span class="built_in">print</span>(b)</span><br><span class="line"><span class="built_in">print</span>(torch.<span class="built_in">max</span>(b,dim = <span class="number">0</span>))</span><br><span class="line"><span class="built_in">print</span>(torch.<span class="built_in">max</span>(b,dim = <span class="number">1</span>))</span><br><span class="line"> </span><br><span class="line">out:</span><br><span class="line">tensor([[<span class="number">1.</span>, <span class="number">2.</span>, <span class="number">3.</span>],</span><br><span class="line">        [<span class="number">4.</span>, <span class="number">5.</span>, <span class="number">6.</span>],</span><br><span class="line">        [<span class="number">7.</span>, <span class="number">8.</span>, <span class="number">9.</span>]])</span><br><span class="line">torch.return_types.<span class="built_in">max</span>(</span><br><span class="line">values=tensor([<span class="number">7.</span>, <span class="number">8.</span>, <span class="number">9.</span>]),</span><br><span class="line">indices=tensor([<span class="number">2</span>, <span class="number">2</span>, <span class="number">2</span>]))</span><br><span class="line">torch.return_types.<span class="built_in">max</span>(</span><br><span class="line">values=tensor([<span class="number">3.</span>, <span class="number">6.</span>, <span class="number">9.</span>]),</span><br><span class="line">indices=tensor([<span class="number">2</span>, <span class="number">2</span>, <span class="number">2</span>]))</span><br><span class="line"> </span><br><span class="line"> </span><br><span class="line"><span class="comment">#cum扫描</span></span><br><span class="line">a = torch.arange(<span class="number">1</span>,<span class="number">10</span>)</span><br><span class="line"><span class="built_in">print</span>(torch.cumsum(a,<span class="number">0</span>))</span><br><span class="line"><span class="built_in">print</span>(torch.cumprod(a,<span class="number">0</span>))</span><br><span class="line"><span class="built_in">print</span>(torch.cummax(a,<span class="number">0</span>).values)</span><br><span class="line"><span class="built_in">print</span>(torch.cummax(a,<span class="number">0</span>).indices)</span><br><span class="line"><span class="built_in">print</span>(torch.cummin(a,<span class="number">0</span>))</span><br><span class="line"> </span><br><span class="line">out:</span><br><span class="line">tensor([ <span class="number">1</span>,  <span class="number">3</span>,  <span class="number">6</span>, <span class="number">10</span>, <span class="number">15</span>, <span class="number">21</span>, <span class="number">28</span>, <span class="number">36</span>, <span class="number">45</span>])</span><br><span class="line">tensor([     <span class="number">1</span>,      <span class="number">2</span>,      <span class="number">6</span>,     <span class="number">24</span>,    <span class="number">120</span>,    <span class="number">720</span>,   <span class="number">5040</span>,  <span class="number">40320</span>, <span class="number">362880</span>])</span><br><span class="line">tensor([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>, <span class="number">7</span>, <span class="number">8</span>, <span class="number">9</span>])</span><br><span class="line">tensor([<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>, <span class="number">7</span>, <span class="number">8</span>])</span><br><span class="line">torch.return_types.cummin(</span><br><span class="line">values=tensor([<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>]),</span><br><span class="line">indices=tensor([<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>]))</span><br><span class="line"> </span><br><span class="line"> </span><br><span class="line"><span class="comment">#torch.sort和torch.topk可以对张量排序</span></span><br><span class="line">a = torch.tensor([[<span class="number">9</span>,<span class="number">7</span>,<span class="number">8</span>],[<span class="number">1</span>,<span class="number">3</span>,<span class="number">2</span>],[<span class="number">5</span>,<span class="number">6</span>,<span class="number">4</span>]]).<span class="built_in">float</span>()</span><br><span class="line"><span class="built_in">print</span>(torch.topk(a,<span class="number">2</span>,dim = <span class="number">0</span>),<span class="string">&quot;\n&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(torch.topk(a,<span class="number">2</span>,dim = <span class="number">1</span>),<span class="string">&quot;\n&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(torch.sort(a,dim = <span class="number">1</span>),<span class="string">&quot;\n&quot;</span>)</span><br><span class="line"> </span><br><span class="line">out:</span><br><span class="line">torch.return_types.topk(</span><br><span class="line">values=tensor([[<span class="number">9.</span>, <span class="number">7.</span>, <span class="number">8.</span>],</span><br><span class="line">        [<span class="number">5.</span>, <span class="number">6.</span>, <span class="number">4.</span>]]),</span><br><span class="line">indices=tensor([[<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>],</span><br><span class="line">        [<span class="number">2</span>, <span class="number">2</span>, <span class="number">2</span>]])) </span><br><span class="line"> </span><br><span class="line">torch.return_types.topk(</span><br><span class="line">values=tensor([[<span class="number">9.</span>, <span class="number">8.</span>],</span><br><span class="line">        [<span class="number">3.</span>, <span class="number">2.</span>],</span><br><span class="line">        [<span class="number">6.</span>, <span class="number">5.</span>]]),</span><br><span class="line">indices=tensor([[<span class="number">0</span>, <span class="number">2</span>],</span><br><span class="line">        [<span class="number">1</span>, <span class="number">2</span>],</span><br><span class="line">        [<span class="number">1</span>, <span class="number">0</span>]])) </span><br><span class="line"> </span><br><span class="line">torch.return_types.sort(</span><br><span class="line">values=tensor([[<span class="number">7.</span>, <span class="number">8.</span>, <span class="number">9.</span>],</span><br><span class="line">        [<span class="number">1.</span>, <span class="number">2.</span>, <span class="number">3.</span>],</span><br><span class="line">        [<span class="number">4.</span>, <span class="number">5.</span>, <span class="number">6.</span>]]),</span><br><span class="line">indices=tensor([[<span class="number">1</span>, <span class="number">2</span>, <span class="number">0</span>],</span><br><span class="line">        [<span class="number">0</span>, <span class="number">2</span>, <span class="number">1</span>],</span><br><span class="line">        [<span class="number">2</span>, <span class="number">0</span>, <span class="number">1</span>]]))</span><br></pre></td></tr></table></figure></p>
<h6 id="矩阵运算">2.5.3 矩阵运算</h6>
<p>矩阵必须是二维的，类似<code>torch.tensor([1,2,3])</code>这样的不是矩阵。矩阵运算包括：<strong>矩阵乘法，矩阵转置，矩阵逆，矩阵求迹，矩阵范数，矩阵行列式，矩阵求特征 值，矩阵分解等运算</strong> <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#例1-3-3 张量的数学运算-矩阵运算</span></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="comment">#矩阵乘法</span></span><br><span class="line">a = torch.tensor([[<span class="number">1</span>,<span class="number">2</span>],[<span class="number">3</span>,<span class="number">4</span>]])</span><br><span class="line">b = torch.tensor([[<span class="number">5</span>,<span class="number">6</span>],[<span class="number">7</span>,<span class="number">8</span>]]</span><br></pre></td></tr></table></figure> <strong>1. 矩阵乘法</strong>：矩阵乘法可以使用<code>a@b</code>，也可以函数<code>torch.matmul(a,b)</code>或者`torch.mm(a,b)</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">a@b</span><br><span class="line">torch.matmul(a,b)</span><br><span class="line">torch.mm(a,b)</span><br></pre></td></tr></table></figure>
<p>输出： <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">tensor([[<span class="number">19</span>, <span class="number">22</span>],</span><br><span class="line">        [<span class="number">43</span>, <span class="number">50</span>]])</span><br><span class="line">tensor([[<span class="number">19</span>, <span class="number">22</span>],</span><br><span class="line">        [<span class="number">43</span>, <span class="number">50</span>]])</span><br><span class="line">tensor([[<span class="number">19</span>, <span class="number">22</span>],</span><br><span class="line">        [<span class="number">43</span>, <span class="number">50</span>]])</span><br></pre></td></tr></table></figure></p>
<p><strong>2. 矩阵转置</strong>：转置直接使用其成员函数<code>t()</code> <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">a.t()</span><br><span class="line"></span><br><span class="line">out:</span><br><span class="line">tensor([[<span class="number">1.</span>, <span class="number">3.</span>],</span><br><span class="line">        [<span class="number">2.</span>, <span class="number">4.</span>]])</span><br></pre></td></tr></table></figure></p>
<p><strong>3. 逆矩阵</strong>：求逆使用Tensorde1<code>inverse()</code>函数。矩阵逆，必须为浮点类型 <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">torch.inverse(a)</span><br><span class="line"></span><br><span class="line">out:</span><br><span class="line">tensor([[-<span class="number">2.0000</span>,  <span class="number">1.0000</span>],</span><br><span class="line">        [ <span class="number">1.5000</span>, -<span class="number">0.5000</span>]], dtype=torch.float64)</span><br></pre></td></tr></table></figure></p>
<p><strong>4. 矩阵求Tr</strong> <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">a = torch.tensor([[<span class="number">1.0</span>,<span class="number">2</span>],[<span class="number">3</span>,<span class="number">4</span>]])</span><br><span class="line"><span class="built_in">print</span>(torch.trace(a))</span><br><span class="line"></span><br><span class="line">out:</span><br><span class="line">tensor(<span class="number">5.</span>)</span><br></pre></td></tr></table></figure></p>
<p><strong>5. 矩阵求范数</strong> <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">torch.norm(a)</span><br><span class="line"></span><br><span class="line">out:</span><br><span class="line">tensor(<span class="number">5.4772</span>)</span><br></pre></td></tr></table></figure></p>
<p><strong>6. 矩阵行列式</strong> <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">torch.det(a)</span><br><span class="line"></span><br><span class="line">out:</span><br><span class="line">tensor(-<span class="number">2.0000</span>)</span><br></pre></td></tr></table></figure></p>
<p><strong>7. 矩阵特征值和特征向量</strong> <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">k=torch.tensor([[<span class="number">1</span>,<span class="number">2</span>],[-<span class="number">5</span>,<span class="number">4</span>]],dtype=<span class="built_in">float</span>)</span><br><span class="line">L_complex,V_complex=torch.linalg.eig(k)</span><br><span class="line"><span class="built_in">print</span>(L_complex,V_complex)</span><br></pre></td></tr></table></figure></p>
<p><strong>8. 矩阵QR分解</strong>：将一个方阵分解为一个正交矩阵q和上三角矩阵r。QR分解实际上是对矩阵a实施Schmidt正交化得到q</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">a=torch.tensor([[<span class="number">1</span>,<span class="number">2</span>],[<span class="number">3</span>,<span class="number">4</span>]],dtype=<span class="built_in">float</span>)</span><br><span class="line">q,r=torch.linalg.qr(a)</span><br><span class="line"><span class="built_in">print</span>(q,r)</span><br></pre></td></tr></table></figure>
<p><strong>9. 矩阵svd分解：</strong>svd分解可以将任意一个矩阵分解为一个正交矩阵u,一个对角阵s和一个正交矩阵v.t()的乘积，svd常用于矩阵压缩和降维 <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">u,s,v=torch.linalg.svd(a)</span><br><span class="line"><span class="built_in">print</span>(u,<span class="string">&quot;\n&quot;</span>,s,<span class="string">&quot;\n&quot;</span>,v)</span><br></pre></td></tr></table></figure> 输出 <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">tensor([[-<span class="number">0.4046</span>, -<span class="number">0.9145</span>],</span><br><span class="line">        [-<span class="number">0.9145</span>,  <span class="number">0.4046</span>]], dtype=torch.float64) </span><br><span class="line">tensor([<span class="number">5.4650</span>, <span class="number">0.3660</span>], dtype=torch.float64) </span><br><span class="line">tensor([[-<span class="number">0.5760</span>, -<span class="number">0.8174</span>],</span><br><span class="line">        [ <span class="number">0.8174</span>, -<span class="number">0.5760</span>]], dtype=torch.float64)</span><br></pre></td></tr></table></figure></p>
<h5 id="广播机制">2.6 广播机制</h5>
<p>Pytorch的广播规则和numpy是一样的:</p>
<ul>
<li><p>1、如果张量的维度不同，将维度较小的张量进行扩展，直到两个张量的维度都一样。</p></li>
<li><p>2、如果两个张量在某个维度上的长度是相同的，或者其中一个张量在该维度上的长度为1， 那么我们就说这两个张量在该维度上是相容的。</p></li>
<li><p>3、如果两个张量在所有维度上都是相容的，它们就能使用广播。</p></li>
<li><p>4、广播之后，每个维度的长度将取两个张量在该维度长度的较大值。</p></li>
<li><p>5、在任何一个维度上，如果一个张量的长度为1，另一个张量长度大于1，那么在该维度上，就好像是对第一个张量进行了复制。 torch.broadcast_tensors可以将多个张量根据广播规则转换成相同的维度。</p></li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#例 1-3-4  广播机制</span></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line">a = torch.tensor([<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>])</span><br><span class="line">b = torch.tensor([[<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>],[<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>],[<span class="number">2</span>,<span class="number">2</span>,<span class="number">2</span>]])</span><br><span class="line"><span class="built_in">print</span>(b + a)</span><br><span class="line">a_broad,b_broad = torch.broadcast_tensors(a,b)</span><br><span class="line"><span class="built_in">print</span>(a_broad,<span class="string">&quot;\n&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(b_broad,<span class="string">&quot;\n&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(a_broad + b_broad)</span><br><span class="line"> </span><br><span class="line">out:</span><br><span class="line">tensor([[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>],</span><br><span class="line">        [<span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>],</span><br><span class="line">        [<span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>]])</span><br><span class="line">tensor([[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>],</span><br><span class="line">        [<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>],</span><br><span class="line">        [<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>]]) </span><br><span class="line"> </span><br><span class="line">tensor([[<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>],</span><br><span class="line">        [<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>],</span><br><span class="line">        [<span class="number">2</span>, <span class="number">2</span>, <span class="number">2</span>]]) </span><br><span class="line"> </span><br><span class="line">tensor([[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>],</span><br><span class="line">        [<span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>],</span><br><span class="line">        [<span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>]])</span><br></pre></td></tr></table></figure>
<h4 id="其他基础知识">3. 其他基础知识</h4>
<p>上面介绍了pytorch的数据结构Tensor以及Tensor的一些操作函数，这里介绍深度学习必须用到的微分求导和动态计算图。</p>
<h5 id="自动微分机制">3.1 自动微分机制</h5>
<p>神经网络通常依赖<strong>反向传播求梯度来更新网络参数</strong>，求梯度过程通常是一件非常复杂而容易出错的事情。而深度学习框架可以帮助我们自动地完成这种求梯度运算。这就是Pytorch的自动微分机制是指： - <strong>Pytorch一般通过反向传播<code>backward()</code>方法实现这种求梯度计算。</strong>该方法求得的梯度将存在对应自变量张量的<code>grad·属性下。 - 除此之外，也能够调用</code>torch.autograd.grad()`函数来实现求梯度计算。</p>
<h6 id="backward方法求导数">3.1.1 backward方法求导数</h6>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">torch.autograd.backward(</span><br><span class="line">    tensors: <span class="type">Union</span>[torch.Tensor, <span class="type">Sequence</span>[torch.Tensor]],</span><br><span class="line">    gradient: <span class="type">Union</span>[torch.Tensor, <span class="type">Sequence</span>[torch.Tensor], NoneType] = <span class="literal">None</span>,</span><br><span class="line">    retain_graph: <span class="type">Union</span>[<span class="built_in">bool</span>, NoneType] = <span class="literal">None</span>,</span><br><span class="line">    create_graph: <span class="built_in">bool</span> = <span class="literal">False</span>,</span><br><span class="line">    grad_variables: <span class="type">Union</span>[torch.Tensor, <span class="type">Sequence</span>[torch.Tensor], NoneType] = <span class="literal">None</span>,</span><br><span class="line">    inputs: <span class="type">Union</span>[torch.Tensor, <span class="type">Sequence</span>[torch.Tensor], NoneType] = <span class="literal">None</span>,</span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<ul>
<li><code>tensor</code>:表示用于求导的张量，如loss，</li>
<li><code>gradient</code>: 设置梯度权重，在计算矩阵的梯度时会用到，也是一个tensor，shape和前面的tensor保持一致</li>
<li><code>retain_graph</code>:表示保存计算图，由于pytorch采用了动态图机制，在每一次反向传播之后，计算图都会被释放掉。如果不想释放，就设置这个参数为True</li>
<li><code>create_graph</code>:创建导数计算图，用于高阶求导</li>
</ul>
<p><strong><em>注</em></strong>：tensor类的<code>backward()</code>函数内部调用了<code>torch.autograd.backward()</code></p>
<p><code>backward()</code>方法通常在一个标量张量上调用，该方法求得的梯度将存在对应自变量张量的<code>grad</code>属性下。<strong>如果调用的张量非标量，则要传入一个和它同形状的gradient参数张量,改张量是设置梯度权重的</strong>。相当于用该gradient参数张量与调用张量作向量点乘，得到的标量结果再反向传播。</p>
<p><strong>下面分别介绍标量和非标量的反向传播：</strong> <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#标量的反向传播</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">test</span>():</span><br><span class="line">    <span class="comment">#f=ax**2+bx+c</span></span><br><span class="line">    x=torch.tensor(<span class="number">1.0</span>,requires_grad=<span class="literal">True</span>)</span><br><span class="line">    a=torch.tensor(<span class="number">1.0</span>)</span><br><span class="line">    b=torch.tensor(-<span class="number">2.0</span>)</span><br><span class="line">    c=torch.tensor(<span class="number">5.6</span>)</span><br><span class="line">    y=a*torch.<span class="built_in">pow</span>(x,<span class="number">2</span>)+b*x+c</span><br><span class="line">    y.backward()</span><br><span class="line">    dy_dx=x.grad</span><br><span class="line">    <span class="built_in">print</span>(dy_dx)</span><br><span class="line"></span><br><span class="line">输出：</span><br><span class="line">tensor(<span class="number">0.</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#非标量的反向传播</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">test1</span>():</span><br><span class="line">    <span class="comment"># f=ax**2+bx+c</span></span><br><span class="line">    x=torch.tensor([[<span class="number">0.0</span>,<span class="number">1.0</span>],[<span class="number">5.0</span>,<span class="number">2.0</span>]],requires_grad=<span class="literal">True</span>)</span><br><span class="line">    a = torch.tensor(<span class="number">1.0</span>)</span><br><span class="line">    b = torch.tensor(-<span class="number">2.0</span>)</span><br><span class="line">    c = torch.tensor(<span class="number">5.6</span>)</span><br><span class="line">    y = a * torch.<span class="built_in">pow</span>(x, <span class="number">2</span>) + b * x + c</span><br><span class="line">    gradient = torch.tensor([[<span class="number">1.0</span>, <span class="number">1.0</span>], [<span class="number">1.0</span>, <span class="number">1.0</span>]])</span><br><span class="line">    y.backward(gradient=gradient)</span><br><span class="line">    dy_dx=x.grad</span><br><span class="line">    <span class="built_in">print</span>(dy_dx)</span><br><span class="line"></span><br><span class="line">输出：</span><br><span class="line">tensor([[-<span class="number">2.</span>,  <span class="number">0.</span>],</span><br><span class="line">        [ <span class="number">8.</span>,  <span class="number">2.</span>]])</span><br></pre></td></tr></table></figure></p>
<p><strong>注：非标量的反向传播也可以用标量的反向传播实现，如下只需加一句<code>z = torch.sum(y*gradient)</code>，然后以<code>z.backward()</code>即可</strong> <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">z = torch.<span class="built_in">sum</span>(y*gradient)</span><br><span class="line">z.backward()</span><br><span class="line">x_grad = x.grad</span><br><span class="line"><span class="built_in">print</span>(x_grad)</span><br></pre></td></tr></table></figure></p>
<h6 id="利用autograd.grad方法求导数">3.1.2 利用autograd.grad方法求导数</h6>
<p><code>torch.autograd.grad()</code>这个方法的功能也是求梯度，可以实现高阶的求导。 <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">torch.autograd.grad(</span><br><span class="line">    outputs: <span class="type">Union</span>[torch.Tensor, <span class="type">Sequence</span>[torch.Tensor]],</span><br><span class="line">    inputs: <span class="type">Union</span>[torch.Tensor, <span class="type">Sequence</span>[torch.Tensor]],</span><br><span class="line">    grad_outputs: <span class="type">Union</span>[torch.Tensor, <span class="type">Sequence</span>[torch.Tensor], NoneType] = <span class="literal">None</span>,</span><br><span class="line">    retain_graph: <span class="type">Union</span>[<span class="built_in">bool</span>, NoneType] = <span class="literal">None</span>,</span><br><span class="line">    create_graph: <span class="built_in">bool</span> = <span class="literal">False</span>,</span><br><span class="line">    only_inputs: <span class="built_in">bool</span> = <span class="literal">True</span>,</span><br><span class="line">    allow_unused: <span class="built_in">bool</span> = <span class="literal">False</span>,</span><br><span class="line">) -&gt; <span class="type">Tuple</span>[torch.Tensor, ...]</span><br></pre></td></tr></table></figure> - <code>outputs</code>：用于求导的张量； - <code>inputs</code>: 需要梯度的张量； - <code>create_graph</code>:创建导数计算图，用于高阶求导 - <code>retain_graph</code>:保存计算图 - <code>grad_outputs</code>:多梯度权重</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#例2-1-2 利用autograd.grad方法求导数</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="comment"># f(x) = a*x**2 + b*x + c的导数</span></span><br><span class="line">x = torch.tensor(<span class="number">0.0</span>,requires_grad = <span class="literal">True</span>) <span class="comment"># x需要被求导</span></span><br><span class="line">a = torch.tensor(<span class="number">1.0</span>)</span><br><span class="line">b = torch.tensor(-<span class="number">2.0</span>)</span><br><span class="line">c = torch.tensor(<span class="number">1.0</span>)</span><br><span class="line">y = a*torch.<span class="built_in">pow</span>(x,<span class="number">2</span>) + b*x + c</span><br><span class="line"><span class="comment"># create_graph 设置为 True 将允许创建更高阶的导数</span></span><br><span class="line">dy_dx = torch.autograd.grad(y,x,create_graph=<span class="literal">True</span>)[<span class="number">0</span>]</span><br><span class="line"><span class="built_in">print</span>(dy_dx.data)</span><br><span class="line"><span class="comment"># 求二阶导数</span></span><br><span class="line">dy2_dx2 = torch.autograd.grad(dy_dx,x)[<span class="number">0</span>]</span><br><span class="line"><span class="built_in">print</span>(dy2_dx2.data)</span><br><span class="line"> </span><br><span class="line">out：</span><br><span class="line">tensor(-<span class="number">2.</span>)</span><br><span class="line">tensor(<span class="number">2.</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#例1-2-2 利用autograd.grad方法求导数，对多个自变量求导数</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line">x1 = torch.tensor(<span class="number">1.0</span>,requires_grad = <span class="literal">True</span>) <span class="comment"># x需要被求导</span></span><br><span class="line">x2 = torch.tensor(<span class="number">2.0</span>,requires_grad = <span class="literal">True</span>)</span><br><span class="line">y1 = x1*x2</span><br><span class="line">y2 = x1+x2</span><br><span class="line"><span class="comment"># 允许同时对多个自变量求导数</span></span><br><span class="line">(dy1_dx1,dy1_dx2) = torch.autograd.grad(outputs=y1,inputs =</span><br><span class="line">[x1,x2],retain_graph = <span class="literal">True</span>)</span><br><span class="line"><span class="built_in">print</span>(dy1_dx1,dy1_dx2)</span><br><span class="line"><span class="comment"># 如果有多个因变量，相当于把多个因变量的梯度结果求和</span></span><br><span class="line">(dy12_dx1,dy12_dx2) = torch.autograd.grad(outputs=[y1,y2],inputs = [x1,x2])</span><br><span class="line"><span class="built_in">print</span>(dy12_dx1,dy12_dx2)</span><br><span class="line"> </span><br><span class="line">out：</span><br><span class="line">tensor(<span class="number">2.</span>) tensor(<span class="number">1.</span>)</span><br><span class="line">tensor(<span class="number">3.</span>) tensor(<span class="number">2.</span>)</span><br></pre></td></tr></table></figure>
<h6 id="利用自动微分和优化器求最小值">3.1.3 利用自动微分和优化器求最小值</h6>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#例2-1-3 利用自动微分和优化器求最小值</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="comment"># f(x) = a*x**2 + b*x + c的最小值</span></span><br><span class="line">x = torch.tensor(<span class="number">0.0</span>,requires_grad = <span class="literal">True</span>) <span class="comment"># x需要被求导</span></span><br><span class="line">a = torch.tensor(<span class="number">1.0</span>)</span><br><span class="line">b = torch.tensor(-<span class="number">2.0</span>)</span><br><span class="line">c = torch.tensor(<span class="number">1.0</span>)</span><br><span class="line">optimizer = torch.optim.SGD(params=[x],lr = <span class="number">0.01</span>)</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">f</span>(<span class="params">x</span>):</span><br><span class="line">	result = a*torch.<span class="built_in">pow</span>(x,<span class="number">2</span>) + b*x + c</span><br><span class="line">	<span class="keyword">return</span>(result)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">500</span>):</span><br><span class="line">	optimizer.zero_grad()</span><br><span class="line">	y = f(x)</span><br><span class="line">	y.backward()</span><br><span class="line">	optimizer.step()</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;y=&quot;</span>,f(x).data,<span class="string">&quot;;&quot;</span>,<span class="string">&quot;x=&quot;</span>,x.data)</span><br><span class="line"> </span><br><span class="line">out：</span><br><span class="line">y= tensor(<span class="number">0.</span>) ; x= tensor(<span class="number">1.0000</span>)</span><br></pre></td></tr></table></figure>
<p><strong><em>注</em></strong>：优化器后续讲解</p>
<h5 id="动态计算图">3.2 动态计算图</h5>
<p>Pytorch的计算图由节点和边组成，<strong>节点表示张量或者Function，边表示张量和Function之间的依赖关系</strong>。Pytorch中的计算图是动态图。 <img src="/2022/10/06/pytorch%E5%9F%BA%E7%A1%80/computgraph.png" width="400"> 从上面可以看出<span class="math inline">\(y = a × b\)</span>，而<span class="math inline">\(a = w + x, b = w + 1\)</span>，只要给出<span class="math inline">\(x\)</span>和<span class="math inline">\(w\)</span>的值，即可根据计算图得出<span class="math inline">\(y\)</span>的值。上图中用求y对w的导数，根据求导规则，如下： <span class="math display">\[
{δy\over δw}={δy\over δa}{δa\over δw}+{δy \over δb}{δb \over δw}\\
=b*1+a*1\\
=b+a\\
=(w+1)+(x+w)\\
=2*w+x+1\\
=2*1+2+1\\
=5
\]</span> 体现到计算图中，就是根节点 y 到叶子节点 w 有两条路径 y -&gt; a -&gt; w和y -&gt;b -&gt; w。根节点依次对每条路径的叶子节点求导，一直到叶子节点w，最后把每条路径的导数相加即可 <img src="/2022/10/06/pytorch%E5%9F%BA%E7%A1%80/compute.png" width="400"> <strong>在tensor中包含一个<code>is_leaf</code>(叶子节点)属性，叶子节点就是用户创建的节点，在上面的例子中，<span class="math inline">\(x\)</span> 和<span class="math inline">\(w\)</span> 是叶子节点，其他所有节点都依赖于叶子节点</strong>。叶子节点的概念主要是为了节省内存，在计算图中的一轮反向传播结束之后，非叶子节点的梯度是会被释放的。</p>
<p>只有叶子节点的梯度保留了下来，而非叶子的梯度为空，如果在反向传播之后仍需要保留非叶子节点的梯度，可以对节点使用<code>retain_grad=True</code> <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#查看是否是叶子节点</span></span><br><span class="line"><span class="built_in">print</span>(w.is_leaf, x.is_leaf, a.is_leaf, b.is_leaf)</span><br><span class="line"></span><br><span class="line"><span class="comment">###result</span></span><br><span class="line"><span class="literal">True</span> <span class="literal">True</span> <span class="literal">False</span> <span class="literal">False</span></span><br></pre></td></tr></table></figure></p>
<h6 id="何为动态">3.2.1 何为动态</h6>
<p><strong>这里的动态主要有两重含义:</strong></p>
<ul>
<li><p><strong>第一层含义是</strong>：计算图的正向传播是立即执行的。无需等待完整的计算图创建完毕，每条语句都会在计算图中动态添加节点和边，并立即执行正向传播得到计算结果。</p></li>
<li><p><strong>第二层含义是</strong>：计算图在反向传播后立即销毁。下次调用需要重新构建计算图。如果在程序中使用了<code>backward()</code>方法执行了反向传播，或者利用<code>torch.autograd.grad()</code>方法计算了梯度，那么创建的计算图会被立即销毁，释放存储空间，下次调用需要重新创建</p></li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#例2-2-1 计算图的正向传播是立即执行的</span></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line">w = torch.tensor([[<span class="number">3.0</span>,<span class="number">1.0</span>]],requires_grad=<span class="literal">True</span>)</span><br><span class="line">b = torch.tensor([[<span class="number">3.0</span>]],requires_grad=<span class="literal">True</span>)</span><br><span class="line">X = torch.randn(<span class="number">10</span>,<span class="number">2</span>)</span><br><span class="line">Y = torch.randn(<span class="number">10</span>,<span class="number">1</span>)</span><br><span class="line">Y_hat = X@w.t() + b <span class="comment"># Y_hat定义后其正向传播被立即执行，与其后面的loss创建语句无关</span></span><br><span class="line">loss = torch.mean(torch.<span class="built_in">pow</span>(Y_hat-Y,<span class="number">2</span>))</span><br><span class="line"><span class="built_in">print</span>(loss.data)</span><br><span class="line"><span class="built_in">print</span>(Y_hat.data)</span><br><span class="line"> </span><br><span class="line">out：</span><br><span class="line">tensor(<span class="number">25.9445</span>)</span><br><span class="line">tensor([[ <span class="number">5.8349</span>],</span><br><span class="line">        [ <span class="number">0.5817</span>],</span><br><span class="line">        [-<span class="number">4.2764</span>],</span><br><span class="line">        [ <span class="number">3.2476</span>],</span><br><span class="line">        [ <span class="number">3.6737</span>],</span><br><span class="line">        [ <span class="number">2.8748</span>],</span><br><span class="line">        [ <span class="number">8.3981</span>],</span><br><span class="line">        [ <span class="number">7.1418</span>],</span><br><span class="line">        [-<span class="number">4.8522</span>],</span><br><span class="line">        [ <span class="number">2.2610</span>]])</span><br><span class="line"> </span><br><span class="line"> </span><br><span class="line"><span class="comment">#计算图在反向传播后立即销毁</span></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line">w = torch.tensor([[<span class="number">3.0</span>,<span class="number">1.0</span>]],requires_grad=<span class="literal">True</span>)</span><br><span class="line">b = torch.tensor([[<span class="number">3.0</span>]],requires_grad=<span class="literal">True</span>)</span><br><span class="line">X = torch.randn(<span class="number">10</span>,<span class="number">2</span>)</span><br><span class="line">Y = torch.randn(<span class="number">10</span>,<span class="number">1</span>)</span><br><span class="line">Y_hat = X@w.t() + b <span class="comment"># Y_hat定义后其正向传播被立即执行，与其后面的loss创建语句无关</span></span><br><span class="line">loss = torch.mean(torch.<span class="built_in">pow</span>(Y_hat-Y,<span class="number">2</span>))</span><br><span class="line"><span class="comment">#计算图在反向传播后立即销毁，如果需要保留计算图, 需要设置retain_graph = True</span></span><br><span class="line">loss.backward() <span class="comment">#loss.backward(retain_graph = True) </span></span><br><span class="line"><span class="comment">#loss.backward() #如果再次执行反向传播将报错</span></span><br></pre></td></tr></table></figure>
<h6 id="动态图机制">3.2.1 动态图机制</h6>
<p>pytroch采用的是动态图机制，而tensorflow采用的是静态图机制。静态图是先搭建，后运算；动态图是运算和搭建同时进行，也就是可以先计算前面节点的值，再根据这些值搭建后面的计算图。优点是灵活，易调节，易调试。 <img src="/2022/10/06/pytorch%E5%9F%BA%E7%A1%80/dynamicR.png" width="600"></p>
<h4 id="数据的读取">4. 数据的读取</h4>
<p>机器学习的五大模块分别是：数据、模块、损失函数、优化器和迭代训练。这里我们介绍数据模块，要对模型进行训练必须要有数据，怎么讲数据读取进来存储是我们要解决的问题。数据模块又可分为以下几部分：</p>
<ul>
<li>数据的收集：<code>Image、label</code></li>
<li>数据的划分：<code>train、test、valid</code></li>
<li>数据的读取：<code>DataLoader</code>，有两个子模块，<code>Sampler</code>和<code>Dataset</code>，<code>Sampler</code>是对数据集生成索引，<code>DataSet</code>是根据索引读取数据</li>
<li>数据预处理：<code>torchvision.transforms</code>模块</li>
</ul>
<p><strong>Pytorch通常使用<code>Dataset</code>和<code>DataLoader</code>这两个工具类来构建数据管道:</strong></p>
<ul>
<li><p><code>Dataset</code>定义了数据集的内容，它相当于一个类似列表的数据结构，具有确定的长度，能够用索 引获取数据集中的元素。</p></li>
<li><p><code>DataLoader</code>定义了按<code>batch</code>加载数据集的方法，它是一个实现了<code>__iter__</code>方法的可迭代对象，每次迭代输出一个<code>batch</code>的数据。<code>DataLoader</code>能够控制<code>batch</code>的大小，<code>batch</code>中元素的采样方法，以及将batch结果整理成模型所需 输入形式的方法，并且能够使用多进程读取数据。</p></li>
<li><p>在绝大部分情况下，用户只需实现<code>Dataset</code>的<code>__len__</code>方法和<code>__getitem__</code>方法，就可以轻松构 建自己的数据集，并用默认数据管道进行加载</p></li>
</ul>
<h5 id="dataloader和dataset概述">4.1 DataLoader和DataSet概述</h5>
<h6 id="获取一个batch数据的步骤">4.1.1 获取一个batch数据的步骤</h6>
<p>让我们考虑一下从一个数据集中获取一个batch的数据需要哪些步骤。 (假定数据集的特征和标签分别表示为张量 <span class="math inline">\(X\)</span> 和 <span class="math inline">\(Y\)</span> ，数据集可以表示为 <span class="math inline">\((X,Y)\)</span> , 假定batch大小为<span class="math inline">\(m\)</span> )</p>
<ul>
<li><ol type="a">
<li>首先我们要确定数据集的长度<span class="math inline">\(n\)</span> ,假设$ n = 1000$ 。</li>
</ol></li>
<li><ol start="2" type="a">
<li>然后我们从<span class="math inline">\(0\)</span> 到<span class="math inline">\(n-1\)</span>的范围中抽样出<span class="math inline">\(m\)</span>个数(batch大小)。假定<span class="math inline">\(m=4\)</span>, 拿到的结果是一个列表，类似： <span class="math inline">\(indices = [1,4,8,9]\)</span></li>
</ol></li>
<li><ol start="3" type="a">
<li>接着我们从数据集中去取这 m 个数对应下标的元素。 拿到的结果是一个元组列表，类似： <span class="math inline">\(samples = [(X[1],Y[1]),(X[4],Y[4]),(X[8],Y[8]), (X[9],Y[9])]\)</span></li>
</ol></li>
<li><ol start="4" type="a">
<li>最后我们将结果整理成两个张量作为输出。 拿到的结果是两个张量，类似$ batch = (features,labels)$其中 <span class="math inline">\(features = torch.stack([X[1],X[4],X[8],X[9]]) labels = torch.stack([Y[1],Y[4],Y[8],Y[9]])\)</span></li>
</ol></li>
</ul>
<h6 id="dataset和dataloader的功能分工">4.1.2 Dataset和DataLoader的功能分工</h6>
<ul>
<li><p>上述第a个步骤确定数据集的长度是由<code>Dataset</code>的<code>__len__</code>方法实现的。</p></li>
<li><p>第b个步骤从<span class="math inline">\(0\)</span> 到<span class="math inline">\(n-1\)</span>的范围中抽样出 m 个数的方法是由 <code>DataLoader</code>的 <code>sampler</code> 和 <code>batch_sampler</code> 参数指定的。<code>sampler</code>参数指定单个元素抽样方法，一般无需用户设置，程序默认在<code>DataLoader</code>的参数<code>shuffle=True</code>时采用随机抽样， <code>shuffle=False</code> 时采用顺序抽样。<code>batch_sampler</code>参数将多个抽样的元素整理成一个列表，一般无需用户设置，默认方法在<code>DataLoader</code>的参数 <code>drop_last=True</code>时会丢弃数据集最后一个长度不能被batch大小整除的批次，在 <code>drop_last=False</code>时保留最后一个批次。</p></li>
<li><p>第c个步骤的核心逻辑根据下标取数据集中的元素 是由<code>Dataset</code>的 <code>__getitem__</code>方法实现的。</p></li>
<li><p>第d个步骤的逻辑由<code>DataLoader</code>的参数 <code>collate_fn</code> 指定。一般情况下也无需用户设置。</p></li>
</ul>
<h5 id="使用dataset创建数据集">4.2 使用Dataset创建数据集</h5>
<p>Dataset创建数据集常用的方法有以下几个：</p>
<ul>
<li><ol type="a">
<li>使用 <code>torch.utils.data.TensorDataset</code> 根据<code>Tensor</code>创建数据集(<code>numpy</code>的<code>array</code>，<code>Pandas</code>的 <code>DataFrame</code>需要先转换成<code>Tensor</code>)。</li>
</ol></li>
<li><ol start="2" type="a">
<li>使用 <code>torchvision.datasets.ImageFolder</code> 根据图片目录创建图片数据集。 <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">dataset=torchvision.datasets.ImageFolder(</span><br><span class="line">                       root, transform=<span class="literal">None</span>, </span><br><span class="line">                       target_transform=<span class="literal">None</span>, </span><br><span class="line">                       loader=&lt;function default_loader&gt;, </span><br><span class="line">                       is_valid_file=<span class="literal">None</span>)</span><br><span class="line">root：图片存储的根目录，即各类别文件夹所在目录的上一级目录。</span><br><span class="line">transform：对图片进行预处理的操作（函数），原始图片作为输入，返回一个转换后的图片。</span><br><span class="line">target_transform：对图片类别进行预处理的操作，输入为 target，输出对其的转换。如果不传该参数，即对 target 不做任何转换，返回的顺序索引 <span class="number">0</span>,<span class="number">1</span>, <span class="number">2</span>…</span><br><span class="line">loader：表示数据集加载方式，通常默认加载方式即可。</span><br><span class="line">is_valid_file：获取图像文件的路径并检查该文件是否为有效文件的函数(用于检查损坏文件)</span><br></pre></td></tr></table></figure></li>
</ol></li>
<li><ol start="3" type="a">
<li>继承<code>torch.utils.data.Dataset</code> 创建自定义数据集。</li>
</ol></li>
<li><ol start="4" type="a">
<li>此外，还可以通过 <code>torch.utils.data.random_split</code> 将一个数据集分割成多份，常用于分割训练集，验证集和测试集。</li>
</ol></li>
<li><ol start="5" type="a">
<li>调用<code>Dataset</code>的加法运算符( + )将多个数据集合并成一个数据集。</li>
</ol></li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#例4-2-1  根据Tensor创建数据集</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> TensorDataset,Dataset,DataLoader,random_split</span><br><span class="line"> </span><br><span class="line"><span class="comment"># 根据Tensor创建数据集</span></span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> datasets</span><br><span class="line">iris = datasets.load_iris()</span><br><span class="line">ds_iris = TensorDataset(torch.tensor(iris.data),torch.tensor(iris.target))</span><br><span class="line"><span class="comment"># 分割成训练集和预测集</span></span><br><span class="line">n_train = <span class="built_in">int</span>(<span class="built_in">len</span>(ds_iris)*<span class="number">0.8</span>)</span><br><span class="line">n_valid = <span class="built_in">len</span>(ds_iris) - n_train</span><br><span class="line">ds_train,ds_valid = random_split(ds_iris,[n_train,n_valid])</span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">type</span>(ds_iris))</span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">type</span>(ds_train))</span><br><span class="line"> </span><br><span class="line">out:</span><br><span class="line">&lt;<span class="keyword">class</span> <span class="string">&#x27;torch.utils.data.dataset.TensorDataset&#x27;</span>&gt;</span><br><span class="line">&lt;<span class="keyword">class</span> <span class="string">&#x27;torch.utils.data.dataset.Subset&#x27;</span>&gt;</span><br><span class="line"> </span><br><span class="line"> </span><br><span class="line"> </span><br><span class="line"> </span><br><span class="line"><span class="comment"># 使用DataLoader加载数据集</span></span><br><span class="line">dl_train,dl_valid = DataLoader(ds_train,batch_size =</span><br><span class="line"><span class="number">8</span>),DataLoader(ds_valid,batch_size = <span class="number">8</span>)</span><br><span class="line"><span class="keyword">for</span> features,labels <span class="keyword">in</span> dl_train:</span><br><span class="line"> <span class="built_in">print</span>(features,labels)</span><br><span class="line"> <span class="keyword">break</span></span><br><span class="line"> </span><br><span class="line">out:</span><br><span class="line">tensor([[<span class="number">6.5000</span>, <span class="number">3.0000</span>, <span class="number">5.2000</span>, <span class="number">2.0000</span>],</span><br><span class="line">        [<span class="number">6.3000</span>, <span class="number">3.4000</span>, <span class="number">5.6000</span>, <span class="number">2.4000</span>],</span><br><span class="line">        [<span class="number">4.9000</span>, <span class="number">2.4000</span>, <span class="number">3.3000</span>, <span class="number">1.0000</span>],</span><br><span class="line">        [<span class="number">6.7000</span>, <span class="number">3.1000</span>, <span class="number">4.7000</span>, <span class="number">1.5000</span>],</span><br><span class="line">        [<span class="number">4.5000</span>, <span class="number">2.3000</span>, <span class="number">1.3000</span>, <span class="number">0.3000</span>],</span><br><span class="line">        [<span class="number">5.7000</span>, <span class="number">2.5000</span>, <span class="number">5.0000</span>, <span class="number">2.0000</span>],</span><br><span class="line">        [<span class="number">5.2000</span>, <span class="number">4.1000</span>, <span class="number">1.5000</span>, <span class="number">0.1000</span>],</span><br><span class="line">        [<span class="number">5.7000</span>, <span class="number">2.6000</span>, <span class="number">3.5000</span>, <span class="number">1.0000</span>]], dtype=torch.float64) tensor([<span class="number">2</span>, <span class="number">2</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">2</span>, <span class="number">0</span>, <span class="number">1</span>], dtype=torch.int32)</span><br><span class="line"> </span><br><span class="line"> </span><br><span class="line"> </span><br><span class="line"><span class="comment"># 演示加法运算符（`+`）的合并作用</span></span><br><span class="line">ds_data = ds_train + ds_valid</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;len(ds_train) = &#x27;</span>,<span class="built_in">len</span>(ds_train))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;len(ds_valid) = &#x27;</span>,<span class="built_in">len</span>(ds_valid))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;len(ds_train+ds_valid) = &#x27;</span>,<span class="built_in">len</span>(ds_data))</span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">type</span>(ds_data))</span><br><span class="line"> </span><br><span class="line">out:</span><br><span class="line"><span class="built_in">len</span>(ds_train) =  <span class="number">120</span></span><br><span class="line"><span class="built_in">len</span>(ds_valid) =  <span class="number">30</span></span><br><span class="line"><span class="built_in">len</span>(ds_train+ds_valid) =  <span class="number">150</span></span><br><span class="line">&lt;<span class="keyword">class</span><span class="string">&#x27;torch.utils.data.dataset.ConcatDataset&#x27;</span>&gt;</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#4-2-2  根据图片目录创建图片数据集</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> torch </span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> DataLoader</span><br><span class="line"><span class="keyword">from</span> torchvision <span class="keyword">import</span> transforms,datasets</span><br><span class="line"> </span><br><span class="line"><span class="comment">#演示一些常用的图片增强操作</span></span><br><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image</span><br><span class="line">img = Image.<span class="built_in">open</span>(<span class="string">&#x27;./data/dog2.jpg&#x27;</span>)</span><br><span class="line">img</span><br><span class="line"> </span><br><span class="line"><span class="comment"># 随机数值翻转</span></span><br><span class="line">transforms.RandomVerticalFlip()(img)</span><br><span class="line"> </span><br><span class="line"><span class="comment">#随机旋转</span></span><br><span class="line">transforms.RandomRotation(<span class="number">45</span>)(img)</span><br><span class="line"> </span><br><span class="line"><span class="comment"># 定义图片增强操作</span></span><br><span class="line">transform_train = transforms.Compose([</span><br><span class="line">	 transforms.RandomHorizontalFlip(), <span class="comment">#随机水平翻转</span></span><br><span class="line">	 transforms.RandomVerticalFlip(), <span class="comment">#随机垂直翻转</span></span><br><span class="line">	 transforms.RandomRotation(<span class="number">45</span>), <span class="comment">#随机在45度角度内旋转</span></span><br><span class="line">	 transforms.ToTensor() <span class="comment">#转换成张量</span></span><br><span class="line">	 ]</span><br><span class="line">)</span><br><span class="line">transform_valid = transforms.Compose([</span><br><span class="line">	 transforms.ToTensor()</span><br><span class="line">	 ]</span><br><span class="line">)</span><br><span class="line"> </span><br><span class="line"><span class="comment"># 根据图片目录创建数据集</span></span><br><span class="line"><span class="comment"># 这里用到的animal数据集是我自己整理的，链接在文章末尾</span></span><br><span class="line"><span class="comment">#注意这里要在train 和  test 目录下按照图片类别分别新建文件夹，文件夹的名称就是类别名，然后把图片分别放入各个文件夹</span></span><br><span class="line">ds_train = datasets.ImageFolder(<span class="string">&quot;data/animal/train/&quot;</span>, transform = transform_train,target_transform= <span class="keyword">lambda</span> t:torch.tensor([t]).<span class="built_in">float</span>())</span><br><span class="line">ds_valid = datasets.ImageFolder(<span class="string">&quot;data/animal/test/&quot;</span>, transform = transform_valid,target_transform= <span class="keyword">lambda</span> t:torch.tensor([t]).<span class="built_in">float</span>())</span><br><span class="line"><span class="built_in">print</span>(ds_train.class_to_idx)</span><br><span class="line"> </span><br><span class="line"><span class="comment"># 使用DataLoader加载数据集</span></span><br><span class="line">dl_train = DataLoader(ds_train,batch_size = <span class="number">2</span>,shuffle = <span class="literal">True</span>,num_workers=<span class="number">1</span>)</span><br><span class="line">dl_valid = DataLoader(ds_valid,batch_size = <span class="number">2</span>,shuffle = <span class="literal">True</span>,num_workers=<span class="number">1</span>)</span><br><span class="line"> </span><br><span class="line"><span class="keyword">for</span> features,labels <span class="keyword">in</span> dl_train:</span><br><span class="line">    <span class="built_in">print</span>(features)</span><br><span class="line">    <span class="built_in">print</span>(labels)</span><br><span class="line">    <span class="keyword">break</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#例4-2-3  创建自定义数据集 </span></span><br><span class="line"><span class="comment">#下面通过继承Dataset类创建douban文本分类任务的自定义数据集。 douban数据集链接在文章末尾。</span></span><br><span class="line"><span class="comment">#大概思路如下：首先，对训练集文本分词构建词典。然后将训练集文本和测试集文本数据转换成 token单词编码。 接着将转换成单词编码的训练集数据和测试集数据按样本分割成多个文件，一个文件代表一个样本。 最后，我们可以根据文件名列表获取对应序号的样本内容，从而构建Dataset数据集。</span></span><br><span class="line"> </span><br><span class="line"> </span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> OrderedDict</span><br><span class="line"><span class="keyword">import</span> re,string,jieba,csv</span><br><span class="line"> </span><br><span class="line"><span class="comment">#from keras.datasets import imdb</span></span><br><span class="line"><span class="comment">#(train_data, train_labels), (test_data, test_labels) = imdb.load_data(num_words=10000)</span></span><br><span class="line"> </span><br><span class="line">MAX_WORDS = <span class="number">10000</span> <span class="comment"># 仅考虑最高频的10000个词</span></span><br><span class="line">MAX_LEN = <span class="number">200</span> <span class="comment"># 每个样本保留200个词的长度</span></span><br><span class="line">BATCH_SIZE = <span class="number">20</span></span><br><span class="line">train_data_path = <span class="string">&#x27;data/douban/train.csv&#x27;</span></span><br><span class="line">test_data_path = <span class="string">&#x27;data/douban/test.csv&#x27;</span></span><br><span class="line">train_token_path = <span class="string">&#x27;data/douban/train_token.csv&#x27;</span></span><br><span class="line">test_token_path = <span class="string">&#x27;data/douban/test_token.csv&#x27;</span></span><br><span class="line">train_samples_path = <span class="string">&#x27;data/douban/train_samples/&#x27;</span></span><br><span class="line">test_samples_path = <span class="string">&#x27;data/douban/test_samples/&#x27;</span></span><br><span class="line"> </span><br><span class="line"><span class="comment">#print(train_data[0])</span></span><br><span class="line"> </span><br><span class="line"> </span><br><span class="line"><span class="comment">##构建词典</span></span><br><span class="line">word_count_dict = &#123;&#125;</span><br><span class="line"><span class="comment">#清洗文本</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">clean_text</span>(<span class="params">text</span>):</span><br><span class="line">    bd=<span class="string">&#x27;[’!&quot;#$%&amp;\&#x27;()*+,-./:;&lt;=&gt;?@[\\]^_`&#123;|&#125;~]+，。！？“”《》：、． &#x27;</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> bd:</span><br><span class="line">        text=text.replace(i,<span class="string">&#x27;&#x27;</span>)   <span class="comment">#字符串替换去标点符号</span></span><br><span class="line">    fenci=jieba.lcut(text)</span><br><span class="line">    <span class="keyword">return</span> fenci</span><br><span class="line"> </span><br><span class="line"><span class="keyword">with</span> <span class="built_in">open</span>(train_data_path,<span class="string">&quot;r&quot;</span>,encoding = <span class="string">&#x27;utf-8&#x27;</span>,newline=<span class="string">&#x27;&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">    reader = csv.reader(f,delimiter=<span class="string">&#x27;,&#x27;</span>)</span><br><span class="line">    <span class="keyword">for</span> row <span class="keyword">in</span> reader:</span><br><span class="line">        <span class="comment">#print(row)</span></span><br><span class="line">        text = row[<span class="number">1</span>]</span><br><span class="line">        label = row[<span class="number">0</span>]</span><br><span class="line">        <span class="comment">#print(label,text)</span></span><br><span class="line">        cleaned_text = clean_text(text)</span><br><span class="line">        <span class="keyword">for</span> word <span class="keyword">in</span> cleaned_text:</span><br><span class="line">            <span class="comment">#print(word)</span></span><br><span class="line">            word_count_dict[word] = word_count_dict.get(word,<span class="number">0</span>)+<span class="number">1</span></span><br><span class="line"> </span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">len</span>(word_count_dict))</span><br><span class="line"> </span><br><span class="line">df_word_dict = pd.DataFrame(pd.Series(word_count_dict,name = <span class="string">&quot;count&quot;</span>))</span><br><span class="line">df_word_dict = df_word_dict.sort_values(by = <span class="string">&quot;count&quot;</span>,ascending =<span class="literal">False</span>)</span><br><span class="line">df_word_dict = df_word_dict[<span class="number">0</span>:MAX_WORDS-<span class="number">2</span>] <span class="comment"># </span></span><br><span class="line">df_word_dict[<span class="string">&quot;word_id&quot;</span>] = <span class="built_in">range</span>(<span class="number">2</span>,MAX_WORDS) <span class="comment">#编号0和1分别留给未知词&lt;unkown&gt;和填充&lt;padding&gt;</span></span><br><span class="line">word_id_dict = df_word_dict[<span class="string">&quot;word_id&quot;</span>].to_dict()</span><br><span class="line">df_word_dict.head(<span class="number">10</span>)</span><br><span class="line"> </span><br><span class="line">out:</span><br><span class="line">    count	word_id</span><br><span class="line">的	<span class="number">68229</span>	<span class="number">2</span></span><br><span class="line">了	<span class="number">20591</span>	<span class="number">3</span></span><br><span class="line">是	<span class="number">15321</span>	<span class="number">4</span></span><br><span class="line">我	<span class="number">9312</span>	<span class="number">5</span></span><br><span class="line">看	<span class="number">7423</span>	<span class="number">6</span></span><br><span class="line">很	<span class="number">7395</span>	<span class="number">7</span></span><br><span class="line">也	<span class="number">7256</span>	<span class="number">8</span></span><br><span class="line">都	<span class="number">7053</span>	<span class="number">9</span></span><br><span class="line">在	<span class="number">6753</span>	<span class="number">10</span></span><br><span class="line">和	<span class="number">6388</span>	<span class="number">11</span></span><br><span class="line"> </span><br><span class="line"> </span><br><span class="line"><span class="comment">#转换token</span></span><br><span class="line"><span class="comment"># 填充文本</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">pad</span>(<span class="params">data_list,pad_length</span>):</span><br><span class="line">    padded_list = data_list.copy()</span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">len</span>(data_list)&gt; pad_length:</span><br><span class="line">        padded_list = data_list[-pad_length:]</span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">len</span>(data_list)&lt; pad_length:</span><br><span class="line">        padded_list = [<span class="number">1</span>]*(pad_length-<span class="built_in">len</span>(data_list))+data_list</span><br><span class="line">    <span class="keyword">return</span> padded_list</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">text_to_token</span>(<span class="params">text_file,token_file</span>):</span><br><span class="line">    <span class="keyword">with</span> <span class="built_in">open</span>(train_data_path,<span class="string">&quot;r&quot;</span>,encoding = <span class="string">&#x27;utf-8&#x27;</span>,newline=<span class="string">&#x27;&#x27;</span>) <span class="keyword">as</span> f,\</span><br><span class="line">        <span class="built_in">open</span>(token_file,<span class="string">&quot;w&quot;</span>,encoding = <span class="string">&#x27;utf-8&#x27;</span>) <span class="keyword">as</span> fout:</span><br><span class="line">        reader = csv.reader(f,delimiter=<span class="string">&#x27;,&#x27;</span>)</span><br><span class="line">        <span class="keyword">for</span> row <span class="keyword">in</span> reader:</span><br><span class="line">            text = row[<span class="number">1</span>]</span><br><span class="line">            label = row[<span class="number">0</span>]</span><br><span class="line">            cleaned_text = clean_text(text)</span><br><span class="line">            word_token_list = [word_id_dict.get(word, <span class="number">0</span>) <span class="keyword">for</span> word <span class="keyword">in</span> cleaned_text]</span><br><span class="line">            pad_list = pad(word_token_list,MAX_LEN)</span><br><span class="line">            out_line = label+<span class="string">&quot;\t&quot;</span>+<span class="string">&quot; &quot;</span>.join([<span class="built_in">str</span>(x) <span class="keyword">for</span> x <span class="keyword">in</span> pad_list])</span><br><span class="line">            fout.write(out_line+<span class="string">&quot;\n&quot;</span>)</span><br><span class="line">text_to_token(train_data_path,train_token_path)</span><br><span class="line">text_to_token(test_data_path,test_token_path)</span><br><span class="line"> </span><br><span class="line"> </span><br><span class="line"> </span><br><span class="line"><span class="comment"># 分割样本</span></span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">if</span> <span class="keyword">not</span> os.path.exists(train_samples_path):</span><br><span class="line">    os.mkdir(train_samples_path)</span><br><span class="line"><span class="keyword">if</span> <span class="keyword">not</span> os.path.exists(test_samples_path):</span><br><span class="line">    os.mkdir(test_samples_path)</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">split_samples</span>(<span class="params">token_path,samples_dir</span>):</span><br><span class="line">    <span class="keyword">with</span> <span class="built_in">open</span>(token_path,<span class="string">&quot;r&quot;</span>,encoding = <span class="string">&#x27;utf-8&#x27;</span>) <span class="keyword">as</span> fin:</span><br><span class="line">        i = <span class="number">0</span></span><br><span class="line">        <span class="keyword">for</span> line <span class="keyword">in</span> fin:</span><br><span class="line">            <span class="keyword">with</span> <span class="built_in">open</span>(samples_dir+<span class="string">&quot;%d.txt&quot;</span>%i,<span class="string">&quot;w&quot;</span>,encoding = <span class="string">&quot;utf-8&quot;</span>) <span class="keyword">as</span> fout:</span><br><span class="line">                fout.write(line)</span><br><span class="line">            i = i+<span class="number">1</span></span><br><span class="line">split_samples(train_token_path,train_samples_path)</span><br><span class="line">split_samples(test_token_path,test_samples_path)</span><br><span class="line"> </span><br><span class="line"> </span><br><span class="line"> </span><br><span class="line"> </span><br><span class="line"> </span><br><span class="line"><span class="comment">#创建数据集</span></span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> DataLoader,Dataset</span><br><span class="line"><span class="keyword">from</span> torchvision <span class="keyword">import</span> transforms,datasets</span><br><span class="line"><span class="keyword">class</span> <span class="title class_">imdbDataset</span>(<span class="title class_ inherited__">Dataset</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self,samples_dir</span>):</span><br><span class="line">        self.samples_dir = samples_dir</span><br><span class="line">        self.samples_paths = os.listdir(samples_dir)</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__len__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">len</span>(self.samples_paths)</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__getitem__</span>(<span class="params">self,index</span>):</span><br><span class="line">        path = self.samples_dir + self.samples_paths[index]</span><br><span class="line">        <span class="keyword">with</span> <span class="built_in">open</span>(path,<span class="string">&quot;r&quot;</span>,encoding = <span class="string">&quot;utf-8&quot;</span>) <span class="keyword">as</span> f:</span><br><span class="line">            line = f.readline()</span><br><span class="line">            label,tokens = line.split(<span class="string">&quot;\t&quot;</span>)</span><br><span class="line">            label = torch.tensor([<span class="built_in">float</span>(label)],dtype = torch.<span class="built_in">float</span>)</span><br><span class="line">            feature = torch.tensor([<span class="built_in">int</span>(x) <span class="keyword">for</span> x <span class="keyword">in</span> tokens.split(<span class="string">&quot; &quot;</span>)],dtype = torch.long)</span><br><span class="line">            <span class="keyword">return</span> (feature,label)</span><br><span class="line">ds_train = imdbDataset(train_samples_path)</span><br><span class="line">ds_test = imdbDataset(test_samples_path)</span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">len</span>(ds_train))</span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">len</span>(ds_test))</span><br><span class="line"> </span><br><span class="line"> </span><br><span class="line">dl_train = DataLoader(ds_train,batch_size = BATCH_SIZE,shuffle = <span class="literal">True</span>,num_workers=<span class="number">4</span>)</span><br><span class="line">dl_test = DataLoader(ds_test,batch_size = BATCH_SIZE,num_workers=<span class="number">4</span>)</span><br><span class="line"><span class="keyword">for</span> features,labels <span class="keyword">in</span> dl_train:</span><br><span class="line">    <span class="built_in">print</span>(features)</span><br><span class="line">    <span class="built_in">print</span>(labels)</span><br><span class="line">    <span class="keyword">break</span></span><br><span class="line"> </span><br><span class="line">out:</span><br><span class="line"> </span><br><span class="line"> </span><br><span class="line"> </span><br><span class="line"><span class="comment">#创建模型</span></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">import</span> importlib</span><br><span class="line"><span class="keyword">from</span> torchkeras <span class="keyword">import</span> Model,summary</span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Net</span>(<span class="title class_ inherited__">Model</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(Net, self).__init__()   <span class="comment">#设置padding_idx参数后将在训练过程中将填充的token始终赋值为0向量</span></span><br><span class="line">        self.embedding = nn.Embedding(num_embeddings = MAX_WORDS,embedding_dim = <span class="number">3</span>,padding_idx = <span class="number">1</span>)</span><br><span class="line">        self.conv = nn.Sequential()</span><br><span class="line">        self.conv.add_module(<span class="string">&quot;conv_1&quot;</span>,nn.Conv1d(in_channels = <span class="number">3</span>,out_channels = <span class="number">16</span>,kernel_size = <span class="number">5</span>))</span><br><span class="line">        self.conv.add_module(<span class="string">&quot;pool_1&quot;</span>,nn.MaxPool1d(kernel_size = <span class="number">2</span>))</span><br><span class="line">        self.conv.add_module(<span class="string">&quot;relu_1&quot;</span>,nn.ReLU())</span><br><span class="line">        self.conv.add_module(<span class="string">&quot;conv_2&quot;</span>,nn.Conv1d(in_channels = <span class="number">16</span>,out_channels = <span class="number">128</span>,kernel_size = <span class="number">2</span>))</span><br><span class="line">        self.conv.add_module(<span class="string">&quot;pool_2&quot;</span>,nn.MaxPool1d(kernel_size = <span class="number">2</span>))</span><br><span class="line">        self.conv.add_module(<span class="string">&quot;relu_2&quot;</span>,nn.ReLU())</span><br><span class="line">        self.dense = nn.Sequential()</span><br><span class="line">        self.dense.add_module(<span class="string">&quot;flatten&quot;</span>,nn.Flatten())</span><br><span class="line">        self.dense.add_module(<span class="string">&quot;linear&quot;</span>,nn.Linear(<span class="number">6144</span>,<span class="number">1</span>))</span><br><span class="line">        self.dense.add_module(<span class="string">&quot;sigmoid&quot;</span>,nn.Sigmoid())</span><br><span class="line">        <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self,x</span>):</span><br><span class="line">            x = self.embedding(x).transpose(<span class="number">1</span>,<span class="number">2</span>)</span><br><span class="line">            x = self.conv(x)</span><br><span class="line">            y = self.dense(x)</span><br><span class="line">            <span class="keyword">return</span> y</span><br><span class="line">model = Net()</span><br><span class="line"><span class="built_in">print</span>(model)</span><br><span class="line">model.summary(input_shape = (<span class="number">200</span>,),input_dtype = torch.LongTensor)</span><br><span class="line"> </span><br><span class="line"> </span><br><span class="line"> </span><br><span class="line"><span class="comment"># 编译模型</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">accuracy</span>(<span class="params">y_pred,y_true</span>):</span><br><span class="line">    y_pred = torch.where(y_pred&gt;<span class="number">0.5</span>,torch.ones_like(y_pred,dtype = torch.float32),torch.zeros_like(y_pred,dtype = torch.float32))</span><br><span class="line">    acc = torch.mean(<span class="number">1</span>-torch.<span class="built_in">abs</span>(y_true-y_pred))</span><br><span class="line">    <span class="keyword">return</span> acc</span><br><span class="line">model.<span class="built_in">compile</span>(loss_func = nn.BCELoss(),optimizer=</span><br><span class="line">torch.optim.Adagrad(model.parameters(),lr = <span class="number">0.02</span>),</span><br><span class="line">metrics_dict=&#123;<span class="string">&quot;accuracy&quot;</span>:accuracy&#125;)</span><br><span class="line"><span class="comment"># 训练模型</span></span><br><span class="line">dfhistory = model.fit(<span class="number">10</span>,dl_train,dl_val=dl_test,log_step_freq= <span class="number">200</span>)</span><br></pre></td></tr></table></figure>
<h5 id="使用dataloader加载数据集">4.3 使用DataLoader加载数据集</h5>
<p>DataLoader能够控制batch的大小，batch中元素的采样方法，以及将batch结果整理成模型所需输入形式的方法，并且能够使用多进程读取数据。 <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">DataLoader(</span><br><span class="line"> dataset,</span><br><span class="line"> batch_size=<span class="number">1</span>,</span><br><span class="line"> shuffle=<span class="literal">False</span>,</span><br><span class="line"> sampler=<span class="literal">None</span>,</span><br><span class="line"> batch_sampler=<span class="literal">None</span>,</span><br><span class="line"> num_workers=<span class="number">0</span>,</span><br><span class="line"> collate_fn=<span class="literal">None</span>,</span><br><span class="line"> pin_memory=<span class="literal">False</span>,</span><br><span class="line"> drop_last=<span class="literal">False</span>,</span><br><span class="line"> timeout=<span class="number">0</span>,</span><br><span class="line"> worker_init_fn=<span class="literal">None</span>,</span><br><span class="line"> multiprocessing_context=<span class="literal">None</span>,</span><br><span class="line">)</span><br></pre></td></tr></table></figure> 一般情况下，我们仅仅会配置<code>dataset, batch_size, shuffle, num_workers, drop_last</code>这五个参 数，其他参数使用默认值即可。</p>
<ul>
<li><strong><code>dataset</code></strong> : 数据集</li>
<li><strong><code>batch_size</code></strong>: 批次大小</li>
<li><strong><code>shuffle</code></strong>: 是否乱序</li>
<li><code>sampler</code>: 样本采样函数，一般无需设置。</li>
<li><code>batch_sampler</code>: 批次采样函数，一般无需设置。</li>
<li><code>num_workers</code>: 使用多进程读取数据，设置的进程数。</li>
<li><code>collate_fn</code>: 整理一个批次数据的函数。</li>
<li><code>pin_memory</code>: 是否设置为锁业内存。默认为<code>False</code>，锁业内存不会使用虚拟内存(硬盘)，从锁 业内存拷贝到GPU上速度会更快。</li>
<li><strong><code>drop_last</code></strong>: 是否丢弃最后一个样本数量不足<code>batch_size</code>批次数据。</li>
<li><code>timeout</code>: 加载一个数据批次的最长等待时间，一般无需设置。</li>
<li><code>worker_init_fn</code>: 每个<code>worker</code>中<code>dataset</code>的初始化函数，常用于 <code>terableDataset</code>。一般不使用。</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#例3-3 使用DataLoader加载数据集</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> torch </span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> DataLoader,TensorDataset,Dataset</span><br><span class="line"><span class="keyword">from</span> torchvision <span class="keyword">import</span> transforms,datasets</span><br><span class="line"><span class="comment">#构建输入数据管道</span></span><br><span class="line">ds = TensorDataset(torch.arange(<span class="number">1</span>,<span class="number">50</span>))</span><br><span class="line">dl = DataLoader(ds,</span><br><span class="line"> batch_size = <span class="number">10</span>,</span><br><span class="line"> shuffle= <span class="literal">True</span>,</span><br><span class="line"> num_workers=<span class="number">2</span>,</span><br><span class="line"> drop_last = <span class="literal">True</span>)</span><br><span class="line"><span class="comment">#迭代数据</span></span><br><span class="line"><span class="keyword">for</span> batch, <span class="keyword">in</span> dl:</span><br><span class="line">    <span class="built_in">print</span>(batch)</span><br><span class="line"> </span><br><span class="line">out:</span><br><span class="line">tensor([<span class="number">35</span>, <span class="number">19</span>,  <span class="number">3</span>,  <span class="number">1</span>, <span class="number">24</span>, <span class="number">20</span>,  <span class="number">8</span>, <span class="number">37</span>, <span class="number">32</span>, <span class="number">38</span>])</span><br><span class="line">tensor([<span class="number">28</span>, <span class="number">26</span>,  <span class="number">7</span>, <span class="number">48</span>,  <span class="number">4</span>, <span class="number">41</span>, <span class="number">15</span>, <span class="number">45</span>, <span class="number">11</span>, <span class="number">14</span>])</span><br><span class="line">tensor([<span class="number">23</span>,  <span class="number">5</span>, <span class="number">10</span>,  <span class="number">6</span>, <span class="number">18</span>, <span class="number">39</span>, <span class="number">31</span>, <span class="number">22</span>, <span class="number">42</span>, <span class="number">12</span>])</span><br><span class="line">tensor([<span class="number">34</span>, <span class="number">47</span>, <span class="number">30</span>, <span class="number">25</span>, <span class="number">29</span>, <span class="number">49</span>, <span class="number">44</span>, <span class="number">46</span>, <span class="number">33</span>, <span class="number">13</span>])</span><br></pre></td></tr></table></figure>
<h4 id="数据的预处理模块">5. 数据的预处理模块</h4>
<p>transforms是pytorch中常用的图像预处理方法，这个在torchvision计算机视觉工具包中。在安装pytorch时顺便安装了torchvision，在torchvision中，有三个主要的模块：</p>
<ul>
<li><code>torchvision.transforms</code>:常用的图像预处理方法，比如：标准化、中心化、旋转、翻转等；</li>
<li><code>torchvision.datasets</code>:常用的数据集的<code>dataset</code>实现，例如：MNIST、CIFAR-10、ImageNet等；</li>
<li><code>torchvision.models</code>:常用的预训练模型，AlexNet、VGG、ResNet等。</li>
</ul>
<h5 id="裁剪">5.1 裁剪</h5>
<h6 id="随机裁剪transforms.randomcrop">5.1.1 随机裁剪：transforms.RandomCrop</h6>
<p>该函数根据给定的size进行随机裁剪 <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">transforms.RandomCrop(</span><br><span class="line">    size,</span><br><span class="line">    padding=<span class="literal">None</span>,</span><br><span class="line">    pad_if_needed=<span class="literal">False</span>,</span><br><span class="line">    fill=<span class="number">0</span>,</span><br><span class="line">    padding_mode=<span class="string">&#x27;constant&#x27;</span>,</span><br><span class="line">)</span><br></pre></td></tr></table></figure> - `size·：可为sequence or int，若为sequence，则为（h， w），若为int，则为（int， int);</p>
<ul>
<li><p>`padding·：可为int or sequence，此参数是设置填充多少个pixel；若为int，表示图像上下左右均填充int个pixel，例如padding=4，表示图像上下左右均填充4个pixel，若为32×32，则图像填充后为40×40；若为sequence，若为2个数，第一个数表示左右填充多少，第二个数表示上下填充多少；当有四个数时表示左、上、右、下</p></li>
<li><p>·pad_if_needed·：若图像小于设定的size，则填充；</p></li>
<li><p>fill：表示需要填充的值，默认为0.当值为int时，表示各通道均填充该值，当值为3时，表示RGB三个通道各需要填充的值；</p></li>
<li><code>padding_mode</code>：填充模式，有4中填充模式：
<ul>
<li>1、<code>constant</code>：常数填充；</li>
<li>2、<code>edge</code>：图像的边缘值填充；</li>
<li>3、<code>reflect</code>：镜像填充，最后一个像素不镜像，例如 [1, 2, 3, 4]. -&gt; [3, 2, 1, 2, 3, 4, 3, 2]；</li>
<li>4、<code>symmetric</code>:镜像填充，最后一个元素填充，例如：[1, 2, 3, 4] -&gt; [2, 1, 1, 2, 3, 4, 4, 3]</li>
</ul></li>
</ul>
<h6 id="中心裁剪transforms.centercrop">5.1.2 中心裁剪transforms.CenterCrop</h6>
<p>依据给定的参数进行中心裁剪 <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">torchvision.transforms.CenterCrop(size)</span><br><span class="line"></span><br><span class="line">size：若为sequence，则为（h, w）, 若为<span class="built_in">int</span>， 则为（<span class="built_in">int</span>， <span class="built_in">int</span>）</span><br></pre></td></tr></table></figure></p>
<h6 id="随机长宽比裁剪transforms.randomresizedcrop">5.1.3 随机长宽比裁剪transforms.RandomResizedCrop()</h6>
<p>随机大小，随机长宽比裁剪原始图片，最后将图片 resize 到设定好的 size <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">torchvision.transforms.RandomResizedCrop(</span><br><span class="line">    size,</span><br><span class="line">    scale=(<span class="number">0.08</span>, <span class="number">1.0</span>),</span><br><span class="line">    ratio=(<span class="number">0.75</span>, <span class="number">1.3333333333333333</span>),</span><br><span class="line">    interpolation=<span class="number">2</span>,</span><br><span class="line">)</span><br></pre></td></tr></table></figure> - <code>size</code>：所需裁减图片尺寸 - <code>scale</code>：随机 crop 的大小区间，如 scale=(0.08, 1.0)，表示随机 crop 出来的图片会在的 0.08 倍至 1 倍之间。 - <code>ratio</code>： 随机长宽比设置</p>
<h6 id="上下左右中心裁剪transforms.fivecrop">5.1.4 上下左右中心裁剪transforms.FiveCrop()</h6>
<p>对图片进行上下左右以及中心裁剪，获得 5 张图片，返回一个 4D-tensor <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torchvision.transforms.FiveCrop(size)</span><br></pre></td></tr></table></figure></p>
<h5 id="翻转和旋转">5.2 翻转和旋转</h5>
<h6 id="翻转">5.2.1 翻转</h6>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#依据概率 p 对 PIL 图片进行水平翻转</span></span><br><span class="line">torchvision.transforms.RandomHorizontalFlip(p=<span class="number">0.5</span>)</span><br><span class="line"><span class="comment">#依概率p垂直翻转</span></span><br><span class="line">torchvision.transforms.RandomVerticalFlip(p=<span class="number">0.5</span>)</span><br><span class="line">p为概率值</span><br></pre></td></tr></table></figure>
<h6 id="旋转">5.2.2 旋转</h6>
<p>依 degrees 随机旋转一定角度 <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">torchvision.transforms.RandomRotation(degrees, resample=<span class="literal">False</span>, </span><br><span class="line">                                      expand=<span class="literal">False</span>, center=<span class="literal">None</span>)</span><br></pre></td></tr></table></figure> - <code>degress</code>：(sequence or float or int) ，若为单个数，如 30，则表示在（-30，+30）之间随机旋转；若为sequence，如(30，60)，则表示在 30-60 度之间随机旋转； - <code>resample</code>：重采样方法选择，可选 PIL.Image.NEAREST, PIL.Image.BILINEAR,PIL.Image.BICUBIC，默认为最近邻; - <code>expand</code>: 是否扩大图片，以保持原图信息； - <code>center</code>: 设置旋转点，默认是中心旋转</p>
<h5 id="图像变换">5.3 图像变换</h5>
<h6 id="resize">5.3.1 resize</h6>
<p>重置图像分辨率 <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torchvision.transforms.Resize(size, interpolation=<span class="number">2</span>)</span><br></pre></td></tr></table></figure></p>
<h6 id="标准化">5.3.2 标准化</h6>
<p>对数据按通道进行标准化，即先减均值，再除以标准差，注意是 hwc <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torchvision.transforms.Normalize(mean, std)</span><br></pre></td></tr></table></figure></p>
<h6 id="转化为tensortransforms.totensor">5.3.3 转化为Tensor:transforms.ToTensor</h6>
<p>将 PIL Image 或者 ndarray 转换为 tensor，并且归一化至[0-1]。注意归一化至[0-1]是直接除以 255，若自己的 ndarray 数据尺度有变化，则需要自行修改。 <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torchvision.transforms.ToTensor()</span><br></pre></td></tr></table></figure> ###### 5.3.4 例子 data_transforms是一个字典，其指定了所有图像训练集和检验集预处理操作 <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#transfomrs.Compose()表示按顺序执行</span></span><br><span class="line">data_transforms = &#123;</span><br><span class="line">    <span class="string">&#x27;train&#x27;</span>: </span><br><span class="line">        transforms.Compose([</span><br><span class="line">        transforms.Resize([<span class="number">96</span>, <span class="number">96</span>]),</span><br><span class="line">        transforms.RandomRotation(<span class="number">45</span>),<span class="comment">#随机旋转，-45到45度之间随机选</span></span><br><span class="line">        transforms.CenterCrop(<span class="number">64</span>),<span class="comment">#从中心开始裁剪</span></span><br><span class="line">        transforms.RandomHorizontalFlip(p=<span class="number">0.5</span>),<span class="comment">#随机水平翻转 选择一个概率概率</span></span><br><span class="line">        transforms.RandomVerticalFlip(p=<span class="number">0.5</span>),<span class="comment">#随机垂直翻转</span></span><br><span class="line">        transforms.ColorJitter(brightness=<span class="number">0.2</span>, contrast=<span class="number">0.1</span>, saturation=<span class="number">0.1</span>, hue=<span class="number">0.1</span>),<span class="comment">#参数1为亮度，参数2为对比度，参数3为饱和度，参数4为色相</span></span><br><span class="line">        transforms.RandomGrayscale(p=<span class="number">0.025</span>),<span class="comment">#概率转换成灰度率，3通道就是R=G=B</span></span><br><span class="line">        transforms.ToTensor(),</span><br><span class="line">        transforms.Normalize([<span class="number">0.485</span>, <span class="number">0.456</span>, <span class="number">0.406</span>], [<span class="number">0.229</span>, <span class="number">0.224</span>, <span class="number">0.225</span>])<span class="comment">#均值，标准差</span></span><br><span class="line">    ]),</span><br><span class="line">    <span class="string">&#x27;valid&#x27;</span>: </span><br><span class="line">        transforms.Compose([</span><br><span class="line">        transforms.Resize([<span class="number">64</span>, <span class="number">64</span>]),</span><br><span class="line">        transforms.ToTensor(),</span><br><span class="line">        transforms.Normalize([<span class="number">0.485</span>, <span class="number">0.456</span>, <span class="number">0.406</span>], [<span class="number">0.229</span>, <span class="number">0.224</span>, <span class="number">0.225</span>])</span><br><span class="line">    ]),</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">#使用datasets.ImageFloder()读取数据</span></span><br><span class="line">image_datasets = &#123;x: datasets.ImageFolder(os.path.join(data_dir, x), data_transforms[x]) <span class="keyword">for</span> x <span class="keyword">in</span> [<span class="string">&#x27;train&#x27;</span>, <span class="string">&#x27;valid&#x27;</span>]&#125;</span><br></pre></td></tr></table></figure></p>
<h4 id="pytorch的模型搭建torch.nn模块">6. pytorch的模型搭建torch.nn模块</h4>
<p>机器学习的五大板块之一就是模型，好的模型事半功倍。</p>
<h5 id="nn.functional-和-nn.module">6.1 nn.functional 和 nn.Module</h5>
<p>Pytorch和神经网络相关的功能组件大多都封装在 torch.nn模块下。 这些功能组件的绝大部分既有函数形式<code>nn.funtional</code>实现，也有类形式<code>nn.Module</code>实现。</p>
<p>其中nn.functional(一般引入后改名为F)有各种功能组件的函数实现,例如: - <strong>激活函数</strong>: <code>F.relu 、F.sigmoid 、F.tanh 、F.softmax</code></p>
<ul>
<li><p><strong>模型层</strong>： <code>F.linear(全连接)、F.conv2d(2d卷积)、F.max_pool2d(2d最大池化)、F.dropout2d、 F.embedding</code></p></li>
<li><p><strong>损失函数</strong>：<code>F.binary_cross_entropy 、F.mse_loss 、F.cross_entropy(交叉熵损失函数)</code></p></li>
</ul>
<p><strong>但为了进一步便于对参数进行管理，一般通过继承 nn.Module 转换成为类的实现形式，并直接封装在 nn 模块下，例如：</strong></p>
<ul>
<li><p><strong>激活函数</strong>： <code>nn.ReLU 、 nn.Sigmoid 、 nn.Tanh 、nn.Softmax</code></p></li>
<li><p><strong>模型层</strong>： <code>nn.Linear 、nn.Conv2d 、nn.MaxPool2d 、nn.Dropout2d 、nn.Embedding</code></p></li>
<li><p><strong>损失函数</strong>：<code>nn.BCELoss 、 nn.MSELoss 、 nn.CrossEntropyLoss</code></p></li>
</ul>
<p>实际上nn.Module除了可以管理其引用的各种参数，还可以管理其引用的子模块，功能十分强大</p>
<h5 id="使用nn.module来管理参数">6.2 使用nn.Module来管理参数</h5>
<p>在Pytorch中，模型的参数是需要被优化器训练的，因此，通常要设置参数为<code>requires_grad = True</code>的张量。 同时，在一个模型中，往往有许多的参数，要手动管理这些参数并不是一件容易的事情。 Pytorch一般将参数用<code>nn.Parameter</code>来表示，并且用<code>nn.Module</code>来管理其结构下的所有参数。 <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#模型，模型自动生成权值矩阵\卷积核</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">CNN</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(CNN, self).__init__()</span><br><span class="line">        self.conv1 = nn.Sequential(         <span class="comment"># 输入大小 (1, 28, 28)</span></span><br><span class="line">            nn.Conv2d(</span><br><span class="line">                in_channels=<span class="number">1</span>,              <span class="comment"># 灰度图</span></span><br><span class="line">                out_channels=<span class="number">16</span>,            <span class="comment"># 要得到几多少个特征图</span></span><br><span class="line">                kernel_size=<span class="number">5</span>,              <span class="comment"># 卷积核大小</span></span><br><span class="line">                stride=<span class="number">1</span>,                   <span class="comment"># 步长</span></span><br><span class="line">                padding=<span class="number">2</span>,                  <span class="comment"># 如果希望卷积后大小跟原来一样，需要设置padding=(kernel_size-1)/2 if stride=1</span></span><br><span class="line">            ),                              <span class="comment"># 输出的特征图为 (16, 28, 28)</span></span><br><span class="line">            nn.ReLU(),                      <span class="comment"># relu层</span></span><br><span class="line">            nn.MaxPool2d(kernel_size=<span class="number">2</span>),    <span class="comment"># 进行池化操作（2x2 区域）, 输出结果为： (16, 14, 14)</span></span><br><span class="line">        )</span><br><span class="line">        self.conv2 = nn.Sequential(         <span class="comment"># 下一个套餐的输入 (16, 14, 14)</span></span><br><span class="line">            nn.Conv2d(<span class="number">16</span>, <span class="number">32</span>, <span class="number">5</span>, <span class="number">1</span>, <span class="number">2</span>),     <span class="comment"># 输出 (32, 14, 14)</span></span><br><span class="line">            nn.ReLU(),                      <span class="comment"># relu层</span></span><br><span class="line">            nn.Conv2d(<span class="number">32</span>, <span class="number">32</span>, <span class="number">5</span>, <span class="number">1</span>, <span class="number">2</span>),</span><br><span class="line">            nn.ReLU(),</span><br><span class="line">            nn.MaxPool2d(<span class="number">2</span>),                <span class="comment"># 输出 (32, 7, 7)</span></span><br><span class="line">        )</span><br><span class="line">        </span><br><span class="line">        self.conv3 = nn.Sequential(         <span class="comment"># 下一个套餐的输入 (16, 14, 14)</span></span><br><span class="line">            nn.Conv2d(<span class="number">32</span>, <span class="number">64</span>, <span class="number">5</span>, <span class="number">1</span>, <span class="number">2</span>),     <span class="comment"># 输出 (32, 14, 14)</span></span><br><span class="line">            nn.ReLU(),             <span class="comment"># 输出 (32, 7, 7)</span></span><br><span class="line">        )</span><br><span class="line">        </span><br><span class="line">        self.out = nn.Linear(<span class="number">64</span> * <span class="number">7</span> * <span class="number">7</span>, <span class="number">10</span>)   <span class="comment"># 全连接层得到的结果</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        x = self.conv1(x)</span><br><span class="line">        x = self.conv2(x)</span><br><span class="line">        x = self.conv3(x)</span><br><span class="line">        x = x.view(x.size(<span class="number">0</span>), -<span class="number">1</span>)           <span class="comment"># flatten操作，结果为：(batch_size, 32 * 7 * 7)</span></span><br><span class="line">        output = self.out(x)</span><br><span class="line">        <span class="keyword">return</span> output</span><br><span class="line"></span><br><span class="line"><span class="comment"># 实例化</span></span><br><span class="line">net = CNN() </span><br><span class="line"><span class="comment">#损失函数</span></span><br><span class="line">criterion = nn.CrossEntropyLoss() </span><br><span class="line"><span class="comment">#优化器，nn.paarameters管理参数</span></span><br><span class="line">optimizer = optim.Adam(net.parameters(), lr=<span class="number">0.001</span>) <span class="comment">#定义优化器，普通的随机梯度下降算法</span></span><br></pre></td></tr></table></figure></p>
<h5 id="使用nn.module来管理子模块">6.2 使用nn.Module来管理子模块</h5>
<p>一般情况下，我们都很少直接使用 nn.Parameter来定义参数构建模型，<strong>而是通过一些拼装一些常用的模型层来构造模型，如上面的<code>CNN</code>类拼接了模型层，自动生成参数值由<code>nn.parameters</code>管理。</strong>这些模型层也是继承自<code>nn.Module</code>的对象,本身也包括参数，<strong>模型层属于我们要定义的模块的子模块。 nn.Module提供了一些方法可以管理这些子模块。</strong></p>
<ul>
<li><p><code>children()</code>: 返回生成器，包括模块下的所有子模块。</p></li>
<li><p><code>named_children()</code>：返回一个生成器，只有模块下的所有子模块，以及它们的名字。</p></li>
<li><p><code>modules()</code>：返回一个生成器，不仅返回模块下的子模块，连子模块下的子模块也会被返回，包括模块本身。</p></li>
<li><p><code>named_modules()</code>：返回一个生成器，包括模块下的所有各个层级的模块以及它们的名字，包括模块本身。</p></li>
</ul>
<p>上面函数返回一个可迭代的生成器，可通过for循环查看返回值。见<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/349156416">PyTorch中的modules()和children()相关函数简析</a></p>
<h5 id="模型层">6.3 模型层</h5>
<p>深度学习模型一般由各种模型层组合而成，<code>torch.nn</code>中内置了非常丰富的各种模型层。它们都属于<code>nn.Module</code>的子类，具备参数管理功能,如上面提到的<code>nn.Linear、 nn.Flatten、 nn.Dropout, nn.BatchNorm2d、nn.Conv2d、nn.AvgPool2d、nn.Conv1d、nn.ConvTranspose2d、nn.Embedding、nn.GRU、nn.LSTM、nn.Transformer</code>，下面一小结我们将列举pytorch内置的模型层</p>
<p><strong><em>如果这些内置模型层不能够满足需求，我们也可以通过继承nn.Module基类构建自定义的模型层。实际上，pytorch不区分模型和模型层，都是通过继承nn.Module进行构建。因此，我们只要继承nn.Module基类并实现forward方法即可自定义模型层。</em></strong></p>
<h5 id="内置模型层">6.4 内置模型层</h5>
<p>这里只对相应的模型层做一个简介，详细的API文档查阅<a target="_blank" rel="noopener" href="https://pytorch.org/vision/stable/models.html">官方文档</a> ###### 6.4.1 基础层 - <code>nn.Linear</code>：全连接层。参数个数 = 输入层特征数× 输出层特征数(weight)＋ 输出层特征数 (bias)</p>
<ul>
<li><p><code>nn.Flatten</code>：压平层，用于将多维张量样本压成一维张量样本。</p></li>
<li><p><code>nn.BatchNorm1d</code>：一维批标准化层。通过线性变换将输入批次缩放平移到稳定的均值和标准差。可以增强模型对输入不同分布的适应性，加快模型训练速度，有轻微正则化效果。一般在激活函数之前使用。可以用afine参数设置该层是否含有可以训练的参数。</p></li>
<li><p><code>nn.BatchNorm2d</code>：二维批标准化层。</p></li>
<li><p><code>nn.BatchNorm3d</code>：三维批标准化层。</p></li>
<li><p><code>nn.Dropout</code>：一维随机丢弃层。一种正则化手段。</p></li>
<li><p><code>nn.Dropout2d</code>：二维随机丢弃层。</p></li>
<li><p><code>nn.Dropout3d</code>：三维随机丢弃层。</p></li>
<li><p><code>nn.Threshold</code>：限幅层。当输入大于或小于阈值范围时，截断之。</p></li>
<li><p><code>nn.ConstantPad2d</code>： 二维常数填充层。对二维张量样本填充常数扩展长度。</p></li>
<li><p><code>nn.ReplicationPad1d</code>： 一维复制填充层。对一维张量样本通过复制边缘值填充扩展长度。</p></li>
<li><p><code>nn.ZeroPad2d</code>：二维零值填充层。对二维张量样本在边缘填充0值.</p></li>
<li><p><code>nn.GroupNorm</code>：组归一化。一种替代批归一化的方法，将通道分成若干组进行归一。不受 batch大小限制，据称性能和效果都优于BatchNorm。</p></li>
<li><p><code>nn.LayerNorm</code>：层归一化。较少使用。</p></li>
<li><p><code>nn.InstanceNorm2d</code>: 样本归一化。较少使用。</p></li>
</ul>
<h6 id="卷积网络相关层">6.4.2 卷积网络相关层</h6>
<ul>
<li><p><code>nn.Conv1d</code>：普通一维卷积，常用于文本。<code>参数个数 = 输入通道数×卷积核尺寸(如3)×卷积核个数 + 卷积核个数</code></p></li>
<li><p><code>nn.Conv2d</code>：普通二维卷积，常用于图像。<code>参数个数 = 输入通道数×卷积核尺寸(如3乘3)×卷积核个数 + 卷积核个数</code>。通过调整<code>dilation</code>参数大于1，可以变成空洞卷积，增大卷积核感受野。 通过调整<code>groups</code>参数不为1，可以变成分组卷积。分组卷积中不同分组使用相同的卷积核，显著减少参数数量。 当<code>groups</code>参数等于通道数时，相当于tensorflow中的二维深度卷积层<code>tf.keras.layers.DepthwiseConv2D</code>。 利用分组卷积和1乘1卷积的组合操作，可以构造相当于<code>Keras</code>中的二维深度可分离卷积层<code>tf.keras.layers.SeparableConv2D</code>。</p></li>
<li><p><code>nn.Conv3d</code>：普通三维卷积，常用于视频。<code>参数个数 = 输入通道数×卷积核尺寸(如3乘3乘3)× 卷积核个数 + 卷积核个数</code> 。</p></li>
<li><p><code>nn.MaxPool1d</code>: 一维最大池化。</p></li>
<li><p><code>nn.MaxPool2d</code>：二维最大池化。一种下采样方式。没有需要训练的参数。</p></li>
<li><p><code>nn.MaxPool3d</code>：三维最大池化。</p></li>
<li><p><code>nn.AdaptiveMaxPool2d</code>：二维自适应最大池化。无论输入图像的尺寸如何变化，输出的图像尺寸是固定的。 该函数的实现原理，大概是通过输入图像的尺寸和要得到的输出图像的 尺寸来反向推算池化算子的<code>padding,stride</code>等参数。</p></li>
<li><p>nn.FractionalMaxPool2d：二维分数最大池化。普通最大池化通常输入尺寸是输出的整数 倍。而分数最大池化则可以不必是整数。分数最大池化使用了一些随机采样策略，有一定的 正则效果，可以用它来代替普通最大池化和Dropout层。</p></li>
<li><p><code>nn.AvgPool2d</code>：二维平均池化。</p></li>
<li><p><code>nn.AdaptiveAvgPool2d</code>：二维自适应平均池化。无论输入的维度如何变化，输出的维度是固定的。</p></li>
<li><p><code>nn.ConvTranspose2d</code>：二维卷积转置层，俗称反卷积层。并非卷积的逆操作，但在卷积核相同的情况下，当其输入尺寸是卷积操作输出尺寸的情况下，卷积转置的输出尺寸恰好是卷积操作的输入尺寸。在语义分割中可用于上采样。</p></li>
<li><p><code>nn.Upsample</code>：上采样层，操作效果和池化相反。可以通过mode参数控制上采样策略 为”nearest”最邻近策略或”linear”线性插值策略。</p></li>
<li><p><code>nn.Unfold</code>：滑动窗口提取层。其参数和卷积操作<code>nn.Conv2d</code>相同。实际上，卷积操作可以等价于<code>nn.Unfold</code>和<code>nn.Linear</code>以及<code>nn.Fold</code>的一个组合。 其中<code>nn.Unfold</code>操作可以从输入中提取各个滑动窗口的数值矩阵，并将其压平成一维。利用<code>nn.Linear</code>将<code>nn.Unfold</code>的输出和卷积核做乘法后，再使用 <code>nn.Fold</code>操作将结果转换成输出图片形状。</p></li>
<li><p><code>nn.Fold</code>：逆滑动窗口提取层。</p></li>
</ul>
<p><strong><em>注意</em></strong>：</p>
<ul>
<li>卷积核个数==输出的feature map（activation map）个数，比如输入是一个<span class="math inline">\(32x32x3\)</span>的图像，<span class="math inline">\(3\)</span>表示RGB三通道，每个<code>filter/kernel</code>是<span class="math inline">\(5x5x3\)</span>，一个卷积核产生一个<span class="math inline">\(feature map\)</span>，下图中，有<span class="math inline">\(6\)</span>个 <span class="math inline">\(5x5x3\)</span>的卷积核，故输出<span class="math inline">\(6\)</span>个<code>feature map（activation map）</code>，大小为<span class="math inline">\(28x28x6\)</span>。 <img src="/2022/10/06/pytorch%E5%9F%BA%E7%A1%80/kernel.png" width="600"></li>
<li>参数个数为：卷积核个数*(卷积核大小)+卷积核个数(偏置数)</li>
<li>偏置数==卷积核个数</li>
</ul>
<h6 id="循环网络相关层">6.4.3 循环网络相关层</h6>
<ul>
<li><p><code>nn.Embedding</code>：嵌入层。一种比<code>Onehot</code>更加有效的对离散特征进行编码的方法。一般用于将输入中的单词映射为稠密向量。嵌入层的参数需要学习。</p></li>
<li><p><code>nn.LSTM</code>：长短记忆循环网络层【支持多层】。最普遍使用的循环网络层。具有携带轨道，遗忘门，更新门，输出门。可以较为有效地缓解梯度消失问题，从而能够适用长期依赖问题。设置<code>bidirectional = True</code>时可以得到双向<code>LSTM</code>。需要注意的时，默认的输入和输出形状是<code>(seq,batch,feature)</code>, 如果需要将<code>batch</code>维度放在第0维，则要设置<code>batch_first</code>参数设置为<code>True</code>。</p></li>
<li><p><code>nn.GRU</code>：门控循环网络层【支持多层】。LSTM的低配版，不具有携带轨道，参数数量少于 LSTM，训练速度更快。</p></li>
<li><p><code>nn.RNN</code>：简单循环网络层【支持多层】。容易存在梯度消失，不能够适用长期依赖问题。 一般较少使用。</p></li>
<li><p><code>nn.LSTMCell</code>：长短记忆循环网络单元。和<code>nn.LSTM</code>在整个序列上迭代相比，它仅在序列上迭代一步。一般较少使用。</p></li>
<li><p><code>nn.GRUCell</code>：门控循环网络单元。和<code>nn.GRU</code>在整个序列上迭代相比，它仅在序列上迭代一步。一般较少使用。</p></li>
<li><p><code>nn.RNNCell</code>：简单循环网络单元。和<code>nn.RNN</code>在整个序列上迭代相比，它仅在序列上迭代一步。一般较少使用。</p></li>
</ul>
<h6 id="transformer相关层">6.4.4 Transformer相关层</h6>
<p><code>Transformer</code>网络结构是替代循环网络的一种结构，解决了循环网络难以并行，难以捕捉长期依赖的缺陷。它是目前NLP任务的主流模型的主要构成部分。<code>Transformer</code>网络结构由<code>TransformerEncoder</code>编码器和<code>TransformerDecoder</code>解码器组成。编码器和解码器的核心是<code>MultiheadAttention</code>多头注意力层。</p>
<ul>
<li><p><code>nn.TransformerEncoder</code>：<code>Transformer</code>编码器结构。由多个<code>nn.TransformerEncoderLayer</code>编 码器层组成。</p></li>
<li><p><code>nn.TransformerDecoder</code>：<code>Transformer</code>解码器结构。由多个<code>nn.TransformerDecoderLayer</code>解码器层组成。</p></li>
<li><p><code>nn.TransformerEncoderLayer</code>：<code>Transformer</code>的编码器层。</p></li>
<li><p><code>nn.TransformerDecoderLayer</code>：<code>Transformer</code>的解码器层。</p></li>
<li><p><code>nn.MultiheadAttention</code>：多头注意力层。</p></li>
</ul>
<h6 id="自定义模型层">6.4.5 自定义模型层</h6>
<p>如果Pytorch的内置模型层不能够满足需求，<strong>我们也可以通过继承<code>nn.Module</code>基类构建自定义的模型层。实际上，pytorch不区分模型和模型层，都是通过继承<code>nn.Module</code>进行构建。 因此，我们只要继承nn.Module基类并实现forward方法即可自定义模型层</strong>。 下面是Pytorch的nn.Linear层的源码，我们可以仿照它来自定义模型层</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/2022/09/28/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="trluper">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Trluper">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/09/28/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" class="post-title-link" itemprop="url">深度学习</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2022-09-28 20:22:08" itemprop="dateCreated datePublished" datetime="2022-09-28T20:22:08+08:00">2022-09-28</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2022-10-06 10:36:35" itemprop="dateModified" datetime="2022-10-06T10:36:35+08:00">2022-10-06</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95/" itemprop="url" rel="index"><span itemprop="name">深度学习算法</span></a>
                </span>
            </span>

          <br>
            <span class="post-meta-item" title="本文字数">
              <span class="post-meta-item-icon">
                <i class="far fa-file-word"></i>
              </span>
                <span class="post-meta-item-text">本文字数：</span>
              <span>8.3k</span>
            </span>
            <span class="post-meta-item" title="阅读时长">
              <span class="post-meta-item-icon">
                <i class="far fa-clock"></i>
              </span>
                <span class="post-meta-item-text">阅读时长 &asymp;</span>
              <span>8 分钟</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h4 id="深度学习简述">1.深度学习简述</h4>
<p>为了学习一种好的表示，需要构建具有一定“深度”的模型，并通过学习算法来让模型自动学习出好的特征表示（从底层特征，到中层特征，再到高层特征），从而最终提升预测模型的准确率．<strong>所谓“深度”是指原始数据进行非线性特征转换的次数</strong> <img src="/2022/09/28/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/DL.png" width="500"></p>
<p>深度学习是将原始的数据特征通过多步的特征转换得到一种特征表示，并进一步输入到预测函数得到最终结果．和“浅层学习”不同，<strong>深度学习需要解决的关键问题是贡献度分配问题（Credit Assignment Problem，CAP）[Minsky,1961]</strong>，即一个系统中不同的组件（component）或其参数对最终系统输出结果的贡献或影响.从某种意义上讲，深度学习可以看作一种强化学习（Reinforcement Learning，RL），每个内部组件并不能直接得到监督信息，需要通过整个模型的最终监督信息（奖励）得到，并且有一定的延时性</p>
<p><strong>深度学习采用的模型主要是神经网络模型，其主要原因是神经网络模型可以使用误差反向传播算法，从而可以比较好地解决贡献度分配问题</strong>．</p>
<h5 id="表示学习">1.1 表示学习</h5>
<p>为了提高机器学习系统的准确率，我们就需要将输入信息转换为有效的特 征，或者更一般性地称为表示（Representation）。如果有一种算法可以自动地学习出有效的特征，并提高最终机器学习模型的性能，那么这种学习就可以叫作<strong>表示学习</strong>。</p>
<p>表示学习的关键是解决<strong>语义鸿沟（Semantic Gap）问题</strong>．语义鸿沟问题是指输入数据的底层特征和高层语义信息之间的不一致性和差异性．比如给定一些关于“车”的图片，由于图片中每辆车的颜色和形状等属性都不尽相同，因此不同图片在像素级别上的表示（即底层特征）差异性也会非常大．但是我们理解这些图片是建立在比较抽象的高层语义概念上的．如果一个预测模型直接建立在底层特征之上，会导致对预测模型的能力要求过高．如果可以有一个好的表示在某种程度上能够反映出数据的高层语义特征，那么我们就能相对容易地构建后续的机器学习模型</p>
<h6 id="局部和分布式表示">4.1.1 局部和分布式表示</h6>
<p>在机器学习中，我们经常使用两种方式来表示特征：<strong>局部表示（Local Representation）和分布式表示（Distributed Representation）．</strong></p>
<ul>
<li><strong>局部表示</strong>：离散表示，one-Hot向量，单一值表示一个东西。
<ul>
<li><strong>优点：</strong>
<ul>
<li>这种离散的表示方式具有很好的解释性，有利于人工归纳和总结特征，并通过特征组合进行高效的特征工程</li>
<li>通过多种特征组合得到的表示向量通常是稀疏的二值向量，当用于线性模型时计算效率非常高</li>
</ul></li>
<li><strong>缺点</strong>：
<ul>
<li>one-hot向量的维数很高，且不能扩展．如果有一种新的颜色，我们就需要增加一维来表示</li>
<li>不同颜色之间的相似度都为0，即我们无法知道“红色”和“中国红”的相似度要高于“红色”和“黑色”的相似度</li>
</ul></li>
</ul></li>
<li><strong>分布式表示</strong>：压缩、低维的稠密向量，使用多个值表示一个东西，如表示颜色的方法是用RGB值来表示颜色，不同颜色对应到R、G、B三维空间中一个点，这种表示方式叫作分布式表示
<ul>
<li>分布式表示的表示能力要强很多，分布式表示的向量维度一般都比较低．我们只需要用一个三维的稠密向量就可以表示所有颜色．并且，分布式表示也很容易表示新的颜色名．此外，不同颜色之间的相似度也很容易计算</li>
</ul></li>
</ul>
<h5 id="端到端学习">1.2 端到端学习</h5>
<p><strong>端到端学习（End-to-End Learning），也称端到端训练，是指在学习过程中不进行分模块或分阶段训练，直接优化任务的总体目标</strong>．在端到端学习中，一般不需要明确地给出不同模块或阶段的功能，中间过程不需要人为干预．端到端学习的训练数据为“输入-输出”对的形式，无须提供其他额外信息．因此，端到端学习和深度学习一样，都是要解决贡献度分配问题．目前，大部分采用神经网络模型的深度学习也可以看作一种端到端的学习</p>
<h4 id="前馈神经网络">2. 前馈神经网络</h4>
<h5 id="神经元">2.1 神经元</h5>
<p>神经网络一般可以看作一个非线性模型，其基本组成单元为具有非线性激活函数的神经元，通过大量神经元之间的连接，使得神经网络成为一种高度非线性的模型．<strong>神经元之间的连接权重就是需要学习的参数，可以在机器学习的框架下通过梯度下降方法来进行学习</strong>． <img src="/2022/09/28/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/Neuron.png" width="300"></p>
<p>激活函数在神经元中非常重要的．为了增强网络的表示能力和学习能力，<strong>激活函数</strong>需要具备以下几点性质：</p>
<ul>
<li>（1） 连续并可导（允许少数点上不可导）的非线性函数．可导的激活函数可以直接利用数值优化的方法来学习网络参数．</li>
<li>（2） 激活函数及其导函数要尽可能的简单，有利于提高网络计算效率．</li>
<li>（3） 激活函数的导函数的值域要在一个合适的区间内，不能太大也不能太小，否则会影响训练的效率和稳定性．</li>
</ul>
<p>下面介绍几种在神经网络中常用的激活函数．</p>
<h6 id="sigmoid函数">2.1.1 sigmoid函数</h6>
<p><code>Sigmoid</code>型函数是指一类 S 型曲线函数，为两端饱和函数．常用的<code>Sigmoid</code>型函数<strong>有<code>Logistic</code>函数和<code>Tanh</code>函数</strong>．<strong>所谓饱和是指</strong>对于函数 <span class="math inline">\(f(x)\)</span>，若 <span class="math inline">\(x → −∞\)</span> 时，其导数 <span class="math inline">\(f^′(x) → 0\)</span>，则称其为左饱和．若<span class="math inline">\(x → +∞\)</span>时，其导数<span class="math inline">\(f′(x) → 0\)</span>，则称其为右饱和．当同时满足左、右饱和时，就称为两端饱和。</p>
<p>在机器学习篇章我们以及介绍果LR函数： <span class="math display">\[
σ(x)={1\over 1+exp(-x)}
\]</span></p>
<p><strong>Logistic 函数可以看成是一个“挤压”函数，把一个实数域的输入“挤压”到(0, 1)</strong>．当输入值在0附近时，Sigmoid型函数近似为线性函数；当输入值靠近两端时，对输入进行抑制．输入越小，越接近于 0；输入越大，越接近于 1．和感知器使用的阶跃激活函数相比，Logistic函数是连续可导的，其数学性质更好．</p>
<p>因为Logistic函数的性质，使得装备了Logistic激活函数的神经元具有以下两点性质： - 1）其输出直接可以看作概率分布，使得神经网络可以更好地和统计学习模型进行结合． - 2）其可以看作一个软性门（Soft Gate），用来控制其他神经元输出信息的数量．</p>
<p><img src="/2022/09/28/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/sigmoid.png" width="300"></p>
<p><strong><code>Tanh</code>函数</strong>： <span class="math display">\[
tanh(x)={exp(x)-exp(-x)\over exp(x)+exp(-x)}
\]</span></p>
<h6 id="relu函数">2.1.2 Relu函数</h6>
<p>ReLU（Rectified Linear Unit，修正线性单元）是目前深度神经网络中经常使用的激活函数．<strong>ReLU实际上是一个斜坡（ramp）函数</strong>，定义为： <span class="math display">\[
Relu(x)=\begin{cases}
x&amp;x≥0\\
0&amp;x&lt;0
\end{cases}
\]</span></p>
<p><strong>优点：</strong></p>
<ul>
<li>采用 ReLU 的神经元只需要进行加、乘和比较的操作，计算上更加高效</li>
<li>具有生物学合理性，比如单侧抑制、宽兴奋边界。Sigmoid 型激活函数会导致一个非稀疏的神经网络，而 ReLU 却具有很好的稀疏性，大约50%的神经元会处于激活状态</li>
<li>在优化方面，相比于Sigmoid型函数的两端饱和，ReLU函数为左饱和函数，且在 $x &gt; 0 $时导数为 1，在一定程度上缓解了神经网络的梯度消失问题，加速梯度下降的收敛速度．</li>
</ul>
<p><strong>缺点</strong>：</p>
<ul>
<li>ReLU 函数的输出是非零中心化的，给后一层的神经网络引入偏置偏移，会影响梯度下降的效率</li>
<li>ReLU 神经元在训练时比较容易“死亡”．在训练时，如果参数在一次不恰当的更新后，第一个隐藏层中的某个 ReLU 神经元在所有的训练数据上都不能被激活，那么这个神经元自身参数的梯度永远都会是0，在以后的训练过程中永远不能被激活．这种现象称为死亡 ReLU 问题</li>
</ul>
<p>为克服上述缺陷，几种变种Relu被广泛使用：</p>
<ul>
<li><p><strong>带泄露的Relu</strong>:带泄露的ReLU（Leaky ReLU）在输入 <span class="math inline">\(x&lt; 0\)</span>时，保持一个很小的梯度𝛾．这样当神经元非激活时也能有一个非零的梯度可以更新参数，避免永远不能被激活[Maas et al., 2013]．带泄露的ReLU的定义如下： <span class="math display">\[
LeakyRelu(x)=\begin{cases}
x&amp;x&gt;0\\
γx&amp;x≤0
\end{cases}
\]</span> 其中<span class="math inline">\(γ\)</span>是一个很小的常数，如0.001</p></li>
<li><p><strong>带参数的ReLU：</strong>带参数的 ReLU（Parametric ReLU，PReLU）引入一个可学习的参数，不同神经元可以有不同的参数 [He et al., 2015]．对于第<span class="math inline">\(i\)</span>个神经元，其 PReLU的定义为 <span class="math display">\[
PRelu(x)=\begin{cases}
x&amp;x&gt;0\\
γ_ix&amp;x≤0
\end{cases}
\]</span> 其中<span class="math inline">\(γ_i\)</span>为 <span class="math inline">\(x≤ 0\)</span> 时函数的斜率．因此，PReLU 是非饱和函数．如果<span class="math inline">\(γ_i=0\)</span>，那么PReLU就退化为ReLU．如果<span class="math inline">\(γ_i\)</span>为一个很小的常数，则PReLU可以看作带泄露的ReLU．PReLU可以允许不同神经元具有不同的参数</p></li>
<li><p><strong>ELU（Exponential Linear Unit，指数线性单元）</strong>：[Clevert et al., 2015] 是一个近似的零中心化的非线性函数，其定义为: <span class="math display">\[
Elu(x)=\begin{cases}
x&amp;x&gt;0\\
γ(exp(x)-1)&amp;x≤0
\end{cases}
\]</span> 其中<span class="math inline">\(γ ≥ 0\)</span>是一个超参数，决定<span class="math inline">\(x≤ 0\)</span>时的饱和曲线，并调整输出均值在0附近</p></li>
<li><p><strong>Softplus函数：</strong>Softplus 函数[Dugas et al., 2001] 可以看作 Rectifier 函数的平滑版本，其定义为 <span class="math display">\[
softplus(x)=log(1+exp(x))
\]</span> Softplus函数其导数刚好是Logistic函数．Softplus函数虽然也具有单侧抑制、宽兴奋边界的特性，却没有稀疏激活性 <img src="/2022/09/28/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/Relu.png" width="400"></p></li>
</ul>
<h5 id="swish函数">2.1.3 Swish函数</h5>
<p>Swish 函数[Ramachandran et al., 2017] 是一种<strong>自门控（Self-Gated）激活函数</strong>，定义为: <span class="math display">\[
swish(x)=xσ(βx)
\]</span> 其中<strong><span class="math inline">\(δ(⋅)\)</span>为 Logistic 函数</strong>，<span class="math inline">\(β\)</span>为可学习的参数或一个固定超参数.<span class="math inline">\(δ(⋅) ∈ (0, 1)\)</span>可以看作一种软性的门控机制．当<span class="math inline">\(δ(βx)\)</span>接近于1时，门处于“开”状态，激活函数的输出近似于<span class="math inline">\(x\)</span>本身；当<span class="math inline">\(δ(βx)\)</span>)接近于0时，门的状态为“关”，激活函数的输出近似于0． <img src="/2022/09/28/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/swish.png" width="400"></p>
<p>Swish函数可以看作线性函数和ReLU函数之间的非线性插值函数，其程度由参数<span class="math inline">\(β\)</span>控制．</p>
<h6 id="gelu函数">2.1.4 GELU函数</h6>
<p>GELU（Gaussian Error Linear Unit，高斯误差线性单元）[Hendrycks et al.,2016] 也是一种通过<strong>门控机制来调整其输出值的激活函数</strong>，和 Swish 函数比较类似 <span class="math display">\[
GELU(x)=xp(X≤x)
\]</span> 其中<span class="math inline">\(p(X ≤ x)\)</span>是高斯分布<span class="math inline">\(N(μ, σ^2)\)</span>的累积分布函数，其中<span class="math inline">\(μ, σ\)</span>为超参数，一般设<span class="math inline">\(μ= 0, σ= 1\)</span>即可．由于高斯分布的累积分布函数为S型函数，因此GELU函数可以用Tanh函数或Logistic函数来近似， <span class="math display">\[
GELU(x)=xσ(1.702x)
\]</span></p>
<h6 id="maxout单元">2.1.5 Maxout单元</h6>
<p>Maxout 单元[Goodfellow et al., 2013] 也是一种<strong>分段线性函数函数</strong>。不同的是Sigmoid 、ReLU 等激活函数的输入是神经元的净输入<span class="math inline">\(z\)</span>，是一个标量而 Maxout 单元的输入是上一层神经元的全部原始输出，是一个向量<span class="math inline">\(x = [x_1; x_2; ⋯ ; x_D]\)</span>．</p>
<p>每个Maxout单元有<span class="math inline">\(K\)</span>个权重向量$w_k ∈ ℝ^D <span class="math inline">\(和偏置\)</span>b_k(1 ≤ k ≤ K)<span class="math inline">\(．对于输入\)</span>x<span class="math inline">\(，可以得到\)</span>K<span class="math inline">\(个净输入\)</span>z_k, 1 ≤ k ≤ K$ <span class="math display">\[
z_k=w^T_k+b_k
\]</span> 其中<span class="math inline">\(w_k= [w_{k,1}, ⋯ , w_{k,D}]^T\)</span> 为第<span class="math inline">\(k\)</span>个权重向量</p>
<h5 id="网络结构">2.2 网络结构</h5>
<p>样通过一定的连接方式或信息传递方式进行协作的神经元可以看作一个网络，就是神经网络．．目前常用的神经网络结构有以下三种：</p>
<ul>
<li><p><strong>前馈网络</strong>：前馈网络中各个神经元按接收信息的先后分为不同的组．<strong>每一组可以看作一个神经层．每一层中的神经元接收前一层神经元的输出，并输出到下一层神经元．整个网络中的信息是朝一个方向传播</strong>，没有反向的信息传播，可以用一个有向无环路图表示。<strong>前馈网络包括全连接前馈网络和卷积神经网络等</strong>．</p></li>
<li><p><strong>记忆网络</strong>：记忆网络，也称为反馈网络，<strong>网络中的神经元不但可以接收其他神经元的信息，也可以接收自己的历史信息．和前馈网络相比，记忆网络中的神经元具有记忆功能，在不同的时刻具有不同的状态．记忆神经网络中的信息传播可以是单向或双向传递</strong>。记忆网络包括循环神经网络、Hopfield 网络、玻尔兹曼机、受限玻尔兹曼机等</p></li>
<li><p><strong>图网络</strong>：<strong>图网络是定义在图结构数据上的神经网络．图中每个节点都由一个或一组神经元构成．节点之间的连接可以是有向的，也可以是无向的．每个节点可以收到来自相邻节点或自身的信息</strong>。图网络是前馈网络和记忆网络的泛化，包含很多不同的实现方式，比如<em>图卷积网络（Graph Convolutional Network，<strong>GCN</strong>）[Kipf et al., 2016]、图注意力网络（Graph Attention Network，<strong>GAT</strong>）[Veličković et al., 2017]、消息传递神经网络（Message Passing Neural Network，<strong>MPNN</strong>）[Gilmer et al., 2017]等</em>． <img src="/2022/09/28/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/neuralStructure.png" width="500"></p></li>
</ul>
<h5 id="反向传播算法">2.3 反向传播算法</h5>
<p>假设采用随机梯度下降进行神经网络参数学习，给定一个样本<span class="math inline">\((x, y)\)</span>，将其输入到神经网络模型中，得到网络输出为<span class="math inline">\(\tilde{𝒚}\)</span>．假设损失函数为$ ℒ(𝒚,)$，<strong>要进行参数学习就需要计算损失函数关于每个参数的导数</strong>．</p>
<p>不失一般性，对第<span class="math inline">\(l\)</span>层中的参数<span class="math inline">\(W^{(l)}\)</span> 和<span class="math inline">\(b{(l)}\)</span>计算偏导数．因为<span class="math inline">\(𝜕ℒ(𝒚,\tilde{y})\over𝜕𝑾^{(𝑙)}\)</span>的计算涉及向量对矩阵的微分，十分繁琐，因此我们先计算<span class="math inline">\(ℒ(𝒚,\tilde{𝒚})\)</span> 关于参数矩阵中每个元素的偏导数$ ℒ(y,)w^{(l)}_{ij}$．根据链式法则: <span class="math display">\[
{δℒ(𝒚,\tilde{y}) \over δw^{(l)}_{ij}}={δz^{(l)}\over δw^{(l)}_{ij} }{δℒ(𝒚,\tilde{y})\over δz^{(l)}} \\
{δℒ(𝒚,\tilde{y}) \over δb^{(l)}}={δz^{(l)}\over δb^{(l)}}{δℒ(𝒚,\tilde{y})\over δz^{(l)}}
\]</span> 上式的第二项都是目标函数关于第<span class="math inline">\(l\)</span>层的神经元<span class="math inline">\(z^{(l)}\)</span>的偏导数，称为误差项，可以一次计算得到．这样我们只需要计算三个偏导数，分别为<span class="math inline">\({δz^{(l)}\over δw^{(l)}_{ij}}、{δz^{(l)}\over δb^{(l)}}、{δℒ(𝒚,\tilde{y})\over δz^{(l)}}\)</span></p>
<p>下面分别计算它们：</p>
<ul>
<li><span class="math inline">\({δz^{(l)}\over δw^{(l)}_{ij}}\)</span>：因为<span class="math inline">\(z=Wx+b\)</span>，则其导数为可以很方便计算得到</li>
<li><span class="math inline">\({δℒ(𝒚,\tilde{y})\over δz^{(l)}}\)</span>：<strong>该偏导数表示第<span class="math inline">\(l\)</span>层神经元对最终损失的影响，也反映了最终损失对第<span class="math inline">\(l\)</span>层神经元的敏感度</strong>，因此一般称为误差项<span class="math inline">\(δ^{(l)}\)</span>，<strong>它也间接反映了神经元对网络的共享程度</strong>，是解决贡献的分配问题的一个指标。详细公式见书p94</li>
</ul>
<h5 id="梯度消失">2.4 梯度消失</h5>
<p>上面可以知道在神经网络中误差反向传播的迭代公式为 <span class="math display">\[
δ^{(l)}=f^{&#39;}_l(z^{(l)})⊙(W^{(l+1)})^Tδ^{(l+1)}
\]</span> 误差从输出层反向传播时，在每一层都要乘以该层的激活函数的导数，当采取Sigmoid型激活函数时，由于其导数<code>&lt;1</code>,经过深度学习，一层一层下去，其误差项不断趋近于0，梯度就会不断衰减,甚至消失，导致网络难以训练.这就是<strong>梯度消失</strong></p>
<p>在深度神经网络中，减轻梯度消失问题的方法有很多种．一种简单有效的方式是使用导数比较大的激活函数，比如ReLU等．</p>
<h4 id="卷积神经网络">3 卷积神经网络</h4>
<p>卷积神经网络（Convolutional Neural Network，CNN或ConvNet）是一种具有<strong>局部连接、权重共享</strong>等特性的深层前馈神经网络．卷积神经网络有个重要的专有词叫<strong>感受野</strong>，，即神经元只接受其所支配的刺激区域内的信号。卷积神经网络的出现解决了全连接前馈网络的两个问题<strong>参数太多</strong>和<strong>局部不变性特征</strong>。</p>
<p><strong>目前的卷积神经网络一般是由卷积层、汇聚层和全连接层交叉堆叠而成的前馈神经网络．卷积神经网络有三个结构上的特性：局部连接、权重共享以及汇聚</strong>．这些特性使得卷积神经网络具有一定程度上的平移、缩放和旋转不变性．和前馈神经网络相比，<strong>卷积神经网络的参数更少</strong>．</p>
<h5 id="卷积和卷积核">3.1 卷积和卷积核</h5>
<p>卷积（Convolution），也叫褶积，是分析数学中一种重要的运算．在信号处理或图像处理中，经常使用一维或二维卷积．</p>
<p>如二维卷积<span class="math inline">\(Y=W*X\)</span>为卷积操作，其中<span class="math inline">\(W\)</span>称为卷积核，<span class="math inline">\(X\)</span>为特征矩阵。 <img src="/2022/09/28/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/Convolution.png" width="400"></p>
<h5 id="互相关">3.2 互相关</h5>
<p>在机器学习和图像处理领域，卷积的主要功能是在一个图像（或某种特征）上滑动一个卷积核（即滤波器），通过卷积操作得到一组新的特征．在计算卷积的过程中，需要进行卷积核翻转。而在计算<strong>自相关</strong>时,则不需要翻转，所谓的翻转是对<strong>卷积核</strong>进行从上到下，从左到右的的颠倒。<strong>因此互相关和卷积的区别仅仅在于卷积核是否进行翻转</strong></p>
<p>在深度学习中我们一般使用自相关，它也称为<strong>不翻转卷积</strong>。</p>
<p><em>注：卷积核是否进行翻转和其特征抽取的能力无关．特别是当卷积核是可学习的参数时，卷积和互相关在能力上是等价的．因此，为了实现上（或描述上）的方便起见，我们用互相关来代替卷积</em></p>
<h5 id="卷积神经网络-1">3.3 卷积神经网络</h5>
<p>卷积神经网络一般由卷积层、汇聚层和全连接层构成．</p>
<h6 id="卷积层">3.3.1 卷积层</h6>
<p>卷积层的作用是提取一个局部区域的特征，<strong>不同的卷积核相当于不同的特征提取器</strong>。而图像为二维结构，因此为了更充分地利用图像的局部信息，通常将神经元组织为三维结构的神经层，其大小为高度M×宽度N×深度D，由<span class="math inline">\(D\)</span>个<span class="math inline">\(M × N\)</span>大小的特征映射构成。<strong>特征映射（Feature Map）为一幅图像（或其他特征映射）在经过卷积提取到的特征</strong>，每个特征映射可以作为一类抽取的图像特征．为了提高卷积网络的表示能力，可以在每一层使用多个不同的特征映射，以更好地表示图像的特征． <img src="/2022/09/28/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/featureMap.png" width="400"> <strong>在输入层，特征映射就是图像本身．如果是灰度图像，就是有一个特征映射，输入层的深度 <span class="math inline">\(D = 1\)</span>；如果是彩色图像，分别有 RGB 三个颜色通道的特征映射，输入层的深度<span class="math inline">\(D = 3\)</span>．</strong></p>
<p>假设现在有一个卷积层的结构如下：</p>
<ul>
<li><p>输入特征映射组：<span class="math inline">\(X∈ ℝ^{M×N×D}\)</span>为三维张量（Tensor），其中每个切片（Slice）矩阵<span class="math inline">\(X^d ∈ ℝ^{M×N }\)</span>为一个输入特征映射，<span class="math inline">\(1 ≤ d ≤D\)</span>；</p></li>
<li><p>输出特征映射组：<span class="math inline">\(y∈ ℝ^{M×N×P}\)</span>为三维张量，其中每个切片矩阵<span class="math inline">\(Y^p ∈ ℝ^{M×N}\)</span>为一个输出特征映射，<span class="math inline">\(1 ≤ p ≤ P\)</span>；</p></li>
<li><p>卷积核：<span class="math inline">\(W∈ ℝ^{M×N×P×D}\)</span>为四维张量，其中每个切片矩阵<span class="math inline">\(W^{pd}∈ ℝ^{U×V}\)</span> 为一个二维卷积核，<span class="math inline">\(1 ≤ p ≤P , 1 ≤ d ≤ D\)</span>．</p></li>
</ul>
<p>因此参数个数为：每一个输出特征映射都需要<span class="math inline">\(D\)</span>个卷积核以及一个偏置．假设每个卷积核的大小为<span class="math inline">\(U × V\)</span>，那么输出<span class="math inline">\(P\)</span>个特征映射共需要<span class="math inline">\(P × D × (U × V ) + P\)</span> 个参数</p>
<h6 id="池化层汇聚层">3.3.2 池化层(汇聚层)</h6>
<p>卷积层虽然可以显著减少网络中连接的数量，但特征映射组中的神经元个数并没有显著减少．如果后面接一个分类器，分类器的输入维数依然很高，很容易出现过拟合．为了解决这个问题，可以在卷积层之后加上一个<strong>汇聚层</strong>，从而降低特征维数，避免过拟合．<strong>汇聚层（Pooling Layer）也叫子采样层（Subsampling Layer），其作用是进行特征选择，降低特征数量，从而减少参数数量</strong>，也叫<strong>池化</strong></p>
<p>假设汇聚层的输入特征映射组为<span class="math inline">\(X∈ ℝ^{M×N×D}\)</span>，对于其中每一个特征映射<span class="math inline">\(X^d ∈ ℝ^{M×N }，1 ≤ d ≤D\)</span>，将其划分为很多区域，这些区域可以重叠，也可以不重叠．<strong>汇聚（Pooling）是指对每个区域进行下采样（Down Sampling）得到一个值，作为这个区域的概括．</strong></p>
<p><strong>常用的池化(汇聚)函数有：</strong></p>
<ul>
<li>最大池化(最大汇聚)：对于一个区域，选择这个区域内所有神经元的最大活性值作为这个区域的表示，即<span class="math inline">\(x^d=max x_i\)</span></li>
<li>平均池化(平均汇聚)：一般是取区域内所有神经元活性值的平均值，即<span class="math inline">\(y^d=avg(\sum x_i)\)</span></li>
</ul>
<p>下图给出了采样最大汇聚进行子采样操作的示例．可以看出，汇聚层不但可以有效地减少神经元的数量，还可以使得网络对一些小的局部形态改变保持不变性，并拥有更大的感受野． <img src="/2022/09/28/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/poolinglayer.png" width="400"></p>
<h4 id="网络优化和正则化">4. 网络优化和正则化</h4>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/2022/09/18/python%E6%95%B0%E5%AD%A6%E5%BA%93/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="trluper">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Trluper">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/09/18/python%E6%95%B0%E5%AD%A6%E5%BA%93/" class="post-title-link" itemprop="url">python数学库</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>
              

              <time title="创建时间：2022-09-18 10:40:45 / 修改时间：16:25:30" itemprop="dateCreated datePublished" datetime="2022-09-18T10:40:45+08:00">2022-09-18</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/python/" itemprop="url" rel="index"><span itemprop="name">python</span></a>
                </span>
            </span>

          <br>
            <span class="post-meta-item" title="本文字数">
              <span class="post-meta-item-icon">
                <i class="far fa-file-word"></i>
              </span>
                <span class="post-meta-item-text">本文字数：</span>
              <span>13k</span>
            </span>
            <span class="post-meta-item" title="阅读时长">
              <span class="post-meta-item-icon">
                <i class="far fa-clock"></i>
              </span>
                <span class="post-meta-item-text">阅读时长 &asymp;</span>
              <span>12 分钟</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h4 id="numpy概述">1. Numpy概述</h4>
<h5 id="概念">1.1 概念</h5>
<p>Python本身含有列表和数组，但对于大数据来说，这些结构是有很多不足的。由于列表的元素可以是任何对象，因此列表中所保存的是对象的指针。对于数值运算来说这种 结构比较浪费内存和CPU资源。至于数组对象，它可以直接保存 数值，和C语言的一维数组比较类似。但是由于它不支持多维，在上面的函数也不多，因此也不适合做数值运算。Numpy提供了两种基本的对象：<code>ndarray(N-dimensional Array Object)</code>和 <code>ufunc(Universal Function Object)</code>。ndarray是存储单一数据类型的多维数组，而ufunc则是能够对数组进行处理的函数。</p>
<h5 id="功能">1.2 功能</h5>
<ul>
<li>创建n维数组(矩阵)</li>
<li>对数组进行函数运算，使用函数计算十分快速，节省了大量的时间，且不需要编写循环，十分方便</li>
<li>数值积分、线性代数运算、傅里叶变换</li>
<li>ndarray快速节省空间的多维数组，提供数组化的算术运算和高级的 广播功能</li>
</ul>
<h5 id="对象">1.3 对象</h5>
<ul>
<li>NumPy中的核心对象是<code>ndarray</code></li>
<li><code>ndarray</code>可以看成数组，存放 同类元素</li>
<li>NumPy里面所有的函数都是围绕ndarray展开的</li>
</ul>
<p><strong><code>ndarray</code> 内部由以下内容组成：</strong></p>
<ul>
<li>一个指向数据(内存或内存映射文件中的一块数据)的指针。</li>
<li>数据类型或 dtype，描述在数组中的固定大小值的格子。</li>
<li>一个表示数组形状(shape)的元组，表示各维度大小的元组。形状为(row×col)</li>
</ul>
<h5 id="数据类型">1.4 数据类型</h5>
<p>numpy 支持的数据类型比 Python 内置的类型要多很多，基本上可以和C语言的数据类型对应上。主要包括<code>int8、int16、int32、int64、uint8、uint16、uint32、uint64、float16、float32、float64</code></p>
<h5 id="数组属性">1.5 数组属性</h5>
<table>
<thead>
<tr class="header">
<th>属性</th>
<th>说明</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><code>ndarray.ndim</code></td>
<td>秩，即轴的数量或维度的数量</td>
</tr>
<tr class="even">
<td><code>ndarray.shape</code></td>
<td>数组的维度(n×m)，对于矩阵，n 行 m 列</td>
</tr>
<tr class="odd">
<td><code>ndarray.size</code></td>
<td>数组元素的总个数，相当于 .shape 中 n*m 的值</td>
</tr>
<tr class="even">
<td><code>ndarray.dtype</code></td>
<td>ndarray 对象的元素类型</td>
</tr>
<tr class="odd">
<td><code>ndarray.itemsize</code></td>
<td>ndarray 对象中每个元素的大小，以字节为单位</td>
</tr>
<tr class="even">
<td><code>ndarray.flags</code></td>
<td>ndarray对象的内存信息</td>
</tr>
<tr class="odd">
<td><code>ndarray.real</code></td>
<td>ndarray元素的实部</td>
</tr>
<tr class="even">
<td><code>ndarray.imag</code></td>
<td>ndarray元素的虚部</td>
</tr>
<tr class="odd">
<td><code>ndarray.data</code></td>
<td>包含实际数组元素的缓冲区，由于一般通过数组的索引获取元素，所以通常不需要使用这个属性。</td>
</tr>
</tbody>
</table>
<h4 id="具体操作">2. 具体操作</h4>
<h5 id="numpy的创建">2.1 Numpy的创建</h5>
<h6 id="利用列表生成数组">2.1.1 利用列表生成数组</h6>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">lst=[<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>]</span><br><span class="line">np1=np.array(lst)</span><br><span class="line"><span class="built_in">print</span>(np1,<span class="built_in">type</span>(np1))</span><br></pre></td></tr></table></figure>
<h6 id="利用random模块生成数组">2.1.2 利用random模块生成数组</h6>
<p><img src="/2022/09/18/python%E6%95%B0%E5%AD%A6%E5%BA%93/numpyRandom.png" width="600"> <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="comment">#0到1标准正态分布</span></span><br><span class="line">arr1 = np.random.randn(<span class="number">3</span>, <span class="number">3</span>)</span><br><span class="line"><span class="comment">#0到1均匀分布</span></span><br><span class="line">arr2 = np.random.rand(<span class="number">3</span>, <span class="number">3</span>)</span><br><span class="line"><span class="comment">#均匀分布的随机数（浮点数），前两个参数表示随机数的范围，第三个表示生成随机数的个数</span></span><br><span class="line">arr3 = np.random.uniform(<span class="number">0</span>, <span class="number">10</span>, <span class="number">2</span>)</span><br><span class="line"><span class="comment">#均匀分布的随机数（整数），前两个参数表示随机数的范围，第三个表示生成随机数的个数</span></span><br><span class="line">arr4 = np.random.randint(<span class="number">0</span>, <span class="number">10</span>, <span class="number">3</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&#x27;arr1 : <span class="subst">&#123;arr1&#125;</span>\narr2 : <span class="subst">&#123;arr2&#125;</span>\narr3 : <span class="subst">&#123;arr3&#125;</span>\narr4 : <span class="subst">&#123;arr4&#125;</span>&#x27;</span>)</span><br><span class="line">out : </span><br><span class="line"><span class="comment"># arr1 : [[-0.31637952 -0.08258995  1.43866984]</span></span><br><span class="line"><span class="comment">#  [-0.11216775  0.43881134  0.11745847]</span></span><br><span class="line"><span class="comment">#  [-1.1770306  -0.97657465  2.2368878 ]]</span></span><br><span class="line"><span class="comment"># arr2 : [[0.16350611 0.4467384  0.9465067 ]</span></span><br><span class="line"><span class="comment">#  [0.1882318  0.40261184 0.93577701]</span></span><br><span class="line"><span class="comment">#  [0.56243911 0.69179631 0.83407725]]</span></span><br><span class="line"><span class="comment"># arr3 : [4.41402883 6.03259052]</span></span><br><span class="line"><span class="comment"># arr4 : [9 7 7]</span></span><br></pre></td></tr></table></figure></p>
<h6 id="创建特定形状数组">2.1.3 创建特定形状数组</h6>
<p><img src="/2022/09/18/python%E6%95%B0%E5%AD%A6%E5%BA%93/spacial.png" width="600"> <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">mport numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="comment">#未初始化的数组</span></span><br><span class="line">arr1 = np.empty((<span class="number">2</span>,<span class="number">3</span>))</span><br><span class="line"><span class="comment">#数组元素以 0 来填充</span></span><br><span class="line">arr2 = np.zeros((<span class="number">2</span>, <span class="number">3</span>))</span><br><span class="line"><span class="comment">#数组元素以 1 来填充</span></span><br><span class="line">arr3 = np.ones((<span class="number">2</span>, <span class="number">3</span>))</span><br><span class="line"><span class="comment">#数组以指定的数来进行填充，这里举例3</span></span><br><span class="line">arr4 = np.full((<span class="number">2</span>, <span class="number">3</span>), <span class="number">3</span>)</span><br><span class="line"><span class="comment">#生成单位，对角线上元素为 1，其他为0</span></span><br><span class="line">arr5 = np.eye(<span class="number">2</span>)</span><br><span class="line"><span class="comment">#二维矩阵输出矩阵对角线的元素，一维矩阵形成一个以一维数组为对角线元素的矩阵</span></span><br><span class="line">arr6 = np.diag(np.array([[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>], [<span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>], [<span class="number">7</span>, <span class="number">8</span>, <span class="number">9</span>]]))</span><br></pre></td></tr></table></figure></p>
<h6 id="linspace">2.1.4 linspace</h6>
<p>此函数类似于arange()函数。在此函数中，指定了范围之间的均匀间隔数量，而不是步长。用法如下 <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">numpy.linspace(start, stop, num, endpoint, retstep, dtype)</span><br></pre></td></tr></table></figure> - <code>start</code> 序列的起始值 - <code>stop</code> 序列的终止值，如果endpoint为true，该值包含于序列中 - <code>num</code> 要生成的等间隔样例数量，默认为50 - <code>endpoint</code> 序列中是否包含stop值，默认为ture - <code>retstep</code> 如果为true，返回样例，以及连续数字之间的步长 - <code>dtype</code> 输出ndarray的数据类型</p>
<h5 id="查找和搜索">2.2 查找和搜索</h5>
<p>Numpy可以通过索引或切片来访问和修改，与 Python 中 list 的切片操作一样，设置start, stop 及 step 参数</p>
<h6 id="切片操作">2.2.1 切片操作</h6>
<p><img src="/2022/09/18/python%E6%95%B0%E5%AD%A6%E5%BA%93/index.png" width="600"></p>
<h6 id="元素查找定位">2.2.2 元素查找定位</h6>
<p>Numpy库中提供了<code>where</code>函数来查找满足条件元素的索引，表示如下：</p>
<ul>
<li><code>np.where(condition, x, y)</code>: 满足条件(condition)，输出x，不满足输出y</li>
<li><code>np.where(condition)</code>: 输出满足条件 (即非0) 元素的坐标</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">a=np.array([<span class="number">2</span>,<span class="number">4</span>,<span class="number">6</span>,<span class="number">8</span>,<span class="number">10</span>,<span class="number">12</span>]).reshape(<span class="number">2</span>,<span class="number">3</span>)</span><br><span class="line">x=np.where(a&gt;<span class="number">5</span>)</span><br><span class="line"><span class="built_in">print</span>(a[x])</span><br></pre></td></tr></table></figure>
<p>输出： <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[<span class="number">6</span>  <span class="number">8</span> <span class="number">10</span> <span class="number">12</span>]</span><br></pre></td></tr></table></figure></p>
<h6 id="高级索引">2.2.3 高级索引</h6>
<p>如果一个<code>ndarray</code>是非元组序列，数据类型为整数或布尔值的ndarray，或者至少一个元素为序列对象的元组，我们就能够用它来索引ndarray。高级索引始终返回数据的副本。使用方法 <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"> </span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np </span><br><span class="line"> </span><br><span class="line">x = np.array([[<span class="number">1</span>,  <span class="number">2</span>],  [<span class="number">3</span>,  <span class="number">4</span>],  [<span class="number">5</span>,  <span class="number">6</span>]]) </span><br><span class="line">y = x[[<span class="number">0</span>,<span class="number">1</span>,<span class="number">2</span>],  [<span class="number">0</span>,<span class="number">1</span>,<span class="number">0</span>]]  </span><br><span class="line"><span class="built_in">print</span> y</span><br></pre></td></tr></table></figure> 输出： <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[<span class="number">1</span>,<span class="number">4</span>,<span class="number">5</span>]</span><br></pre></td></tr></table></figure> 解析:上面的x下标索引的两个列表进行两两配对<code>(0,0),(1,1),(2,0)</code>位置，得到`x[0,0],x[1,1],x[2,0]等就是输出结果</p>
<h5 id="删除">2.3 删除</h5>
<p><code>np.delete(arr, index, axis=None)</code></p>
<ul>
<li>第一个参数：要处理的矩阵，</li>
<li>第二个参数，处理的位置，下标</li>
<li>第三个参数，0表示按照行删除，1表示按照列删除，默认为0</li>
<li>返回值为删除后的剩余元素构成的矩阵</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">m=np.delete(a,[<span class="number">0</span>],<span class="number">0</span>)</span><br><span class="line"><span class="built_in">print</span>(m)</span><br></pre></td></tr></table></figure>
<h5 id="numpy的拼接和分割">2.4 numpy的拼接和分割</h5>
<h6 id="拼接">2.4.1 拼接</h6>
<p>下面的图列举了常见的用于数组或向量 合并的方法。 <img src="/2022/09/18/python%E6%95%B0%E5%AD%A6%E5%BA%93/append.png" width="600"></p>
<ul>
<li><code>append、concatenate</code>以及<code>stack</code>都有一个<code>axis</code>参数，用于控制数组的合 并方式是按行还是按列。</li>
<li>对于<code>append</code>和<code>concatenate</code>，待合并的数组必须有相同的行数或列数</li>
<li><code>stack、hstack、dstack</code>，要求待合并的数组必须具有相同的形状</li>
</ul>
<h6 id="分割">2.4.2 分割</h6>
<ul>
<li>水平分割：<code>np.split(arr,n,axis=1) 或 np.hsplit(arr,n)</code>：按列分成<code>n</code>份。返回一个<code>list</code></li>
<li>垂直分割：<code>np.split(arr,n,axis=0)</code> 或 <code>np.vsplit(arr,n)</code>：按行分成n份，返回一个<code>list</code></li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">splitTes=np.split(a,<span class="number">2</span>,<span class="number">0</span>)</span><br><span class="line">splitTes2=np.split(a,<span class="number">3</span>,<span class="number">1</span>)</span><br></pre></td></tr></table></figure>
<h5 id="维度变换">2.5 维度变换</h5>
<p>在机器学习以及深度学习的任务中，通常需要将处理好的数据以模型能 接收的格式输入给模型，然后由模型通过一系列的运算，最终返回一个处理 结果。然而，由于不同模型所接收的输入格式不一样，往往需要先对其进行 一系列的变形和运算，从而将数据处理成符合模型要求的格式。在矩阵或者 数组的运算中，经常会遇到需要把多个向量或矩阵按某轴方向合并，或展平 (如在卷积或循环神经网络中，在全连接层之前，需要把矩阵展平)的情 况。下面介绍几种常用的数据变形方法。 <img src="/2022/09/18/python%E6%95%B0%E5%AD%A6%E5%BA%93/switch.png" width="600"></p>
<h6 id="reshape">2.5.1 reshape</h6>
<p>不会改变数组内额元素，返回一个指定的shap维度的数组，按原顺序放置元素 <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#三行四列，如果单指定行数或列数，另一个参数可设置为-1</span></span><br><span class="line">x=np.arange(<span class="number">12</span>).reshape(<span class="number">3</span>,<span class="number">4</span>)</span><br></pre></td></tr></table></figure></p>
<h6 id="resize">2.5.2 resize</h6>
<p>改变向量的维度，同<code>reshape</code>作用 <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#二行五列</span></span><br><span class="line">arr=np.arange(<span class="number">10</span>)</span><br><span class="line">arr.resize(<span class="number">2</span>,<span class="number">5</span>)</span><br></pre></td></tr></table></figure></p>
<h6 id="转置">2.5.3 转置</h6>
<p>将矩阵转置 <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">arr=np.nrange(<span class="number">12</span>).reshape(<span class="number">3</span>,<span class="number">4</span>)</span><br><span class="line"><span class="built_in">print</span>(arr.T)</span><br></pre></td></tr></table></figure></p>
<h6 id="向量展平">2.5.4 向量展平</h6>
<p>将多维数组转为一维数组，不会产生数组的副本 <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">arr=arr.ravel()</span><br></pre></td></tr></table></figure></p>
<h6 id="向量展平之flatten">2.5.5 向量展平之flatten</h6>
<p>把矩阵转换为向量，这种需求经常出现在卷积网络与全连接层之间 <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">arr = np.arange(<span class="number">8</span>).reshape(<span class="number">2</span>, <span class="number">4</span>)</span><br><span class="line">arr.flatten() <span class="comment"># out : array([0, 1, 2, 3, 4, 5, 6, 7])</span></span><br></pre></td></tr></table></figure></p>
<h6 id="squeeze">2.5.6 squeeze</h6>
<p>这是一个降维的函数，把矩阵中为1维的维度去掉 <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">arr = np.arange(<span class="number">8</span>).reshape(<span class="number">2</span>, <span class="number">4</span>, <span class="number">1</span>)</span><br><span class="line">arr.shape <span class="comment"># out : (2, 4, 1)</span></span><br><span class="line">arr.squeeze().shape <span class="comment"># out : (2, 4)</span></span><br></pre></td></tr></table></figure></p>
<h6 id="transpose">2.5.7 transpose</h6>
<p>对高维矩阵进行轴对换，这个在深度学习中经常使用，比如把图片中表 示颜色顺序的RGB改为GBR。 <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">arr = np.arange(<span class="number">12</span>).reshape(<span class="number">2</span>, <span class="number">6</span>, <span class="number">1</span>)</span><br><span class="line">arr.shape <span class="comment"># out : (2, 6, 1)</span></span><br><span class="line">arr.transpose(<span class="number">1</span>, <span class="number">2</span>, <span class="number">0</span>).shape <span class="comment"># out : (6, 1, 2)</span></span><br></pre></td></tr></table></figure></p>
<h5 id="矩阵运算">2.6 矩阵运算</h5>
<h6 id="对于元素相乘">2.6.1 对于元素相乘</h6>
<p>对应元素相乘（Element-Wise Product）是两个矩阵中对应元素乘积。 np.multiply函数用于数组或矩阵对应元素相乘，输出与相乘数组或矩阵的大 小一致。 <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">a = np.array([[<span class="number">1</span>,<span class="number">0</span>],[<span class="number">0</span>,<span class="number">1</span>]])</span><br><span class="line">b = np.array([[<span class="number">4</span>,<span class="number">1</span>],[<span class="number">2</span>,<span class="number">2</span>]])</span><br><span class="line">np.multiply(a, b) <span class="comment"># 等效于a * b，out : array([[4, 0], [0, 2]])</span></span><br></pre></td></tr></table></figure></p>
<h6 id="点积">2.6.2 点积</h6>
<p>点积运算(Dot Product)又称为内积，在Numpy用np.dot或者np.matmul表示 <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">a = np.array([[<span class="number">1</span>,<span class="number">0</span>],[<span class="number">0</span>,<span class="number">1</span>]])</span><br><span class="line">b = np.array([[<span class="number">4</span>,<span class="number">1</span>],[<span class="number">2</span>,<span class="number">2</span>]])</span><br><span class="line">np.dot(a, b) <span class="comment"># 等效于np.matmul(a, b) out : array([[4, 1], [2, 2]])</span></span><br></pre></td></tr></table></figure></p>
<h6 id="行列式">2.6.3 行列式</h6>
<p>计算行列式的值 <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">arr = np.array([[<span class="number">1</span>,<span class="number">2</span>], [<span class="number">3</span>,<span class="number">4</span>]]) </span><br><span class="line">np.linalg.det(arr) <span class="comment"># out : -2.0000000000000004</span></span><br></pre></td></tr></table></figure></p>
<h6 id="求逆">2.6.4 求逆</h6>
<p>求逆 <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">arr = np.array([[<span class="number">1</span>,<span class="number">2</span>], [<span class="number">3</span>,<span class="number">4</span>]])</span><br><span class="line">np.linalg.inv(arr) <span class="comment"># out : array([[-2. ,  1. ], [ 1.5, -0.5]])</span></span><br></pre></td></tr></table></figure></p>
<h6 id="特征值和特征向量">2.6.5 特征值和特征向量</h6>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">A = np.random.<span class="built_in">randint</span>(<span class="number">-10</span>,<span class="number">10</span>,(<span class="number">4</span>,<span class="number">4</span>))</span><br><span class="line">C = np.<span class="built_in">dot</span>(A.T, A)</span><br><span class="line">vals, vecs = np.linalg.<span class="built_in">eig</span>(C) </span><br><span class="line"><span class="built_in">print</span>(f<span class="number">&#x27;</span>特征值 : &#123;vals&#125;, 特征向量 : &#123;vecs&#125;<span class="string">&#x27;)</span></span><br><span class="line"><span class="string">#out : </span></span><br><span class="line"><span class="string">特征值 : [395.26566729 358.52489695  44.41465068  52.79478508]</span></span><br><span class="line"><span class="string">特征向量 : [[ 0.30221599  0.64309202 -0.64757004 -0.27522935]</span></span><br><span class="line"><span class="string">             [ 0.87819925 -0.03518532  0.18871425  0.43808105]</span></span><br><span class="line"><span class="string">             [-0.35779498  0.26192443 -0.27010759  0.85464626]</span></span><br><span class="line"><span class="string">             [ 0.09702746 -0.71874212 -0.68708214  0.04374437]]</span></span><br><span class="line"><span class="string"></span></span><br></pre></td></tr></table></figure>
<h5 id="numpy--matplotlib">2.7 numpy- Matplotlib</h5>
<p>Matplotlib 是 Python 的绘图库。 它可与 NumPy 一起使用，提供了一种有效的 MatLab 开源替代方案。 它也可以和图形工具包一起使用，如 PyQt 和 wxPython。 <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> matplotlib <span class="keyword">import</span> pyplot <span class="keyword">as</span> plt</span><br></pre></td></tr></table></figure> 这里<code>pyplot()</code>是 <code>matplotlib</code>库中最重要的函数，用于绘制 2D 数据。 以下脚本绘制方程<code>y = 2x + 5：</code></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np </span><br><span class="line"><span class="keyword">from</span> matplotlib <span class="keyword">import</span> pyplot <span class="keyword">as</span> plt </span><br><span class="line"> </span><br><span class="line">x = np.arange(<span class="number">1</span>,<span class="number">11</span>) <span class="comment">#x=np.linspace(1,11,2000)</span></span><br><span class="line">y =  <span class="number">2</span>  * x +  <span class="number">5</span> </span><br><span class="line">plt.title(<span class="string">&quot;Matplotlib demo&quot;</span>) </span><br><span class="line">plt.xlabel(<span class="string">&quot;x axis caption&quot;</span>) </span><br><span class="line">plt.ylabel(<span class="string">&quot;y axis caption&quot;</span>) </span><br><span class="line">plt.plot(x,y) plt.show()</span><br></pre></td></tr></table></figure>
<p><a target="_blank" rel="noopener" href="https://quantfabric.blog.csdn.net/article/details/123096127?spm=1001.2014.3001.5502">Numpy详细教程</a></p>
<h4 id="pandas概述">3.pandas概述</h4>
<p>Pandas是Python的一个大数据处理模块。Pandas使用一个二维的数据结构DataFrame来表示表格式的数据，相比较于Numpy，Pandas可以存储混合的数据结构，同时使用NaN来表示缺失的数据，而不用像Numpy一样要手工处理缺失的数据，并且Pandas使用轴标签来表示行和列。它具有：</p>
<ul>
<li>便捷的数据处理能力</li>
<li>独特的数据结构</li>
<li>读取文件方便</li>
<li>封装了<code>matplotlib</code>的画图和<code>numpy</code>的计算</li>
</ul>
<h5 id="pandas的数据结构">3.1 pandas的数据结构</h5>
<p>Pandas有三种主要数据结构<code>Series、DataFrame、Panel</code>。</p>
<ul>
<li><p><code>Series</code>是带有标签的一维数组，可以保存任何数据类型（整数，字符串，浮点数，Python对象等），轴标签统称为索引（index）。</p></li>
<li><p><code>DataFrame</code>是带有标签的二维数据结构，具有<code>index</code>（行标签）和<code>columns</code>（列标签）。如果传递<code>index</code>或<code>columns</code>，则会用于生成的D<code>ataFrame</code>的<code>index</code>或<code>columns</code>。</p></li>
<li><p><code>Panel</code>是一个三维数据结构，由<code>items、major_axis、minor_axis</code>定义。<code>items</code>（条目），即轴0，每个条目对应一个<code>DataFrame</code>；<code>major_axis（主轴）</code>，即轴1，是每个<code>DataFrame</code>的index（行）；<code>minor_axis</code>（副轴），即轴2，是每个<code>DataFrame</code>的columns（列）。</p></li>
</ul>
<h5 id="series">3.2 Series</h5>
<p>Series是能够保存任何类型数据(整数，字符串，浮点数，Python对象等)的一维标记数组，轴标签统称为index（索引）。series是一种一维数据结构，每一个元素都带有一个索引，其中索引可以为数字或字符串。Series结构名称： <img src="/2022/09/18/python%E6%95%B0%E5%AD%A6%E5%BA%93/Series.png" width="400"></p>
<h6 id="构造对象">3.2.1 构造对象</h6>
<p>Series的构造函数如下： <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pandas.Series( data, index, dtype, copy)</span><br></pre></td></tr></table></figure> - <code>data</code>：构建Series的数据，可以是ndarray，list，dict，constants。 - <code>index</code>：索引值必须是唯一的和散列的，与数据的长度相同。 如果没有索引被传递，默认为np.arange(n)。 - <code>dtype</code>：数据类型，如果没有，将推断数据类型。 - <code>copy</code>：是否复制数据，默认为false。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    data = np.array([<span class="string">&#x27;a&#x27;</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">4</span>, <span class="number">6</span>])</span><br><span class="line">    s = pd.Series(data,index=[<span class="number">101</span>, <span class="number">102</span>, <span class="number">103</span>, <span class="string">&#x27;hello&#x27;</span>, <span class="string">&#x27;world&#x27;</span>])</span><br><span class="line">    <span class="built_in">print</span>(s)</span><br><span class="line"><span class="comment"># output:</span></span><br><span class="line"><span class="comment"># 101      a</span></span><br><span class="line"><span class="comment"># 102      1</span></span><br><span class="line"><span class="comment"># 103      2</span></span><br><span class="line"><span class="comment"># hello    4</span></span><br><span class="line"><span class="comment"># world    6</span></span><br><span class="line"><span class="comment"># dtype: object</span></span><br></pre></td></tr></table></figure>
<p>使用<code>ndarray</code>作为数据时，传递的索引必须与<code>ndarray</code>具有相同的长度。 如果没有传递索引值，那么默认的索引是<code>range(n)</code>，其中n是数组长度，即<code>[0,1,2,3…. range(len(array))-1] - 1]</code>。</p>
<h6 id="series数据的访问">3.2.2 Series数据的访问</h6>
<p>Series中的数据可以使用有序序列的方式进行访问。 <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    s = pd.Series([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>], index=[<span class="string">&#x27;a&#x27;</span>, <span class="string">&#x27;b&#x27;</span>, <span class="string">&#x27;c&#x27;</span>, <span class="string">&#x27;d&#x27;</span>, <span class="string">&#x27;e&#x27;</span>])</span><br><span class="line">    <span class="built_in">print</span>(s[<span class="number">0</span>])</span><br><span class="line">    <span class="built_in">print</span>(s[-<span class="number">1</span>])</span><br><span class="line">    <span class="built_in">print</span>(s[-<span class="number">3</span>:])</span><br><span class="line"></span><br><span class="line"><span class="comment"># output:</span></span><br><span class="line"><span class="comment"># 1</span></span><br><span class="line"><span class="comment"># 5</span></span><br><span class="line"><span class="comment"># c    3</span></span><br><span class="line"><span class="comment"># d    4</span></span><br><span class="line"><span class="comment"># e    5</span></span><br><span class="line"><span class="comment"># dtype: int64</span></span><br></pre></td></tr></table></figure></p>
<h6 id="series属性">3.2.3 Series属性</h6>
<ul>
<li><code>Series.axes</code>：返回行轴标签列表</li>
<li><code>Series.dtype</code>：返回对象的数据类型</li>
<li><code>Series.empty</code>：如果对象为空，返回True</li>
<li><code>Series.ndim</code>：返回底层数据的维数，默认为1</li>
<li><code>Series.size</code>：返回基础数据中的元素数</li>
<li><code>Series.values</code>：将对象作为ndarray返回</li>
<li><code>Series.head()</code>：返回前n行</li>
<li><code>Series.tail()</code>：返回后n行</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"> </span><br><span class="line"> </span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    s = pd.Series([<span class="string">&quot;Bauer&quot;</span>, <span class="number">30</span>, <span class="number">90</span>], index=[<span class="string">&#x27;Name&#x27;</span>, <span class="string">&#x27;Age&#x27;</span>, <span class="string">&#x27;Score&#x27;</span>])</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;Series=================&quot;</span>)</span><br><span class="line">    <span class="built_in">print</span>(s)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;axes===================&quot;</span>)</span><br><span class="line">    <span class="built_in">print</span>(s.axes)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;dtype==================&quot;</span>)</span><br><span class="line">    <span class="built_in">print</span>(s.dtype)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;empty==================&quot;</span>)</span><br><span class="line">    <span class="built_in">print</span>(s.empty)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;ndim===================&quot;</span>)</span><br><span class="line">    <span class="built_in">print</span>(s.ndim)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;size===================&quot;</span>)</span><br><span class="line">    <span class="built_in">print</span>(s.size)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;values=================&quot;</span>)</span><br><span class="line">    <span class="built_in">print</span>(s.values)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;head()=================&quot;</span>)</span><br><span class="line">    <span class="built_in">print</span>(s.head(<span class="number">2</span>))</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;tail()=================&quot;</span>)</span><br><span class="line">    <span class="built_in">print</span>(s.tail(<span class="number">2</span>))</span><br><span class="line"> </span><br><span class="line"><span class="comment"># output:</span></span><br><span class="line"><span class="comment"># Series=================</span></span><br><span class="line"><span class="comment"># Name     Bauer</span></span><br><span class="line"><span class="comment"># Age         30</span></span><br><span class="line"><span class="comment"># Score       90</span></span><br><span class="line"><span class="comment"># dtype: object</span></span><br><span class="line"><span class="comment"># axes===================</span></span><br><span class="line"><span class="comment"># [Index([&#x27;Name&#x27;, &#x27;Age&#x27;, &#x27;Score&#x27;], dtype=&#x27;object&#x27;)]</span></span><br><span class="line"><span class="comment"># dtype==================</span></span><br><span class="line"><span class="comment"># object</span></span><br><span class="line"><span class="comment"># empty==================</span></span><br><span class="line"><span class="comment"># False</span></span><br><span class="line"><span class="comment"># ndim===================</span></span><br><span class="line"><span class="comment"># 1</span></span><br><span class="line"><span class="comment"># size===================</span></span><br><span class="line"><span class="comment"># 3</span></span><br><span class="line"><span class="comment"># values=================</span></span><br><span class="line"><span class="comment"># [&#x27;Bauer&#x27; 30 90]</span></span><br><span class="line"><span class="comment"># head()=================</span></span><br><span class="line"><span class="comment"># Name    Bauer</span></span><br><span class="line"><span class="comment"># Age        30</span></span><br><span class="line"><span class="comment"># dtype: object</span></span><br><span class="line"><span class="comment"># tail()=================</span></span><br><span class="line"><span class="comment"># Age      30</span></span><br><span class="line"><span class="comment"># Score    90</span></span><br><span class="line"><span class="comment"># dtype: object</span></span><br></pre></td></tr></table></figure>
<h5 id="dataframe">3.3 DataFrame</h5>
<p>数据帧(DataFrame)是二维的表格型数据结构，即数据以行和列的表格方式排列，DataFrame是Series的容器。 <img src="/2022/09/18/python%E6%95%B0%E5%AD%A6%E5%BA%93/DataFrame.png" width="400"> <strong>特点：</strong></p>
<ul>
<li>底层数据列是不同的类型</li>
<li>大小可变</li>
<li>标记轴(行和列)</li>
<li>可以对行和列执行算术运算</li>
</ul>
<h6 id="dataframe对象构造">3.3.1 DataFrame对象构造</h6>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pandas.DataFrame( data, index, columns, dtype, copy)</span><br></pre></td></tr></table></figure>
<ul>
<li><code>data</code>：构建DataFrame的数据，可以是ndarray，series，map，lists，dict，constant和其它DataFrame。</li>
<li><code>index</code>：行索引标签，如果没有传递索引值，索引默认为np.arrange(n)。</li>
<li><code>columns</code>：列索引标签，如果没有传递索列引值，默认列索引是np.arange(n)。</li>
<li><code>dtype</code>：每列的数据类型。</li>
<li><code>copy</code>：如果默认值为False，则此命令(或任何它)用于复制数据。</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    data = &#123;<span class="string">&#x27;one&#x27;</span>: pd.Series([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>], index=[<span class="string">&#x27;a&#x27;</span>, <span class="string">&#x27;b&#x27;</span>, <span class="string">&#x27;c&#x27;</span>]),</span><br><span class="line">            <span class="string">&#x27;two&#x27;</span>: pd.Series([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>], index=[<span class="string">&#x27;a&#x27;</span>, <span class="string">&#x27;b&#x27;</span>, <span class="string">&#x27;c&#x27;</span>, <span class="string">&#x27;d&#x27;</span>]),</span><br><span class="line">            <span class="string">&#x27;three&#x27;</span>: pd.Series([<span class="number">10</span>, <span class="number">20</span>, <span class="number">30</span>], index=[<span class="string">&#x27;a&#x27;</span>, <span class="string">&#x27;b&#x27;</span>, <span class="string">&#x27;c&#x27;</span>])&#125;</span><br><span class="line">    df = pd.DataFrame(data, columns=[<span class="string">&#x27;one&#x27;</span>, <span class="string">&#x27;two&#x27;</span>, <span class="string">&#x27;three&#x27;</span>])</span><br><span class="line">    <span class="built_in">print</span>(df)</span><br><span class="line">    <span class="keyword">del</span>(df[<span class="string">&#x27;two&#x27;</span>])</span><br><span class="line">    <span class="built_in">print</span>(df)</span><br><span class="line"></span><br><span class="line"><span class="comment"># output:</span></span><br><span class="line"><span class="comment">#  one  two  three</span></span><br><span class="line"><span class="comment"># a  1.0    1   10.0</span></span><br><span class="line"><span class="comment"># b  2.0    2   20.0</span></span><br><span class="line"><span class="comment"># c  3.0    3   30.0</span></span><br><span class="line"><span class="comment"># d  NaN    4    NaN</span></span><br><span class="line"><span class="comment">#    one  three</span></span><br><span class="line"><span class="comment"># a  1.0   10.0</span></span><br><span class="line"><span class="comment"># b  2.0   20.0</span></span><br><span class="line"><span class="comment"># c  3.0   30.0</span></span><br><span class="line"><span class="comment"># d  NaN    NaN</span></span><br></pre></td></tr></table></figure>
<h6 id="dataframe属性">3.3.2 DataFrame属性</h6>
<ul>
<li><code>DataFrame.T</code>：转置行和列</li>
<li><code>DataFrame.axes</code>：返回一个列，行轴标签和列轴标签作为唯一的成员。</li>
<li><code>DataFrame.dtypes</code>：返回对象的数据类型</li>
<li><code>DataFrame.empty</code>：如果NDFrame完全为空，返回True</li>
<li><code>DataFrame.ndim</code>：返回轴/数组维度的大小</li>
<li><code>DataFrame.shape</code>：返回表示DataFrame维度的元组</li>
<li><code>DataFrame.size</code>：返回DataFrame的元素数</li>
<li><code>DataFrame.values</code>：将对象作为ndarray返回</li>
<li><code>DataFrame.head()</code>：返回前n行</li>
<li><code>DataFrame.tail()</code>：返回后n行</li>
</ul>
<h6 id="示例">3.3.3 示例</h6>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 导入库</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建一个符合正态分布的10个股票5天的涨跌幅数据,0表示正态分布的中心点，1表示标准宽度，越大曲线越矮越胖，[10,5]数据shape</span></span><br><span class="line">stocks = np.random.normal(<span class="number">0</span>, <span class="number">1</span>, [<span class="number">10</span>, <span class="number">5</span>])</span><br><span class="line"><span class="comment"># 使用pandas中的数据结构 DataFrame() 处理数据</span></span><br><span class="line">stocks_rise = pd.DataFrame(stocks)</span><br><span class="line"><span class="comment"># 设置行索引,获取行数 stocks_rise.shape[0] 进行遍历 列表生成式</span></span><br><span class="line">index_row = [<span class="string">&#x27;股票&#123;&#125;&#x27;</span>.<span class="built_in">format</span>(i+<span class="number">1</span>) <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(stocks_rise.shape[<span class="number">0</span>])]</span><br><span class="line"><span class="comment"># 设置列索引，日期是一个时间序列，为了简便，使用pd.date_range()生成一组连续的时间序列</span></span><br><span class="line"><span class="comment"># pd.date_range(start,end,periods,freq) start:开始时间, end:结束时间</span></span><br><span class="line"><span class="comment"># periods:时间天数, freq:递进单位，默认1天，&#x27;B&#x27;默认略过周末</span></span><br><span class="line">index_col = pd.date_range(start=<span class="string">&#x27;20220201&#x27;</span>,periods=stocks_rise.shape[<span class="number">1</span>],freq=<span class="string">&#x27;B&#x27;</span>)</span><br><span class="line"><span class="comment"># 添加索引，注意数据是ndarray数据 index表示行索引，columns表示列索引</span></span><br><span class="line">data = pd.DataFrame(stocks, index=index_row, columns=index_col)</span><br><span class="line"><span class="built_in">print</span>(data)</span><br><span class="line"></span><br><span class="line">      <span class="number">2022</span>-02-01  <span class="number">2022</span>-02-02  <span class="number">2022</span>-02-03  <span class="number">2022</span>-02-04  <span class="number">2022</span>-02-07</span><br><span class="line">股票<span class="number">1</span>    -<span class="number">2.054041</span>   -<span class="number">1.170757</span>    <span class="number">0.162393</span>    <span class="number">0.253333</span>   -<span class="number">1.638837</span></span><br><span class="line">股票<span class="number">2</span>    -<span class="number">1.463734</span>    <span class="number">0.408459</span>    <span class="number">0.530070</span>   -<span class="number">0.925281</span>    <span class="number">1.454630</span></span><br><span class="line">股票<span class="number">3</span>    -<span class="number">0.511517</span>   -<span class="number">0.827591</span>   -<span class="number">2.076265</span>    <span class="number">0.139486</span>    <span class="number">0.658707</span></span><br><span class="line">股票<span class="number">4</span>    -<span class="number">1.698789</span>    <span class="number">0.250902</span>   -<span class="number">0.624713</span>    <span class="number">1.378845</span>   -<span class="number">1.672292</span></span><br><span class="line">股票<span class="number">5</span>     <span class="number">0.683233</span>   -<span class="number">1.083694</span>    <span class="number">0.810567</span>    <span class="number">0.421215</span>    <span class="number">1.375385</span></span><br><span class="line">股票<span class="number">6</span>    -<span class="number">0.296111</span>   -<span class="number">0.946959</span>    <span class="number">0.836536</span>   -<span class="number">1.179879</span>   -<span class="number">0.397406</span></span><br><span class="line">股票<span class="number">7</span>     <span class="number">0.017772</span>    <span class="number">0.180210</span>    <span class="number">2.022776</span>    <span class="number">0.436337</span>   -<span class="number">1.555866</span></span><br><span class="line">股票<span class="number">8</span>     <span class="number">0.638262</span>   -<span class="number">0.790932</span>    <span class="number">1.077822</span>   -<span class="number">1.746631</span>   -<span class="number">0.591360</span></span><br><span class="line">股票<span class="number">9</span>    -<span class="number">0.681391</span>   -<span class="number">0.613255</span>   -<span class="number">1.849094</span>    <span class="number">0.438304</span>   -<span class="number">0.503742</span></span><br><span class="line">股票<span class="number">10</span>   -<span class="number">0.243500</span>   -<span class="number">1.733623</span>   -<span class="number">1.137840</span>    <span class="number">0.124976</span>   -<span class="number">0.415727</span></span><br></pre></td></tr></table></figure>
<h5 id="panel">3.4 panel</h5>
<p>panel 是三维的数据结构，是DataFrame的容器，Panel的3个轴如下：</p>
<ul>
<li><code>items</code>：axis 0，每个项目对应于内部包含的数据帧(DataFrame)。</li>
<li><code>major_axis</code> ：axis 1，是每个数据帧(DataFrame)的索引(行)。</li>
<li><code>minor_axis</code>：axis 2，是每个数据帧(DataFrame)的列。</li>
</ul>
<h6 id="panel对象构造">3.4.1 Panel对象构造</h6>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pandas.Panel(data, items, major_axis, minor_axis, dtype, copy)</span><br></pre></td></tr></table></figure>
<ul>
<li><code>data</code>：构建Panel的数据，采取各种形式，如<code>ndarray，series，map，lists，dict，constant</code>和另一个数据帧<code>DataFrame</code>。</li>
<li><code>items</code>：axis=0</li>
<li><code>major_axis</code>：axis=1</li>
<li><code>minor_axis</code>：axis=2</li>
<li><code>dtype</code>：每列的数据类型</li>
<li><code>copy</code>：复制数据，默认 - false</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"> </span><br><span class="line"> </span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    data = np.random.rand(<span class="number">2</span>, <span class="number">4</span>, <span class="number">5</span>)</span><br><span class="line">    p = pd.Panel(data, items=[<span class="string">&quot;item1&quot;</span>, <span class="string">&quot;item2&quot;</span>], major_axis=[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>], minor_axis=[<span class="string">&#x27;a&#x27;</span>, <span class="string">&#x27;b&#x27;</span>, <span class="string">&#x27;c&#x27;</span>, <span class="string">&#x27;d&#x27;</span>, <span class="string">&#x27;e&#x27;</span>])</span><br><span class="line">    <span class="built_in">print</span>(p)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;item1&quot;</span>)</span><br><span class="line">    <span class="built_in">print</span>(p[<span class="string">&quot;item1&quot;</span>])</span><br><span class="line">    <span class="built_in">print</span>(p.major_xs(<span class="number">2</span>))</span><br><span class="line">    <span class="built_in">print</span>(p.minor_xs(<span class="string">&#x27;b&#x27;</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># output:</span></span><br><span class="line"><span class="comment"># &lt;class &#x27;pandas.core.panel.Panel&#x27;&gt;</span></span><br><span class="line"><span class="comment"># Dimensions: 2 (items) x 4 (major_axis) x 5 (minor_axis)</span></span><br><span class="line"><span class="comment"># Items axis: item1 to item2</span></span><br><span class="line"><span class="comment"># Major_axis axis: 1 to 4</span></span><br><span class="line"><span class="comment"># Minor_axis axis: a to e</span></span><br><span class="line"><span class="comment"># item1</span></span><br><span class="line"><span class="comment">#           a         b         c         d         e</span></span><br><span class="line"><span class="comment"># 1  0.185626  0.976123  0.566263  0.273208  0.675442</span></span><br><span class="line"><span class="comment"># 2  0.209664  0.205190  0.217200  0.158447  0.400683</span></span><br><span class="line"><span class="comment"># 3  0.499591  0.963321  0.759330  0.089930  0.362824</span></span><br><span class="line"><span class="comment"># 4  0.723158  0.585642  0.629246  0.886086  0.493039</span></span><br><span class="line"><span class="comment">#       item1     item2</span></span><br><span class="line"><span class="comment"># a  0.209664  0.592154</span></span><br><span class="line"><span class="comment"># b  0.205190  0.661562</span></span><br><span class="line"><span class="comment"># c  0.217200  0.743716</span></span><br><span class="line"><span class="comment"># d  0.158447  0.055882</span></span><br><span class="line"><span class="comment"># e  0.400683  0.245760</span></span><br><span class="line"><span class="comment">#       item1     item2</span></span><br><span class="line"><span class="comment"># 1  0.976123  0.630320</span></span><br><span class="line"><span class="comment"># 2  0.205190  0.661562</span></span><br><span class="line"><span class="comment"># 3  0.963321  0.741791</span></span><br><span class="line"><span class="comment"># 4  0.585642  0.729366</span></span><br></pre></td></tr></table></figure>
<h6 id="panel数据索引">3.4.2 Panel数据索引</h6>
<p>Panel右三个数据索引，因此可使用它们获取相应的结果，这里使用<code>Items</code>访问<code>Panel</code>可以获取相应的<code>DataFrame</code> <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    data = &#123;<span class="string">&#x27;Table1&#x27;</span>: pd.DataFrame(np.random.randn(<span class="number">4</span>, <span class="number">3</span>),</span><br><span class="line">                                   index=[<span class="string">&#x27;rank1&#x27;</span>, <span class="string">&#x27;rank2&#x27;</span>, <span class="string">&#x27;rank3&#x27;</span>, <span class="string">&#x27;rank4&#x27;</span>],</span><br><span class="line">                                   columns=[<span class="string">&#x27;Name&#x27;</span>, <span class="string">&#x27;Age&#x27;</span>, <span class="string">&#x27;Score&#x27;</span>]),</span><br><span class="line">            <span class="string">&#x27;Table2&#x27;</span>: pd.DataFrame(np.random.randn(<span class="number">4</span>, <span class="number">3</span>),</span><br><span class="line">                                   index=[<span class="string">&#x27;rank1&#x27;</span>, <span class="string">&#x27;rank2&#x27;</span>, <span class="string">&#x27;rank3&#x27;</span>, <span class="string">&#x27;rank4&#x27;</span>],</span><br><span class="line">                                   columns=[<span class="string">&#x27;Name&#x27;</span>, <span class="string">&#x27;Age&#x27;</span>, <span class="string">&#x27;Score&#x27;</span>]</span><br><span class="line">                                   )</span><br><span class="line">            &#125;</span><br><span class="line">    p = pd.Panel(data)</span><br><span class="line">    <span class="built_in">print</span>(p[<span class="string">&#x27;Table1&#x27;</span>])</span><br><span class="line"> </span><br><span class="line"><span class="comment"># output:</span></span><br><span class="line"><span class="comment">#            Name       Age     Score</span></span><br><span class="line"><span class="comment"># rank1 -1.240644 -0.820041  1.656150</span></span><br><span class="line"><span class="comment"># rank2  1.830655 -0.258068 -0.728560</span></span><br><span class="line"><span class="comment"># rank3  1.268695  1.259693 -1.005151</span></span><br><span class="line"><span class="comment"># rank4 -0.139876  0.611589  2.343394</span></span><br></pre></td></tr></table></figure></p>
<h6 id="panel属性">3.4.3 Panel属性</h6>
<ul>
<li><code>Panel.T</code>：转置行和列</li>
<li><code>Panel.axes</code>：返回一个列，行轴标签和列轴标签作为唯一的成员。</li>
<li><code>Panel.dtypes</code>：返回对象的数据类型</li>
<li><code>Panel.empty</code>：如果NDFrame完全为空，返回True</li>
<li><code>Panel.ndim</code>：返回轴/数组维度的大小</li>
<li><code>Panel.shape</code>：返回表示DataFrame维度的元组</li>
<li><code>Panel.size</code>：返回DataFrame的元素数</li>
<li><code>Panel.values</code>：将对象作为ndarray返回</li>
</ul>
<p><a target="_blank" rel="noopener" href="https://blog.csdn.net/weixin_45698637/article/details/122766366">Pandas教程</a></p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/2022/09/17/python%E7%8E%AF%E5%A2%83%E9%85%8D%E7%BD%AE/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="trluper">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Trluper">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/09/17/python%E7%8E%AF%E5%A2%83%E9%85%8D%E7%BD%AE/" class="post-title-link" itemprop="url">python环境配置</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2022-09-17 14:02:12" itemprop="dateCreated datePublished" datetime="2022-09-17T14:02:12+08:00">2022-09-17</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2022-10-09 20:04:49" itemprop="dateModified" datetime="2022-10-09T20:04:49+08:00">2022-10-09</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/python/" itemprop="url" rel="index"><span itemprop="name">python</span></a>
                </span>
            </span>

          <br>
            <span class="post-meta-item" title="本文字数">
              <span class="post-meta-item-icon">
                <i class="far fa-file-word"></i>
              </span>
                <span class="post-meta-item-text">本文字数：</span>
              <span>6.8k</span>
            </span>
            <span class="post-meta-item" title="阅读时长">
              <span class="post-meta-item-icon">
                <i class="far fa-clock"></i>
              </span>
                <span class="post-meta-item-text">阅读时长 &asymp;</span>
              <span>6 分钟</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h4 id="使用anaconda">1. 使用Anaconda</h4>
<p>Anaconda，中文大蟒蛇，是一个开源的Python发行版本，其包含了conda、Python等180多个科学包及其依赖.可通过<a target="_blank" rel="noopener" href="https://mirrors.tuna.tsinghua.edu.cn/anaconda/archive/">Anaconda下载</a>。选择合适版本。</p>
<p><strong><em>注意</em></strong>：一个python环境应当只安装一个版本的tensorflow，如果还需要安装其他版本的tensorflow，应该再创建一个python环境</p>
<h5 id="anaconda创建指定python环境">1.1 anaconda创建指定python环境</h5>
<p>主要命令为配置自己环境<code>conda create -n 环境的名字 python=版本号</code></p>
<ul>
<li>步骤一：打开Anaconda powershell prompt。</li>
<li>步骤二：输入命令<code>conda create -n py36 anaconda=2020.02 python=3.6</code>，版本<code>python=3.x</code>自己指定。</li>
<li>步骤三：应用该环境<code>conda activate py36</code></li>
<li>下载的环境会在<code>D:\pythonDeve\Anaconda\envs\py36</code>处</li>
</ul>
<blockquote>
<p>退出该环境使用<code>conda deactivate py36</code> 显示当前所有已发现的环境：<code>conda info --envs</code></p>
</blockquote>
<h5 id="pycharm中导入该python版本编译环境">1.2pycharm中导入该python版本编译环境：</h5>
<ul>
<li>步骤一：打开pycharm，点击<code>File\settings\project:项目名</code> <img src="/2022/09/17/python%E7%8E%AF%E5%A2%83%E9%85%8D%E7%BD%AE/one.png" width="500"></li>
<li>步骤二：选择上面提到的路径导入 <img src="/2022/09/17/python%E7%8E%AF%E5%A2%83%E9%85%8D%E7%BD%AE/two.png" width="500"></li>
</ul>
<h5 id="配置python环境变量">1.3 配置python环境变量</h5>
<p>配置python环境变量是为了能在命令行窗口中能够找到该命令。一般建议配置，因为后续的许多安装都要用到<code>pip</code>和<code>conda</code>。 <img src="/2022/09/17/python%E7%8E%AF%E5%A2%83%E9%85%8D%E7%BD%AE/three.png" width="500"></p>
<h5 id="安装指定版本tensorflow">1.4 安装指定版本tensorflow</h5>
<ul>
<li>查看当前可使用的tensorflow版本<code>conda search  --full tensorflow</code></li>
</ul>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line">(py36) PS C:\Users\Trluper&gt; conda search  --full tensorflow</span><br><span class="line">&gt;&gt;</span><br><span class="line">Loading channels: done</span><br><span class="line"># Name                       Version           Build  Channel</span><br><span class="line">tensorflow                     <span class="number">1.7</span><span class="number">.0</span>               <span class="number">0</span>  pkgs/main</span><br><span class="line">tensorflow                     <span class="number">1.7</span><span class="number">.1</span>               <span class="number">0</span>  pkgs/main</span><br><span class="line">tensorflow                     <span class="number">1.8</span><span class="number">.0</span>               <span class="number">0</span>  pkgs/main</span><br><span class="line">tensorflow                     <span class="number">1.8</span><span class="number">.0</span>      haa95532_0  pkgs/main</span><br><span class="line">tensorflow                     <span class="number">1.9</span><span class="number">.0</span> eigen_py35hb0e21f4_1  pkgs/main</span><br><span class="line">tensorflow                     <span class="number">1.9</span><span class="number">.0</span> eigen_py36h0b764b7_1  pkgs/main</span><br><span class="line">tensorflow                     <span class="number">1.9</span><span class="number">.0</span> gpu_py35h0075c17_1  pkgs/main</span><br><span class="line">tensorflow                     <span class="number">1.9</span><span class="number">.0</span> gpu_py36hfdee9c2_1  pkgs/main</span><br><span class="line">tensorflow                    <span class="number">1.10</span><span class="number">.0</span> eigen_py35h38c8211_0  pkgs/main</span><br><span class="line">tensorflow                    <span class="number">1.10</span><span class="number">.0</span> eigen_py36h849fbd8_0  pkgs/main</span><br><span class="line">tensorflow                    <span class="number">1.10</span><span class="number">.0</span> gpu_py35ha5d5ef7_0  pkgs/main</span><br><span class="line">tensorflow                    <span class="number">1.10</span><span class="number">.0</span> gpu_py36h3514669_0  pkgs/main</span><br><span class="line">tensorflow                    <span class="number">1.10</span><span class="number">.0</span> mkl_py35h4a0f5c2_0  pkgs/main</span><br><span class="line">tensorflow                    <span class="number">1.10</span><span class="number">.0</span> mkl_py36hb361250_0  pkgs/main</span><br><span class="line">tensorflow                    <span class="number">1.11</span><span class="number">.0</span> eigen_py36h346fd36_0  pkgs/main</span><br><span class="line">tensorflow                    <span class="number">1.11</span><span class="number">.0</span> gpu_py36h5dc63e2_0  pkgs/main</span><br><span class="line">tensorflow                    <span class="number">1.11</span><span class="number">.0</span> mkl_py36h41bbc20_0  pkgs/main</span><br><span class="line">tensorflow                    <span class="number">1.12</span><span class="number">.0</span> eigen_py36h67ac661_0  pkgs/main</span><br><span class="line">tensorflow                    <span class="number">1.12</span><span class="number">.0</span> gpu_py36ha5f9131_0  pkgs/main</span><br><span class="line">tensorflow                    <span class="number">1.12</span><span class="number">.0</span> mkl_py36h4f00353_0  pkgs/main</span><br><span class="line">tensorflow                    <span class="number">1.13</span><span class="number">.1</span> eigen_py36hf0a88a9_0  pkgs/main</span><br><span class="line">tensorflow                    <span class="number">1.13</span><span class="number">.1</span> eigen_py37h2a8d240_0  pkgs/main</span><br><span class="line">tensorflow                    <span class="number">1.13</span><span class="number">.1</span> gpu_py36h1635174_0  pkgs/main</span><br><span class="line">tensorflow                    <span class="number">1.13</span><span class="number">.1</span> gpu_py36h9006a92_0  pkgs/main</span><br><span class="line">tensorflow                    <span class="number">1.13</span><span class="number">.1</span> gpu_py37h83e5d6a_0  pkgs/main</span><br><span class="line">tensorflow                    <span class="number">1.13</span><span class="number">.1</span> gpu_py37hbc1a9d5_0  pkgs/main</span><br><span class="line">tensorflow                    <span class="number">1.13</span><span class="number">.1</span> mkl_py36hd212fbe_0  pkgs/main</span><br><span class="line">tensorflow                    <span class="number">1.13</span><span class="number">.1</span> mkl_py37h9463c59_0  pkgs/main</span><br><span class="line">tensorflow                    <span class="number">1.14</span><span class="number">.0</span> eigen_py36hf4fd08c_0  pkgs/main</span><br><span class="line">tensorflow                    <span class="number">1.14</span><span class="number">.0</span> eigen_py37hcf3f253_0  pkgs/main</span><br><span class="line">tensorflow                    <span class="number">1.14</span><span class="number">.0</span> gpu_py36h305fd99_0  pkgs/main</span><br><span class="line">tensorflow                    <span class="number">1.14</span><span class="number">.0</span> gpu_py36heb2afb7_0  pkgs/main</span><br><span class="line">tensorflow                    <span class="number">1.14</span><span class="number">.0</span> gpu_py37h2fabf85_0  pkgs/main</span><br><span class="line">....</span><br></pre></td></tr></table></figure>
<ul>
<li>查看tensorflow包信息及依赖关系<code>conda  info  tensorflow</code></li>
</ul>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">(py36) PS C:\Users\Trluper&gt; conda  info  tensorflow</span><br><span class="line">tensorflow <span class="number">1.15</span><span class="number">.0</span> mkl_py36h997801b_0</span><br><span class="line">------------------------------------</span><br><span class="line">file name   : tensorflow<span class="number">-1.15</span><span class="number">.0</span>-mkl_py36h997801b_0.conda</span><br><span class="line">name        : tensorflow</span><br><span class="line">version     : <span class="number">1.15</span><span class="number">.0</span></span><br><span class="line">build string: mkl_py36h997801b_0</span><br><span class="line">build number: <span class="number">0</span></span><br><span class="line">channel     : https:<span class="comment">//repo.anaconda.com/pkgs/main/win-64</span></span><br><span class="line">size        : <span class="number">4</span> KB</span><br><span class="line">arch        : None</span><br><span class="line">constrains  : ()</span><br><span class="line">legacy_bz2_md5: ad6e18c8cdd17fe2bd396177ba904065</span><br><span class="line">legacy_bz2_size: <span class="number">3829</span></span><br><span class="line">license     : Apache <span class="number">2.0</span></span><br><span class="line">license_family: Apache</span><br><span class="line">md5         : aa94e7a35e3cf46219e5750ea5674fa0</span><br><span class="line">platform    : None</span><br><span class="line">sha256      : <span class="number">04b</span>f8a4c378f150163cbdfe1cf5d996c1251b9231acf2d723336668259a4e0e4</span><br><span class="line">subdir      : win<span class="number">-64</span></span><br><span class="line">timestamp   : <span class="number">1573235601611</span></span><br><span class="line">url         : https:<span class="comment">//repo.anaconda.com/pkgs/main/win-64/tensorflow-1.15.0-mkl_py36h997801b_0.conda</span></span><br><span class="line">dependencies:</span><br><span class="line">    _tflow_select <span class="number">2.3</span><span class="number">.0</span> mkl</span><br><span class="line">    python <span class="number">3.6</span>.*</span><br><span class="line">    tensorboard &gt;=<span class="number">1.15</span><span class="number">.0</span></span><br><span class="line">    tensorflow-base <span class="number">1.15</span><span class="number">.0</span> mkl_py36h190a33d_0</span><br><span class="line">    tensorflow-estimator &gt;=<span class="number">1.15</span><span class="number">.0</span></span><br></pre></td></tr></table></figure>
<p>主要看<code>dependencies</code>，可以看到tensorflow对应的python版本</p>
<ul>
<li>安装指定版本tensorflow：<code>conda  install tensorflow-cpu=1.13</code>，其中<code>-cpu</code>指cpu版本，还有<code>-gpu</code>版本</li>
</ul>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//指定清华源，但有一些会没有</span></span><br><span class="line">pip install -i  https:<span class="comment">//pypi.mirrors.ustc.edu.cn/simple  tensorflow-cpu==2.1.0</span></span><br><span class="line"><span class="comment">//国外源</span></span><br><span class="line">pip install tensorflow-cpu==<span class="number">2.1</span><span class="number">.0</span></span><br></pre></td></tr></table></figure>
<ul>
<li>激活tensorflow<code>conda activate tensorflow</code></li>
</ul>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">conda activate tensorflow</span><br></pre></td></tr></table></figure>
<ul>
<li>查看该环境下安装的tensorflow：<code>conda list tensorflow</code></li>
</ul>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">(py36) PS C:\Users\Trluper&gt; conda list tensorflow</span><br><span class="line"><span class="meta"># packages in environment at D:\pythonDeve\Anaconda\envs\py36:</span></span><br><span class="line">#</span><br><span class="line"># Name                    Version                   Build  Channel</span><br><span class="line">tensorflow-cpu            <span class="number">2.1</span><span class="number">.0</span>                    pypi_0    pypi</span><br><span class="line">tensorflow-estimator      <span class="number">2.1</span><span class="number">.0</span>                    pypi_0    pypi</span><br></pre></td></tr></table></figure>
<h5 id="安装指定版本keras">1.5 安装指定版本keras</h5>
<ul>
<li>查询所有版本的keras</li>
</ul>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">(py36) PS C:\Users\Trluper&gt; conda info keras</span><br><span class="line">keras <span class="number">2.6</span><span class="number">.0</span> pyhd3eb1b0_0</span><br><span class="line">------------------------</span><br><span class="line">file name   : keras<span class="number">-2.6</span><span class="number">.0</span>-pyhd3eb1b0_0.conda</span><br><span class="line">name        : keras</span><br><span class="line">version     : <span class="number">2.6</span><span class="number">.0</span></span><br><span class="line">build string: pyhd3eb1b0_0</span><br><span class="line">build number: <span class="number">0</span></span><br><span class="line">channel     : https:<span class="comment">//repo.anaconda.com/pkgs/main/noarch</span></span><br><span class="line">size        : <span class="number">721</span> KB</span><br><span class="line">arch        : None</span><br><span class="line">constrains  : (<span class="string">&#x27;tensorflow 2.6.0&#x27;</span>,)</span><br><span class="line">legacy_bz2_md5: <span class="number">9</span>c47afd01ba08eeb2130cf9811c59918</span><br><span class="line">legacy_bz2_size: <span class="number">848011</span></span><br><span class="line">license     : MIT</span><br><span class="line">license_family: MIT</span><br><span class="line">md5         : <span class="number">4</span>aeef3bf046f7e1ed44efe59fb7d3e23</span><br><span class="line">noarch      : python</span><br><span class="line">package_type: noarch_python</span><br><span class="line">platform    : None</span><br><span class="line">sha256      : c0156c162ce479d5eafd468acb956b977b7b9ae9236494b698d5b4dab02cdd7d</span><br><span class="line">subdir      : noarch</span><br><span class="line">timestamp   : <span class="number">1643826831715</span></span><br><span class="line">url         : https:<span class="comment">//repo.anaconda.com/pkgs/main/noarch/keras-2.6.0-pyhd3eb1b0_0.conda</span></span><br><span class="line">dependencies:</span><br><span class="line">    python &gt;=<span class="number">3.6</span></span><br><span class="line">....</span><br></pre></td></tr></table></figure>
<ul>
<li>安装指定版本keras</li>
</ul>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">(py36) PS C:\Users\Trluper&gt; pip install keras==<span class="number">2.6</span><span class="number">.0</span></span><br></pre></td></tr></table></figure>
<h5 id="annconda中使用jupyter">1.6 Annconda中使用Jupyter</h5>
<p>在Annconda prompt中切换自己想要的python环境，然后直接输入<code>jupyter notebook</code>即可 <figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">(base) PS C:\Users\Trluper&gt; <span class="function">conda activate <span class="title">py36</span></span></span><br><span class="line"><span class="function"><span class="params">(py36)</span> PS C:\Users\Trluper&gt; jupyter notebook</span></span><br></pre></td></tr></table></figure></p>
<h5 id="jupyter导入文件夹">1.7 jupyter导入文件夹</h5>
<p>导入压缩包后，创建一个python文件输入下述代码并运行，可能会出现乱码 <figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> zipfile</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line">files = zipfile.<span class="built_in">ZipFile</span>(<span class="string">&#x27;文件夹名称.zip&#x27;</span>,<span class="string">&#x27;r&#x27;</span>)</span><br><span class="line">files.<span class="built_in">extractall</span>(os.<span class="built_in">getcwd</span>())</span><br></pre></td></tr></table></figure> 当然你要可以在本地jupyter的工作路径上直接解压该文件更快，也更推荐这种方法： <img src="/2022/09/17/python%E7%8E%AF%E5%A2%83%E9%85%8D%E7%BD%AE/jupyter.png" width="500"></p>
<h5 id="更改jupyter的工作路径">1.8 更改jupyter的工作路径</h5>
<p>默认情况下jupytr的工作路径wei<code>C:\Users\用户名</code>，我们不希望放在C盘，可以通过以下方法更改。</p>
<ul>
<li>首先找到对于的python环境的Annconda prompt进入,输入命令<code>jupyter notebook --generate-config</code>得到路径 <img src="/2022/09/17/python%E7%8E%AF%E5%A2%83%E9%85%8D%E7%BD%AE/Annconda.png" width="500"></li>
<li>根据上面运行处的路径打开<code>C:\Users\Trluper\.jupyter\jupyter_notebook_config.py</code>文件</li>
<li>在文件中找到<code>c.NotebookApp.notebook_dir = ''</code>，删除注释符<code>#</code>，并在单引号内写上自己的工作路径 <img src="/2022/09/17/python%E7%8E%AF%E5%A2%83%E9%85%8D%E7%BD%AE/config.png" width="500"></li>
<li>之后找到<code>jupyter Notebook</code>,点击属性，删除后面的<code>&quot;%USERPROFILE%/”</code> <img src="/2022/09/17/python%E7%8E%AF%E5%A2%83%E9%85%8D%E7%BD%AE/delete.png" width="500"></li>
</ul>
<h5 id="安装pytorch">1.9 安装pytorch</h5>
<p>pytorch的安装可以直接在<a target="_blank" rel="noopener" href="https://pytorch.org/get-started/previous-versions/">官网</a>复制相应版本的pytorch进行安装，注意区分GPU版本和CPU版本。复制后在Annconda prompt下进行执行，在执行前自己先切换到要安装pytorch的python虚拟环境，下面是pytorch 1.8.1版本的安装语句，上面的cuda是其对应cuda版本。 <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># CUDA 11.1</span></span><br><span class="line">pip install torch==<span class="number">1.8</span><span class="number">.1</span>+cu111 torchvision==<span class="number">0.9</span><span class="number">.1</span>+cu111 torchaudio==<span class="number">0.8</span><span class="number">.1</span> -f https://download.pytorch.org/whl/torch_stable.html</span><br><span class="line"></span><br><span class="line"><span class="comment"># CUDA 10.2</span></span><br><span class="line">pip install torch==<span class="number">1.8</span><span class="number">.1</span>+cu102 torchvision==<span class="number">0.9</span><span class="number">.1</span>+cu102 torchaudio==<span class="number">0.8</span><span class="number">.1</span> -f https://download.pytorch.org/whl/torch_stable.html</span><br><span class="line"></span><br><span class="line"><span class="comment"># CUDA 10.1</span></span><br><span class="line">pip install torch==<span class="number">1.8</span><span class="number">.1</span>+cu101 torchvision==<span class="number">0.9</span><span class="number">.1</span>+cu101 torchaudio==<span class="number">0.8</span><span class="number">.1</span> -f https://download.pytorch.org/whl/torch_stable.html</span><br><span class="line"></span><br><span class="line"><span class="comment"># CPU only</span></span><br><span class="line">pip install torch==<span class="number">1.8</span><span class="number">.1</span>+cpu torchvision==<span class="number">0.9</span><span class="number">.1</span>+cpu torchaudio==<span class="number">0.8</span><span class="number">.1</span> -f https://download.pytorch.org/whl/torch_stable.html</span><br></pre></td></tr></table></figure> <strong><em>注：一般来说cuda高版本可兼容低版本pytorch，但高pytorch不能兼容低版本的cuda</em></strong></p>
<p><img src="/2022/09/17/python%E7%8E%AF%E5%A2%83%E9%85%8D%E7%BD%AE/cuda.png" width="500"> python和pytorch的对应版本一般只要求python在3.6和3.9之间</p>
<h5 id="安装mmlab的mmcv">1.10 安装mmlab的mmcv</h5>
<p><strong>MMCV 有两个版本：</strong></p>
<ul>
<li>mmcv-full: 完整版，包含所有的特性以及丰富的开箱即用的 CUDA 算子。注意完整版本可能需要更长时间来编译。</li>
<li>mmcv: 精简版，不包含 CUDA 算子但包含其余所有特性和功能，类似 MMCV 1.0 之前的版本。如果你不需要使用 CUDA 算子的话，精简版可以作为一个考虑选项。</li>
</ul>
<p><strong>安装CUDA版mmcv</strong>:</p>
<ul>
<li>从网址中选择自己对应的cuda版本、pytorch版本，如下<code>https://download.openmmlab.com/mmcv/dist/cu101/torch1.8.0/index.html</code>，其中cu后面的三个数字对应你的cuda版本号，torch后面的数字对应逆的pytorch版本号。 <img src="/2022/09/17/python%E7%8E%AF%E5%A2%83%E9%85%8D%E7%BD%AE/mmcv.png" width="500"></li>
<li>将选择的<code>whl</code>包下载后，到指定的python虚拟环境进行安装，这里我将包下载到了<code>D:\Applycation_SapceOfSourse</code>,进入该路径后，执行<code>pip install  mmcv_full-1.5.3-cp37-cp37m-win_amd64.whl</code> <img src="/2022/09/17/python%E7%8E%AF%E5%A2%83%E9%85%8D%E7%BD%AE/install.png" width="500"></li>
<li>安装完成后运行<code>check_installation.py</code>脚本看是否安装成功</li>
</ul>
<p><strong>精简版</strong>： <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install mmcv</span><br></pre></td></tr></table></figure></p>
<p>详情看<a target="_blank" rel="noopener" href="https://github.com/open-mmlab/mmcv/blob/master/README_zh-CN.md">mmlab的官方github</a></p>
<h5 id="安装mmlab的mmdetection3d">1.11 安装mmlab的mmdetection3D</h5>
<p>待更新</p>
<p>详情看<a target="_blank" rel="noopener" href="https://github.com/open-mmlab/mmdetection3d/blob/master/docs/zh_cn/getting_started.md">mmlab的mmdetection3D</a></p>
<h4 id="错误总览">2. 错误总览</h4>
<h5 id="cannot-import-name-get_config">2.1 cannot import name 'get_config'</h5>
<ul>
<li><p>问题：导入语句<code>from keras.datasets import mnist</code>出现<code>ImportError: cannot import name ‘get_config’</code></p></li>
<li><p><code>解决方法</code>： <code>ImportError: cannot import name ‘get_config’</code> 这个类型报错的问题我遇到的都可以通过导入包时使用tensorflow.keras解决,如下：成功解决 <figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">from tensorflow.keras.datasets <span class="keyword">import</span> mnist</span><br></pre></td></tr></table></figure></p></li>
</ul>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/2022/09/12/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="trluper">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Trluper">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/09/12/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" class="post-title-link" itemprop="url">机器学习</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2022-09-12 20:47:43" itemprop="dateCreated datePublished" datetime="2022-09-12T20:47:43+08:00">2022-09-12</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2022-10-05 09:27:16" itemprop="dateModified" datetime="2022-10-05T09:27:16+08:00">2022-10-05</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95/" itemprop="url" rel="index"><span itemprop="name">深度学习算法</span></a>
                </span>
            </span>

          <br>
            <span class="post-meta-item" title="本文字数">
              <span class="post-meta-item-icon">
                <i class="far fa-file-word"></i>
              </span>
                <span class="post-meta-item-text">本文字数：</span>
              <span>13k</span>
            </span>
            <span class="post-meta-item" title="阅读时长">
              <span class="post-meta-item-icon">
                <i class="far fa-clock"></i>
              </span>
                <span class="post-meta-item-text">阅读时长 &asymp;</span>
              <span>12 分钟</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h4 id="我的思考">0. 我的思考</h4>
<p>两个可供突破的大方向：</p>
<ul>
<li>是否能推动深度学习的训练数据规模减小？</li>
<li>怎么使得训练模型的结果更加精确。</li>
</ul>
<p>资源:</p>
<ul>
<li>kaggle机器学习竞赛</li>
</ul>
<h4 id="机器学习和深度学习关系概述">1. 机器学习和深度学习关系概述</h4>
<p>人工智能、机器学习和深度学习三者的关系如下所示： <img src="/2022/09/12/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/relationship.png" width="400"> <img src="/2022/09/12/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/knowladge.png" width="400"></p>
<ul>
<li><p><strong>机器学习</strong>：机器学习系统是训练出来的，而不是明确用代码编写出来的。人们通过输入数据和从这些数据中预期得到的答案，系统输出相应的规则。</p></li>
<li><p><strong>深度学习</strong>：<strong>深度学习里面的&quot;深度&quot;不是指通过学习获得更深层次的理解，而是指通过一系列连续的表示层来获得更有效果的数据特征。一个深度学习模型有多少层就被称为模型的深度是多少</strong>。一般来说，现代深度学习通常包含数十个甚至上百个连续的表示层。<strong>这些层也被称为神经网络</strong> <img src="/2022/09/12/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/CNN.png" width="500"></p></li>
</ul>
<h5 id="深度学习的指标">1.2 深度学习的指标</h5>
<p>在上面我们已经知道，深度学习是<strong>通过各层对输入数据不断提取特征</strong>，提取方式的具体操作就是<strong>每层都是对输入数据进行权重计算</strong>，其本质就是一串数字，</p>
<p>如下图所示，<span class="math inline">\(X_i\)</span>是从图片拉伸出来的像素，它是不会变的，无法对其进行操作。而<span class="math inline">\(W\)</span>矩阵就是参数矩阵，<span class="math inline">\(b\)</span>是神经元偏置。<strong>因此对于深度学习，对于参数矩阵的选择是一项艰巨的任务，参数的好坏决定了输出结果的优劣</strong>。但是我们不能糊里糊涂的对参数进行更改，这样可能适得其反，<strong>必须借助某一个评估指标来更改，这就是接下来要介绍的损失函数</strong> <img src="/2022/09/12/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/pixels.png" width="500"></p>
<h5 id="损失函数">1.2.1 损失函数</h5>
<p><strong>损失函数</strong>：损失函数的任务，就是通过一定数学式子衡量出该深度学习的实际输出与预期标准值之间的距离。</p>
<p><strong>通过损失函数计算的损失值作为反馈信号来对参数矩阵中的权值进行微调，来降低对应的损失值</strong>。这种调节右优化器来完成，它实现了所谓的<strong>反向传播</strong>算法。 <img src="/2022/09/12/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/optimizer.png" width="300"></p>
<blockquote>
<p>一般来说，一开始神经网络的权值是随机赋值的，那么第一次的输出也就通常与预期值相差甚远，但随着权重值的不断微调，损失在也在降低，这就是为什么说学习模型是训练出来的。</p>
</blockquote>
<h6 id="softmax分类器">1.2.2 softmax分类器</h6>
<p>上面我们经过各层神经网络最终得到一个输入的得分值，但我们想一想如果结果给与我们一个概率不是更好，那么把得分值转化为概率就是<code>softmax</code>的作用。</p>
<p><code>softmax</code>分类器将各个得分值<strong>进行放大然后归一化后再计算概率</strong></p>
<h5 id="卷积神经网络">1.3 卷积神经网络</h5>
<p>神经网络本质上就是将我们的输入信息转换为特征矩阵。<strong>卷积神经网络就是对同一个区域可以提取多个特征值，传统的神经网络一般只提取单个特征值，这样卷积更加全面稳实。</strong></p>
<p>卷积神经网络右输入层、隐含层、输出层组成：</p>
<ul>
<li><p>输入层：卷积神经网络的输入层可以处理多维数据，常见地，一维卷积神经网络的输入层接收一维或二维数组，其中一维数组通常为时间或频谱采样；二维数组可能包含多个通道；二维卷积神经网络的输入层接收二维或三维数组；三维卷积神经网络的输入层接收四维数组。由于使用<strong>梯度下降算法</strong>进行学习，卷积神经网络的输入特征需要进行标准化处理。具体地，在将学习数据输入卷积神经网络前，<strong>需在通道或时间/频率维对输入数据进行归一化</strong>，若输入数据为像素，也可将分布于的原始像素值归一化至<span class="math inline">\([0,1]\)</span>区间</p></li>
<li><p><strong>隐含层</strong>：隐含层包含卷积层、池化层和全连接层3类常见构筑。卷积层中的卷积核包含权重系数，而池化层不包含权重系数</p>
<ul>
<li><p><strong>卷积核</strong>：卷积层的功能是对输入数据进行特征提取，其内部包含多个<strong>卷积核</strong>，组成卷积核的每个元素都对应一个权重系数和一个偏差量（bias vector）。卷积核就是图像处理时，给定输入图像，输入图像中一个小区域中像素加权平均后成为输出图像中的每个对应像素，其中权值由一个函数定义，这个函数称为卷积核。</p></li>
<li><p><strong>激励函数</strong>：卷积层中包含激励函数以协助表达复杂特征，卷积神经网络通常使用线性整流函数<code>Rectified Linear Unit, ReLU</code>,它的作用就是对于上一层经过卷积后的数据经过<code>relu</code>的<code>max(0,x)</code>筛选</p></li>
<li><strong>池化层</strong>：在卷积层进行特征提取后，为避免特征量太多计算缓慢，对其进行特征值压缩，输出的特征图会被传递至池化层进行特征选择和信息过滤。池化层包含预设定的池化函数，其功能是将特征图中单个点的结果替换为其相邻区域的特征图统计量。</li>
<li><p><strong>全连接层</strong>：卷积神经网络中的卷积层和池化层能够对输入数据进行特征提取，<strong>全连接层的作用则是对提取的特征进行非线性组合以得到输出，即全连接层本身不被期望具有特征提取能力</strong>，而是试图利用现有的高阶特征完成学习目标 <img src="/2022/09/12/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/car.png" width="500"></p></li>
</ul></li>
</ul>
<h4 id="机器学习">2. 机器学习</h4>
<p>当我们用机器学习来解决实际任务时，会面对多种多样的数据形式，比如声音、图像、文本等．不同数据的特征构造方式差异很大．对于图像这类数据，我们可以很自然地将其表示为一个连续的向量． 将图像数据表示为向量的方法有很多种，比如直接将一幅图像的所有像素值（灰度值或RGB 值）组成一个连续向量．而对于文本数据，因为其一般由离散符号组成，并且每个符号在计算机内部都表示为无意义的编码，所以通常很难找到合适的表示方式．因此，在实际任务中使用机器学习模型一般会包含以下几个步骤</p>
<ul>
<li><strong>（1）数据预处理</strong>：对数据的原始形式进行初步的数据清理（比如去掉一些有缺失特征的样本，或去掉一些冗余的数据特征等）和加工（对数值特征进行缩放和归一化等），并构建成可用于训练机器学习模型的数据集．</li>
<li><strong>（2）特征提取</strong>：从数据的原始特征中提取一些对特定机器学习任务有用的高质量特征．比如在图像分类中提取边缘、尺度不变特征变换（Scale InvariantFeature Transform，SIFT）特征，在文本分类中去除停用词等．</li>
<li><strong>（3）特征转换</strong>：对特征进行进一步的加工，比如降维和升维． 很多特征转换方法也都是机器学习方法．降维包括特征抽取（Feature Extraction）和特征选择（Feature Selection）两种途径．常用的特征转换方法有主成分分析（Principal Components Analysis，PCA）、 线性判别分析（Linear Discriminant Analysis，LDA）等．</li>
<li><strong>（4）预测</strong>：机器学习的核心部分，学习一个函数并进行预测 <img src="/2022/09/12/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/MLprocedrue.png" width="500"></li>
</ul>
<p>上述流程中，每步特征处理以及预测一般都是分开进行的．<strong>传统的机器学习模型主要关注最后一步，即构建预测函数．但是实际操作过程中，不同预测模型的性能相差不多</strong>，<strong><em>而前三步中的特征处理对最终系统的准确性有着十分关键的作用．特征处理一般都需要人工干预完成，利用人类的经验来选取好的特征，并最终提高机器学习系统的性能．因此，很多的机器学习问题变成了特征工程（Feature Engineering）问题．开发一个机器学习系统的主要工作量都消耗在了预处理、特征提取以及特征转换上</em></strong>． <img src="/2022/09/12/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/pro.png" width="500"></p>
<h5 id="机器学习类型">2.1 机器学习类型</h5>
<p>常见的机器学习有监督学习、无监督学习和强化学习。</p>
<ul>
<li>监督学习：训练集有标签，学习准则为期望风险最小化，进行最大似然估计</li>
<li>无监督学习：训练集没有标签，学习准则为最大似然估计和最小重构错误</li>
<li>强化学习：智能体和环境交互的规矩t和累积奖励G，学习准则中会使用策略评估和改进。</li>
</ul>
<p>总的来说学习的分类有，有监督主要是回归、分类问题；无监督学习问题主要为聚类、降维和密度估计</p>
<h5 id="机器学习要素">2.2 机器学习要素</h5>
<p>机器学习的主要四要素是数据、模型、学习准则和优化算法。</p>
<h6 id="模型">2.2.1 模型</h6>
<ul>
<li>模型：以回归问题为例，输入空间<span class="math inline">\(X\)</span>和输出空间<span class="math inline">\(Y\)</span>构成了一个样本空间．对于样本空间中的样本<span class="math inline">\((x, y) ∈ X × Y\)</span>，假定x和y 之间的关系可以通过一个未知的真实映射函数<span class="math inline">\(y =g(x)\)</span>或真实条件概率分布<span class="math inline">\(P_r(y|x)\)</span>来描述．机器学习的目标是找到一个模型来近 似真实映射函数<span class="math inline">\(g(x)\)</span>或真实条件概率分布<span class="math inline">\(p_r(y|x)\)</span>．</li>
</ul>
<h6 id="学习准则">2.2.2 学习准则</h6>
<ul>
<li>学习准则：这里使用了损失函数，损失函数是一个非负实数函数，用来量化模型预测和真实标签之间的差异．下面介绍几种常用的损失函数：
<ul>
<li><strong>平方损失函数</strong>：平方损失函数（Quadratic Loss Function）经常用在预测标签<span class="math inline">\(y\)</span>为实数值的任务中，定义为 <span class="math display">\[L(y,f(x;θ))={1 \over 2}(y-f(x;θ))^2\]</span></li>
<li><strong>交叉熵损失函数</strong>：交叉熵损失函数（Cross-Entropy Loss Function）一般用于分类问题．假设样本的标签 <span class="math inline">\(y ∈ {1, ⋯ , C}\)</span> 为离散的类别，模型<span class="math inline">\(f(x; θ) ∈ [0, 1]^C\)</span>的输出为类别标签的条件概率分布。如下：比如对于三分类问题，一个样本的标签向量为 <span class="math inline">\(y = [0, 0, 1]^T\)</span>，模型预测的标签分布为 <span class="math inline">\(f(x; θ) = [0.3, 0.3, 0.4]^T\)</span>，则它们的交叉熵为 <span class="math display">\[ L(y,f(x;θ))=−(0 × log(0.3) + 0 ×log(0.3) + 1 × log(0.4)) = − log(0.4)\]</span></li>
<li><strong>Hinge损失函数</strong>：对于二分类问题，假设<span class="math inline">\(y∈{−1, +1}，f(x; θ)∈ℝ\)</span>，Hinge损失函数（Hinge Loss Function）为： <span class="math display">\[L(y,f(x;θ))=max(0,1-yf(x;θ))\]</span></li>
</ul></li>
</ul>
<h6 id="风险最小化准则">2.2.3 风险最小化准则</h6>
<p>一个好的模型 <span class="math inline">\(f(x;θ)\)</span>应当有一个比较小的期望错误，但由于不知道真实的数据分布和映射函数，实际上无法计算其期望风险 ℛ(θ)．给定一个训练集 <span class="math inline">\(D ={(x^{(n)}, y^{(n)})}_{n=1}^N\)</span>，我们可以计算的是<strong>经验风险（Empirical Risk）</strong>，即在训练集上的<strong>平均损失</strong>: <span class="math display">\[R_D^{emp}(θ)={1\over N} \sum_{n=1}^NL(y^{(n)},f(x^{(n)};θ))\]</span></p>
<p>因此，一个切实可行的学习准则是找到一组参数<span class="math inline">\(θ^*\)</span> 使得经验风险最小，即: <span class="math display">\[θ^*=argminR_D^{emp}(θ)\]</span> 这就是经验风险最小化（Empirical Risk Minimization，ERM）准则</p>
<p><strong>过拟合</strong>：</p>
<p>根据大数定理可知，当训练集大小 |𝒟| 趋向于无穷大时，经验风险就趋于期望风险。然而通常情况下，我们无法获取无限的训练样本，并且训练样本往往是真实数据的一个很小的子集或者包含一定的噪声数据，不能很好地反映全部数据的真实分布．<strong>经验风险最小化原则很容易导致模型在训练集上错误率很低，但是在未知数据上错误率很高．这就是所谓的过拟合（Overfitting）</strong></p>
<p><strong>过拟合问题往往是由于训练数据少和噪声以及模型能力强等原因造成的</strong>．为了解决过拟合问题，一般在经验风险最小化的基础上再引入<strong>参数的正则化（Regularization）来限制模型能力</strong>，使其不要过度地最小化经验风险。这种准则就是<strong>结构风险最小化（Structure Risk Minimization，SRM）准则</strong>： <span class="math display">\[θ^*=argminR_D^{struct}(θ)=argminR_D^{emp}(θ)+{1\over 2}λ||θ||^2\]</span> 其中<span class="math inline">\(‖θ‖\)</span>是<span class="math inline">\(ℓ2\)</span> 范数的正则化项，用来减少参数空间，避免过拟合；<span class="math inline">\(λ\)</span>用来控制正则化的强度．正则化项也可使用其他函数，比如$ ℓ1<span class="math inline">\(范数．\)</span>ℓ1 $范数的引入通常会使得参数有一定稀疏性，因此在很多算法中也经常使用． 从贝叶斯学习的角度来讲，正则化是引入了参数的先验分布，使其不完全依赖训练数据 <img src="/2022/09/12/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/overAndlack.png" width="400"></p>
<p><strong><em>注</em></strong>：所有损害优化的方法都可称为正则化</p>
<h6 id="最优化">2.2.4 最优化</h6>
<p>在确定了训练集<span class="math inline">\(X\)</span>、假设空间<span class="math inline">\(ℱ\)</span>以及学习准则后，如何找到最优的模型<span class="math inline">\(f(x, θ^∗)\)</span> 就成了一个最优化（Optimization）问题．机器学习的训练过程其实就是最优化问题的求解过程。</p>
<p>参数与超参数 在机器学习中，优化又可以分为<strong>参数优化和超参数优化</strong>．模型<span class="math inline">\(f(x; θ)\)</span>中的<span class="math inline">\(θ\)</span>称为模型的参数，可以通过优化算法进行学习．除了可学习的参数<span class="math inline">\(θ\)</span>之外，<strong>还有一类参数是用来定义模型结构或优化策略的，这类参数叫作超参数(Hyper-Parameter）</strong></p>
<p>常见的超参数包括：<strong>聚类算法中的类别个数、梯度下降法中的步长、正则化项的系数、神经网络的层数、支持向量机中的核函数等</strong>．<em><u>超参数的选取一般都是组合优化问题，很难通过优化算法来自动学习．因此，超参数优化是机器学习的一个经验性很强的技术，通常是按照人的经验设定，或者通过搜索的方法对一组超参数组合进行不断试错调整</u>．</em></p>
<ul>
<li><strong>优化算法：</strong>
<ul>
<li><p><strong>梯度下降法</strong>：为了充分利用凸优化中一些高效、成熟的优化方法，比如共轭梯度、拟牛顿法等，很多机器学习方法都倾向于选择合适的模型和损失函数，以构造一个凸函数作为优化目标．但也有很多模型（比如神经网络）的优化目标是非凸的，只能退而求其次找到局部最优解。在机器学习中，<strong>最简单、常用的优化算法就是梯度下降法</strong>，即首先初始化参数<span class="math inline">\(θ_0\)</span>，然后按下面的迭代公式来计算训练集<span class="math inline">\(D\)</span>上风险函数的最小值: <span class="math display">\[θ_{t+1}=θ_t-α{δR_D(θ)\over δθ}=θ_t-α{1\over N}\sum{_{n=1}^N}{δL(y^{(n)},f(x^{(n)};θ)\over δθ}\]</span> 其中<span class="math inline">\(θ_t\)</span>为第<span class="math inline">\(t\)</span>次迭代时的参数值，<span class="math inline">\(α\)</span>为搜索步长．在机器学习中，<span class="math inline">\(α\)</span>一般称为学习率（Learning Rate）</p></li>
<li><p><strong>提前停止</strong>：针对梯度下降的优化算法，<strong>除了加正则化项之外，还可以通过提前停止来防止过拟合</strong>．在梯度下降训练的过程中，由于过拟合的原因，在训练样本上收敛的参数，并不一定在测试集上最优．因此，除了训练集和测试集之外，有时也会使用一个验证集（Validation Set）来进行模型选择，测试模型在验证集上是否最优. 在每次迭代时，把新得到的模型 <span class="math inline">\(f(x; θ)\)</span> 在验证集上进行测试，并计算错误率．如果在验证集上的错误率不再下降，就停止迭代．这种策略叫提前停止（EarlyStop）．如果没有验证集，可以在训练集上划分出一个小比例的子集作为验证集 <img src="/2022/09/12/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/EarlyStop.png" width="400"></p></li>
<li><p><strong>随机梯度下降法</strong>：上面的<strong>梯度下降法</strong>的目标函数是整个训练集上的风险函数，批量梯度下降法在每次迭代时需要计算每个样本上损失函数的梯度并求和．当训练集中的样本数量<span class="math inline">\(N\)</span>很大时，空间复杂度比较高，每次迭代的计算开销也很大。<strong>批量梯度下降法相当于是从真实数据分布中采集<span class="math inline">\(N\)</span>个样本</strong>，并由它们计算出来的经验风险的梯度来近似期望风险的梯度．为了减少每次迭代的计算复杂度，我们也可以在每次迭代时只采集一个样本，计算这个样本损失函数的梯度并更新参数，即<strong>随机梯度下降法（Stochastic Gradient Descent，SGD</strong>）．当经过足够次数的迭代时，随机梯度下降 作增量梯度下降法．也可以收敛到局部最优解[Nemirovski et al., 2009] <img src="/2022/09/12/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/random.png" width="400"></p></li>
</ul>
<p>批量梯度下降和随机梯度下降之间的区别在于，每次迭代的优化目标是对所有样本的平均损失函数还是对单个样 本的损失函数．由于随机梯度下降实现简单，收敛速度也非常快，因此使用非常广泛．随机梯度下降相当于在批 量梯度下降的梯度上引入了随机噪声．在非凸优化问题中，随机梯度下降更容易逃离局部最优点．</p>
<ul>
<li><strong>小批量梯度下降法</strong>：随机梯度下降法的一个缺点是无法充分利用计算机的并行计算能力．小批量梯度下降法（Mini-Batch Gradient Descent）是批量梯度下降和随机梯度下降的折中．每次迭代时，我们随机选取一小部分训练样本来计算梯度并更新参数，这样既可以兼顾随机梯度下降法的优点，也可以提高训练效率： <span class="math display">\[θ_{t+1}←θ_t-α{1\over k}\sum_{(x,y)∈δ_t}{δL(y,f(x;θ))\over δθ}\]</span></li>
</ul></li>
</ul>
<h5 id="线性回归">2.3 线性回归</h5>
<p>本节通过一个简单的模型（线性回归）来具体了解机器学习的一般过程，以及不同学习准则（经验风险最小化、结构风险最小化、最大似然估计、最大后验估计）之间的关系．线性回归（Linear Regression）是机器学习和统计学中最基础和最广泛应用的模型，是一种对自变量和因变量之间关系进行建模的回归分析．自变量数量为1时称为简单回归，自变量数量大于1时称为多元回归。如下： <span class="math display">\[f(x;w,b)=w^Tx+b\]</span> 其中权重向量<span class="math inline">\(w∈ ℝ^D\)</span> 和偏置<span class="math inline">\(b∈ ℝ\)</span>都是可学习的参数，<span class="math inline">\(f(x;w,b)=w^Tx+b\)</span>即为线性模型</p>
<h6 id="参数学习">2.3.1 参数学习</h6>
<p>给定一组包含 𝑁 个训练样本的训练集 <span class="math inline">\(D = {(x^{(n)}, y{(n)})}_{n=1}^N=1\)</span>，我们希望能够学习一个最优的线性回归的模型参数<span class="math inline">\(w\)</span>．我们介绍四种不同的参数估计方法：经验风险最小化、结构风险最小化、最大似然估计、最大后验估计．</p>
<h5 id="经验风险最小化">2.4 经验风险最小化</h5>
<p>由于线性回归的标签𝑦和模型输出都为连续的实数值，因此平方损失函数非常合适衡量真实标签和预测标签之间的差异．根据经验风险最小化准则，训练集<span class="math inline">\(w\)</span>上的经验风险定义 <span class="math display">\[R(w)=\sum_{n=1}^NL(y^{(n)},f(x^{(n)};w)\\
={1\over 2}\sum_{n=1}^N(y^{(n)}-w^Tx^{(n)})^2 \\
={1\over 2}||y-X^Tw||^2
\]</span></p>
<p>风险函数<span class="math inline">\(ℛ(w)\)</span>是关于𝒘的凸函数，其对<span class="math inline">\(w\)</span>的偏导数为: <span class="math display">\[
{δR(w)\over δw}={1\over 2}{δ||y-X^Tw||^2\over δw}\\
=-X(y-X^Tw)
\]</span> 令其等于0，则可得<span class="math inline">\(w^*=(XX^T)^{(-1)}Xy\)</span> 上述求解线性回归参数的方法为<strong>最小二乘法</strong>。在最小二乘法，<span class="math inline">\(XX^T∈R^{(D+1)×(D+1)}\)</span>必须存在逆矩阵。</p>
<h5 id="结构风险最小化">2.5 结构风险最小化</h5>
<p>最小二乘法的基本要求是各个特征之间相互独立，保证<span class="math inline">\(XX^T\)</span>可逆，且即使可逆，如果特征值之间有较大的多重共线性，也会使得<span class="math inline">\(XX^T\)</span>在的逆在数值上无法准确计算。<strong>因此此时给<span class="math inline">\(XX^T\)</span>的对角线加上一个常数<span class="math inline">\(λ\)</span>，使其满秩</strong> <span class="math display">\[R(w)={1\over 2}||y-X^Tw||^2+{1\over 2}λ||w||\]</span></p>
<h5 id="最大似然估计">2.6 最大似然估计</h5>
<p>一类是样本的特征向量<span class="math inline">\(x\)</span>和标签<span class="math inline">\(y\)</span> 之间存在未知的函数关系 <span class="math inline">\(y = ℎ(x)\)</span>，另一类是条件概率$ p(y|x)$ 服从某个未知分布．最小二乘法是属于第一类，直接建模 <span class="math inline">\(x\)</span> 和标签 <span class="math inline">\(y\)</span> 之间的函数关系．此外，线性回归还可以从建模条件概率<span class="math inline">\(p(y|x)\)</span>的角度来进行参数估计。</p>
<p>假设标签<span class="math inline">\(y\)</span>为一个随机变量，并由函数<span class="math inline">\(f(x; w) = w^Tx\)</span> 加上一个随机噪声<span class="math inline">\(ε\)</span>决定，即<span class="math inline">\(f(x; w) = w^Tx+ε\)</span> 其中<span class="math inline">\(ε\)</span>服从均值为0，方差为<span class="math inline">\(δ^2\)</span>的高斯分布，则<span class="math inline">\(y\)</span>服从均值为<span class="math inline">\(w^Tx\)</span>,方差为<span class="math inline">\(δ^2\)</span>的高斯分布： <span class="math display">\[
p(y|x;w,δ)={1\over \sqrt{2\piδ}}exp(-{(y-w^Tx)^2\over2δ^2})
\]</span></p>
<p>接下来计算<span class="math inline">\(w\)</span>在训练集上的似然函数，后对似然函数取对数方便计算，令其导等于0，求出解得: <span class="math display">\[w^{ML}=(XX^T)^{-1}Xy\]</span></p>
<h5 id="最大后验估计">2.7 最大后验估计</h5>
<p><strong>最大似然估计的一个缺点是当训练数据比较少时会发生过拟合，估计的参数可能不准确．为了避免过拟合，我们可以给参数加上一些先验知识</strong>．略，详看书</p>
<h5 id="偏差-方差分解">2.8 偏差-方差分解</h5>
<p>为了避免过拟合，我们经常会在模型的拟合能力和复杂度之间进行权衡．拟合能力强的模型一般复杂度会比较高，容易导致过拟合．相反，如果限制模型的复杂度，降低其拟合能力，又可能会导致欠拟合．因此，如何在模型的拟合能力和复杂度之间取得一个较好的平衡，对一个机器学习算法来讲十分重要．<strong>偏差-方差分解（Bias-Variance Decomposition）为我们提供了一个很好的分析和指导工具．</strong></p>
<h5 id="机器学习算法类型">2.9 机器学习算法类型</h5>
<p>机器学习算法可以按照不同的标准来进行分类．比如按函数<span class="math inline">\(f(y; θ)\)</span>的不同，机器学习算法可以分为<strong>线性模型和非线性模型</strong>；按照学习准则的不同，机器学习算法也可以分为<strong>统计方法和非统计方法</strong>．</p>
<p>但一般来说，我们会<strong>按照训练样本提供的信息以及反馈方式的不同</strong>，将机器学习算法分为以下几类：</p>
<ul>
<li><strong>监督学习</strong>：如果机器学习的目标是建模样本的特征<span class="math inline">\(x\)</span>和标签<span class="math inline">\(y\)</span>之间的关系：<span class="math inline">\(y =f(x; θ)\)</span>或<span class="math inline">\(p(y|x; θ)\)</span>，并且训练集中每个样本都有标签，那么这类机器学习称为监督学习（Supervised Learning）.根据标签类型的不同，监督学习又可以分为<strong>回归问题、分类问题和结构化学习问题</strong>．
<ul>
<li>回归（Regression）问题中的标签<span class="math inline">\(y\)</span>是连续值（实数或连续整数），f(x; θ)$的输出也是连续值．</li>
<li><strong>分类（Classification）问题</strong>中的标签<span class="math inline">\(y\)</span>是离散的类别（符号）．在分类问题中，学习到的模型也称为分类器（Classifier）。分类问题根据其类别数量又可分为二分类（Binary Classification）和多分类（Multi-class Classification）问题．</li>
<li>结构化学习（Structured Learning）问题是一种特殊的分类问题．在结构化学习中，标签𝒚通常是结构化的对象，比如序列、树或图等．由于结构化学习的输出空间比较大，因此我们一般定义一个联合特征空间，将<span class="math inline">\(x, y\)</span>映射为该空间中的联合特征向量<span class="math inline">\(\phi(x, y)\)</span>，预测模型可以写为: <span class="math display">\[y=arg max f(\phi(x,y);θ)\]</span> 计算<span class="math inline">\(argmax\)</span>s得过程也称为解码Decoding过程</li>
</ul></li>
<li><p><strong>无监督学习</strong>：无监督学习（Unsupervised Learning，UL）是指从不包含目标标签的训练样本中自动学习到一些有价值的信息．典型的无监督学习问题有聚类、密度估计、特征学习、降维等．</p></li>
<li><p><strong>强化学习</strong>：强化学习（Reinforcement Learning，RL）是一类通过交互来学习的机器学习算法．在强化学习中，智能体根据环境的状态做出一个动作，并得到即时或延时的奖励．智能体在和环境的交互中不断学习并调整策略，以取得最大化的期望总回报</p></li>
</ul>
<h4 id="线性模型">3. 线性模型</h4>
<p>线性模型（Linear Model）是机器学习中应用最广泛的模型，指通过样本特征的线性组合来进行预测的模型。本节我们主要介绍四种不同线性分类模型：<strong>Logistic回归、Softmax回归、感知器和支持向量机</strong>，这些模型的区别主要在于使用了不同的损失函数</p>
<h5 id="线性判别函数和决策边界">3.1 线性判别函数和决策边界</h5>
<p>一个线性分类模型（Linear Classification Model）或线性分类器（Linear Classifier），是由一个（或多个）线性的判别函数<span class="math inline">\(f(x; w) =w^T + b\)</span> 和非线性的决策函数<span class="math inline">\(g(⋅)\)</span>组成．我们首先考虑二分类的情况，然后再扩展到多分类的情况．</p>
<h6 id="二分类">3.1.1 二分类</h6>
<p>二分类问题是最简单得，其类别标签只有两个值，通常设为<span class="math inline">\({+1,-1}\)</span>。因此只需要一个线性判别函数<span class="math inline">\(f(x; w) =w^T + b\)</span>。在特征空间<span class="math inline">\(R^D\)</span>中所有满足<span class="math inline">\(f(x;w)=0\)</span>得点组成一个<strong>分割超平面</strong>，也称为决策边界或决策平面，它将特征空间一分为二，两个区域各对应一个类边。特征空间中样本点到决策平面得距离为： <span class="math display">\[
\gamma={f(x;w)\over||w||}
\]</span> <img src="/2022/09/12/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/binaryclassfier.png" width="300"></p>
<h6 id="多分类">3.1.2 多分类</h6>
<p>多分类（Multi-class Classification）问题是指分类的类别数<span class="math inline">\(C\)</span>大于 2．多分类一般需要多个线性判别函数，但设计这些判别函数有很多种方式。一个多分类问题常用以下三种：</p>
<ul>
<li><strong>一对其余方式</strong>：把多分类问题转换为 C个“一对其余”的二分类问题．这种方式共需要<span class="math inline">\(C\)</span>个判别函数，其中第c个判别函数<span class="math inline">\(f_c\)</span> 是将类别c的样本和不属于类别c的样本分开</li>
<li><strong>“一对一”方式</strong>：把多分类问题转换为 <span class="math inline">\(C(C − 1)/2\)</span>个“一对一”的二分类问题．这种方式共需要<span class="math inline">\(C(C − 1)/2\)</span>个判别函数，其中第<span class="math inline">\((i, j)\)</span>个判别函数是把类别i和类别j的样本分开</li>
<li><strong>“argmax”方式</strong>：这是一种改进的“一对其余”方式，共需要<span class="math inline">\(C\)</span>个判别函数: <span class="math display">\[
f_C(x;w_c)=w^T_cx+b_C
\]</span> 表示对于样本<span class="math inline">\(x\)</span>，如果存在一个类边<span class="math inline">\(c\)</span>，相对于所有其他类边<span class="math inline">\(\tilde{c}(\tilde{c}≠c)\)</span>有<span class="math inline">\(f_c(x;w_c)&gt;f_{\tilde{c}(x,w_{\tilde{c}})}\)</span>。那么<span class="math inline">\(x\)</span>属于类边<span class="math inline">\(c\)</span></li>
</ul>
<p>上述得三种分类判别函数中前两个都有缺陷，那就是会存在一些难以区分得区域，二<strong>argmax</strong>方式很好得解决了这个问题</p>
<h5 id="logistic回归">3.2 Logistic回归</h5>
<p>Logistic 回归（Logistic Regression，LR）是一种常用的处理二分类问题的线性模型．在本节中，我们采用<span class="math inline">\(y ∈ [0, 1]\)</span>以符合Logistic回归的描述习惯。为解决连续线性函数不适合分类问题，引入非线性函数g来预测后验概率<span class="math inline">\(p(y=1|x)\)</span>: <span class="math display">\[
p(y=1|x)=g(f(x;w))
\]</span> 上面<span class="math inline">\(g\)</span>也被称为<strong>激活函数</strong>，它得作业其实就是将通过<span class="math inline">\(f(x;w)\)</span>得出得值进行(0,1)间的压缩，使其成为概率。反函数<span class="math inline">\(g^{-1}\)</span>称为<strong>联系函数</strong>。</p>
<p>因此使用LR作为激活函数时，相应的后验概率为（为简单起见，<span class="math inline">\(w\)</span>和<span class="math inline">\(x\)</span>均为增广矩阵）： <span class="math display">\[
p(y=1|x)={1\over 1+exp(-w^Tx)}；
p(y=0|x)=1-p(y=1|x)
\]</span> <img src="/2022/09/12/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/logistic.png" width="300"></p>
<h6 id="参数学习-1">3.2.1 参数学习</h6>
<p>Logistic 回归采用交叉熵作为损失函数，并使用梯度下降法来对参数进行优化．</p>
<p>给定<span class="math inline">\(N\)</span>个训练样本<span class="math inline">\((x^{(n)},y^{(n)})_{n=1}^N\)</span>，使用LR回归模型对每个样本进行预测，输出其标签为1的后验概率，<strong>记为<span class="math inline">\(\tilde{y}^{(n)}\)</span></strong>。则交叉熵损失函数后，其风险函数为： <span class="math display">\[
R(w)={-1\over N}\sum_{n=1}^N(p_r(y^{(n)}=1|x^{(n)}))log\tilde{y}^{(n)}+p_r(y^{(n)}=0|x^{(n)}))log(1-\tilde{y}^{(n)}))\\
={-1\over N}\sum_{n=1}^N(y^{(n)}log\tilde{y}^{(n)}+(1-y^{(n)})log(1-\tilde{y}^{(n)}))   \qquad式3.2.1
\]</span></p>
<p>然后对其进行求导的<span class="math inline">\({δR(w)\over δw}=-{1\over N}\sum x^{(n)}(y^{(n)}-\tilde{y}^{(n)})\)</span>。然后使用梯度下降法进行迭代更新进行优化： <span class="math display">\[
w_{t+1}=w_t+α{δR(w)\over δw}
\]</span></p>
<p>由式子<span class="math inline">\(3.2.1\)</span>可知，其风险函数是关于参数<span class="math inline">\(w\)</span>连续可导的凸函数，因此除了使用梯度下降，LR回归还可以用高阶的优化方法，如牛顿法。</p>
<h5 id="softmax回归">3.3 softmax回归</h5>
<p>Softmax 回归（Softmax Regression），也称为多项（Multinomial）或多类（Multi-Class）的Logistic回归，<strong>是Logistic回归在多分类问题上的推广</strong>。对于多类问题，类别标签<span class="math inline">\(y∈ {1, 2, ⋯ , C}\)</span>可以有<span class="math inline">\(C\)</span> 个取值．给定一个样本<span class="math inline">\(x\)</span>，Softmax回归预测的属于类别<span class="math inline">\(c\)</span>的条件概率为: <span class="math display">\[
\tilde{y}=p(y=c|x)=softmax(w^T_cx)\\
={exp(w_c^Tx)\over \sum ^C_{\tilde{c}=1}exp(w^T_\tilde{c}x)}
\]</span></p>
<p><strong>其决策函数为：</strong> <span class="math display">\[
\{y=[I(C=c]\}=arg max_{c=1}^C p(y=c|x)
\]</span></p>
<h6 id="参数学习-2">3.3.1 参数学习</h6>
<p>给定<span class="math inline">\(N\)</span>个训练样本<span class="math inline">\({(x^{(n)}, y^{(n)})}_{n=1}^N\)</span>，<strong>Softmax回归使用交叉熵损失函数来学习最优的参数矩阵<span class="math inline">\(W\)</span></strong>．为了方便起见，我们用<span class="math inline">\(C\)</span>维的one-hot向量<span class="math inline">\(y ∈(0, 1)^c\)</span> 来表示类别标签．对于类别<span class="math inline">\(C\)</span>，其向量表示为: <span class="math display">\[
y=[I(1=c),I(2=c),...,I(C=c)]
\]</span></p>
<p>采用交叉熵损失函数时，softmax的风险函数为： <span class="math display">\[
R(w)=-{1\over N}\sum_{n=1}^N\sum_{c=1}^C y_c^{(n)}log\tilde{y}_c^{(n)} \\
=-{1\over N}\sum_{n=1}^N (y^{(n)})^Tlog\tilde{y}^{(n)}
\]</span></p>
<p>利用梯度下降法，对其求导可得梯度： <span class="math display">\[
{δR(W)\over δW}=-{1\over N}\sum_{n=1}^N x^{(n)}(y^{(n)}-\tilde{y}^{(n)})^T
\]</span> 然后进行迭代更新： <span class="math display">\[
w_{t+1}=w_t+α{δR(w)\over δw}
\]</span></p>
<h4 id="深度学习简述">4.深度学习简述</h4>
<p>为了学习一种好的表示，需要构建具有一定“深度”的模型，并通过学习算法来让模型自动学习出好的特征表示（从底层特征，到中层特征，再到高层特征），从而最终提升预测模型的准确率．<strong>所谓“深度”是指原始数据进行非线性特征转换的次数</strong> <img src="/2022/09/12/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/DL.png" width="500"></p>
<p>深度学习是将原始的数据特征通过多步的特征转换得到一种特征表示，并进一步输入到预测函数得到最终结果．和“浅层学习”不同，<strong>深度学习需要解决的关键问题是贡献度分配问题（Credit Assignment Problem，CAP）[Minsky,1961]</strong>，即一个系统中不同的组件（component）或其参数对最终系统输出结果的贡献或影响.从某种意义上讲，深度学习可以看作一种强化学习（Reinforcement Learning，RL），每个内部组件并不能直接得到监督信息，需要通过整个模型的最终监督信息（奖励）得到，并且有一定的延时性</p>
<p><strong>深度学习采用的模型主要是神经网络模型，其主要原因是神经网络模型可以使用误差反向传播算法，从而可以比较好地解决贡献度分配问题</strong>．</p>
<h5 id="表示学习">4.1 表示学习</h5>
<p>为了提高机器学习系统的准确率，我们就需要将输入信息转换为有效的特 征，或者更一般性地称为表示（Representation）。如果有一种算法可以自动地学习出有效的特征，并提高最终机器学习模型的性能，那么这种学习就可以叫作<strong>表示学习</strong>。</p>
<p>表示学习的关键是解决<strong>语义鸿沟（Semantic Gap）问题</strong>．语义鸿沟问题是指输入数据的底层特征和高层语义信息之间的不一致性和差异性．比如给定一些关于“车”的图片，由于图片中每辆车的颜色和形状等属性都不尽相同，因此不同图片在像素级别上的表示（即底层特征）差异性也会非常大．但是我们理解这些图片是建立在比较抽象的高层语义概念上的．如果一个预测模型直接建立在底层特征之上，会导致对预测模型的能力要求过高．如果可以有一个好的表示在某种程度上能够反映出数据的高层语义特征，那么我们就能相对容易地构建后续的机器学习模型</p>
<h6 id="局部和分布式表示">4.1.1 局部和分布式表示</h6>
<p>在机器学习中，我们经常使用两种方式来表示特征：<strong>局部表示（Local Representation）和分布式表示（Distributed Representation）．</strong></p>
<ul>
<li><strong>局部表示</strong>：离散表示，one-Hot向量，单一值表示一个东西。
<ul>
<li><strong>优点：</strong>
<ul>
<li>这种离散的表示方式具有很好的解释性，有利于人工归纳和总结特征，并通过特征组合进行高效的特征工程</li>
<li>通过多种特征组合得到的表示向量通常是稀疏的二值向量，当用于线性模型时计算效率非常高</li>
</ul></li>
<li><strong>缺点</strong>：
<ul>
<li>one-hot向量的维数很高，且不能扩展．如果有一种新的颜色，我们就需要增加一维来表示</li>
<li>不同颜色之间的相似度都为0，即我们无法知道“红色”和“中国红”的相似度要高于“红色”和“黑色”的相似度</li>
</ul></li>
</ul></li>
<li><strong>分布式表示</strong>：压缩、低维的稠密向量，使用多个值表示一个东西，如表示颜色的方法是用RGB值来表示颜色，不同颜色对应到R、G、B三维空间中一个点，这种表示方式叫作分布式表示
<ul>
<li>分布式表示的表示能力要强很多，分布式表示的向量维度一般都比较低．我们只需要用一个三维的稠密向量就可以表示所有颜色．并且，分布式表示也很容易表示新的颜色名．此外，不同颜色之间的相似度也很容易计算</li>
</ul></li>
</ul>
<h5 id="端到端学习">4.2 端到端学习</h5>
<p><strong>端到端学习（End-to-End Learning），也称端到端训练，是指在学习过程中不进行分模块或分阶段训练，直接优化任务的总体目标</strong>．在端到端学习中，一般不需要明确地给出不同模块或阶段的功能，中间过程不需要人为干预．端到端学习的训练数据为“输入-输出”对的形式，无须提供其他额外信息．因此，端到端学习和深度学习一样，都是要解决贡献度分配问题．目前，大部分采用神经网络模型的深度学习也可以看作一种端到端的学习</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/2022/09/09/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="trluper">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Trluper">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/09/09/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0/" class="post-title-link" itemprop="url">计算机视觉课程笔记</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2022-09-09 20:14:55" itemprop="dateCreated datePublished" datetime="2022-09-09T20:14:55+08:00">2022-09-09</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2022-09-12 22:37:58" itemprop="dateModified" datetime="2022-09-12T22:37:58+08:00">2022-09-12</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/" itemprop="url" rel="index"><span itemprop="name">计算机视觉</span></a>
                </span>
            </span>

          <br>
            <span class="post-meta-item" title="本文字数">
              <span class="post-meta-item-icon">
                <i class="far fa-file-word"></i>
              </span>
                <span class="post-meta-item-text">本文字数：</span>
              <span>222</span>
            </span>
            <span class="post-meta-item" title="阅读时长">
              <span class="post-meta-item-icon">
                <i class="far fa-clock"></i>
              </span>
                <span class="post-meta-item-text">阅读时长 &asymp;</span>
              <span>1 分钟</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h4 id="深度学习cnn卷积神经网络算法精讲">1. 深度学习CNN卷积神经网络算法精讲</h4>
<h5 id="cnn与transformer的不同点">1.1 CNN与Transformer的不同点</h5>
<ul>
<li>要明白CNN就是对图像提取特征，提取不是一次性提取，而是多次，由局部向全局做特征。</li>
<li>Transformer则与CNN不同，直接全局考提取特征，这就避免了多次提取特征。</li>
</ul>
<h4 id="ai当下最火模块注意力机制解读">2. AI当下最火模块注意力机制解读</h4>
<h4 id="视觉领域transformer应用实例">3. 视觉领域transformer应用实例</h4>
<p>Transformer则与CNN不同，直接全局考提取特征，这就避免了多次提取特征。</p>
<h5 id="应用">3.1 应用</h5>
<h4 id="视觉当下最新研究方向与进展">4. 视觉当下最新研究方向与进展</h4>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  


  
  <nav class="pagination">
    <span class="page-number current">1</span><a class="page-number" href="/page/2/">2</a><span class="space">&hellip;</span><a class="page-number" href="/page/4/">4</a><a class="extend next" rel="next" href="/page/2/"><i class="fa fa-angle-right" aria-label="下一页"></i></a>
  </nav>



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">trluper</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">31</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">24</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">21</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://github.com/trluper" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;trluper" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
  </div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2022</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Trluper</span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-chart-area"></i>
    </span>
    <span title="站点总字数">539k</span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-coffee"></i>
    </span>
    <span title="站点阅读时长">8:10</span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://mist.theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Mist</a> 强力驱动
  </div>

        
<div class="busuanzi-count">
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
    <span class="post-meta-item" id="busuanzi_container_site_uv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="fa fa-user"></i>
      </span>
      <span class="site-uv" title="总访客量">
        <span id="busuanzi_value_site_uv"></span>
      </span>
    </span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item" id="busuanzi_container_site_pv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="fa fa-eye"></i>
      </span>
      <span class="site-pv" title="总访问量">
        <span id="busuanzi_value_site_pv"></span>
      </span>
    </span>
</div>








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/muse.js"></script>


<script src="/js/next-boot.js"></script>

<script src="/js/bookmark.js"></script>




  




  
<script src="/js/local-search.js"></script>













  

  
      

<script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
          load: ['[tex]/mhchem'],
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
          packages: {'[+]': ['mhchem']},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }
</script>

    

  

</body>
</html>
